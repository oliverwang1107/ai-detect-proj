{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98438fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Extract ELA(int8) / PRNU(int8) / CLIP(float32) → save as .npy\n",
    "# ======================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import os, random, warnings, json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageChops, ImageFile\n",
    "from skimage import io as skio\n",
    "from skimage.util import img_as_float32\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from skimage.transform import resize\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ---------------- Config（改這裡） ----------------\n",
    "REAL_DIR = Path(\"/home/yaya/ai-detect-proj/data/Pic\")\n",
    "FAKE_DIR = Path(\"/home/yaya/ai-detect-proj/data/6kflux\")\n",
    "\n",
    "OUT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_test\")  # 會自動建立\n",
    "# 會生成：\n",
    "#   OUT_ROOT/ela_real_npy/*.npy   （int8）\n",
    "#   OUT_ROOT/ela_fake_npy/*.npy   （int8）\n",
    "#   OUT_ROOT/prnu_real_npy/*.npy  （int8）\n",
    "#   OUT_ROOT/prnu_fake_npy/*.npy  （int8）\n",
    "#   OUT_ROOT/clip_real_npy/*.npy  （float32）\n",
    "#   OUT_ROOT/clip_fake_npy/*.npy  （float32）\n",
    "\n",
    "RUN_CLASSES = [\"fake\"]                 # 可以改 [\"real\"], [\"fake\"], 或 [\"real\",\"fake\"]\n",
    "SELECT_FEATURES = [\"ela\", \"prnu\", \"clip\"]  # 想只跑其中幾個就刪掉其餘\n",
    "\n",
    "# ⬇︎ 抽樣上限：每個 class 最多處理多少張（None=不限制）\n",
    "MAX_PER_CLASS = 30000  # 例如只取 1 萬張；或設 None 表示全取\n",
    "\n",
    "# ELA 參數\n",
    "IMG_SIZE     = 256     # 先把最短邊放到 >= 這個長度後做中心裁切\n",
    "ELA_QUALITY  = 90\n",
    "ELA_SCALE    = 15      # 只是把差值放大以增強對比；之後仍會映射再量化\n",
    "ELA_FEASZ    = 128     # ELA 輸出尺寸\n",
    "# ELA int8 格式說明：先把 0..1 → uint8(0..255) → int8 = uint8 - 128  （方便存成 int8）\n",
    "# 之後讀取時若要還原 0..1，可用：(arr_i8.astype(float)+128)/255.0\n",
    "\n",
    "# PRNU 參數\n",
    "PRNU_CROP_FROM = 256   # 中心裁起始邊長（不足會放大）\n",
    "PRNU_OUT_SIZE  = 256   # PRNU 輸出尺寸\n",
    "PRNU_WAVELET   = \"db8\" # 小波基\n",
    "PRNU_MODE      = \"soft\"\n",
    "\n",
    "# PRNU 量化（int8）設定：對稱量化 q = clip(round(x/S*127), -127, 127)\n",
    "PRNU_Q_MODE    = \"per_file\"  # 'per_file'（推薦）| 'global'\n",
    "PRNU_Q_PERC    = 0.999       # 用 |x| 的 p99.9 當尺度 S（對 outlier 不敏感）\n",
    "PRNU_Q_SAMPLES = 4096        # 每張抽幾個像素估分位數\n",
    "\n",
    "# CLIP 模型（需要 pip 安裝 openai-clip；會 lazy-load）\n",
    "CLIP_MODEL_NAME = \"ViT-L/14\"  # 你也可用 \"ViT-B/32\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1d2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ===== 并行/批次設定（可依機器調）=====\n",
    "N_WORKERS_CPU = max(1, (os.cpu_count() or 4) - 1)  # 給 ELA/PRNU 用的進程數\n",
    "CLIP_BATCH    = 64                                   # RTX 4060 + ViT-L/14 可先用 64（OOM 就降 48/32）\n",
    "DL_WORKERS    = min(4, max(1, (os.cpu_count() or 4)//2))  # DataLoader 的 CPU worker\n",
    "PIN_MEMORY    = True\n",
    "\n",
    "# 避免多進程下 BLAS 過度多執行緒互打架\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# tqdm in notebook（自動 fallback）\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm.auto import tqdm\n",
    "TQDM_KW = dict(dynamic_ncols=True, leave=False)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def _atomic_save_npy(path: Path, arr: np.ndarray):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # 方法 A：簡單！臨時檔用 \".tmp.npy\"（np.save 不會再加 .npy）\n",
    "    tmp = path.with_suffix(\".tmp.npy\")\n",
    "    np.save(tmp, arr, allow_pickle=False)\n",
    "    os.replace(tmp, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7812b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Utils ----------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def all_images(root: Path):\n",
    "    return [p for p in root.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "\n",
    "def ensure_dirs():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    for feat in SELECT_FEATURES:\n",
    "        for cls in RUN_CLASSES:\n",
    "            (OUT_ROOT / f\"{feat}_{cls}_npy\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def center_resize_crop_PIL(img: Image.Image, to_size: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    if min(w, h) < to_size:\n",
    "        s = to_size / min(w, h)\n",
    "        img = img.resize((int(round(w*s)), int(round(h*s))), Image.BICUBIC)\n",
    "        w, h = img.size\n",
    "    x0, y0 = (w - to_size)//2, (h - to_size)//2\n",
    "    return img.crop((x0, y0, x0 + to_size, y0 + to_size))\n",
    "\n",
    "def center_resize_crop_np(img: np.ndarray, crop_from=512, out_size=256) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    if min(h, w) < crop_from:\n",
    "        s = crop_from / min(h, w)\n",
    "        img = resize(img, (int(round(h*s)), int(round(w*s))),\n",
    "                     preserve_range=True, anti_aliasing=True).astype(img.dtype)\n",
    "        h, w = img.shape[:2]\n",
    "    y0, x0 = (h - crop_from)//2, (w - crop_from)//2\n",
    "    img = img[y0:y0+crop_from, x0:x0+crop_from]\n",
    "    if crop_from != out_size:\n",
    "        img = resize(img, (out_size, out_size),\n",
    "                     preserve_range=True, anti_aliasing=True).astype(img.dtype)\n",
    "    return img\n",
    "\n",
    "# ---------------- ELA（→ int8） ----------------\n",
    "def _to_int8_offset128_from_01(x01: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"x01 in [0,1] → uint8(0..255) → int8(-128..127)\"\"\"\n",
    "    u8 = np.rint(np.clip(x01, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    i8 = (u8.astype(np.int16) - 128).astype(np.int8)\n",
    "    return i8\n",
    "\n",
    "def extract_ela_i8(p: Path) -> np.ndarray | None:\n",
    "    \"\"\"回傳 int8 (H,W) 大小為 ELA_FEASZ×ELA_FEASZ；儲存格式：uint8-128\"\"\"\n",
    "    try:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        img = center_resize_crop_PIL(img, IMG_SIZE)\n",
    "        # JPEG 重壓 & 差分\n",
    "        buf = BytesIO()\n",
    "        img.save(buf, format=\"JPEG\", quality=int(ELA_QUALITY), subsampling=0, optimize=False)\n",
    "        buf.seek(0)\n",
    "        diff = ImageChops.difference(img, Image.open(buf)).point(lambda x: x * ELA_SCALE)\n",
    "        diff = diff.convert(\"L\").resize((ELA_FEASZ, ELA_FEASZ))\n",
    "        arr01 = np.asarray(diff, dtype=np.float32) / 255.0  # 0..1\n",
    "        q = _to_int8_offset128_from_01(arr01)\n",
    "        return q\n",
    "    except Exception as e:\n",
    "        print(\"[ELA] skip\", p.name, \"|\", e)\n",
    "        return None\n",
    "\n",
    "# ---------------- PRNU（→ int8） ----------------\n",
    "def _sample_abs_vals(a: np.ndarray, k: int, rng=np.random.default_rng(SEED)) -> np.ndarray:\n",
    "    v = a.reshape(-1).astype(np.float32, copy=False)\n",
    "    if v.size <= k: return np.abs(v)\n",
    "    idx = rng.integers(0, v.size, size=k, endpoint=False)\n",
    "    return np.abs(v[idx])\n",
    "\n",
    "def _fast_percentile(v: np.ndarray, q: float) -> float:\n",
    "    if v.size == 0: return 1e-8\n",
    "    k = int(q * (v.size - 1))\n",
    "    val = np.partition(v, k)[k]\n",
    "    return float(max(val, 1e-8))\n",
    "\n",
    "def _prnu_quant_i8(a: np.ndarray, S: float) -> np.ndarray:\n",
    "    x = np.clip(a, -S, S) / S * 127.0\n",
    "    q = np.rint(x).astype(np.int16)\n",
    "    q = np.clip(q, -127, 127).astype(np.int8)\n",
    "    return q\n",
    "\n",
    "def extract_prnu_i8(p: Path) -> np.ndarray | None:\n",
    "    \"\"\"回傳 int8 (PRNU_OUT_SIZE, PRNU_OUT_SIZE)，對稱量化\"\"\"\n",
    "    try:\n",
    "        im = skio.imread(str(p))\n",
    "        if im.ndim == 2:\n",
    "            im = np.repeat(im[..., None], 3, axis=-1)\n",
    "        im = img_as_float32(im)  # 0..1\n",
    "        crop = center_resize_crop_np(im, PRNU_CROP_FROM, PRNU_OUT_SIZE)\n",
    "        gray = crop.mean(axis=2, dtype=np.float32)\n",
    "        denoised = denoise_wavelet(gray, channel_axis=None, mode=PRNU_MODE,\n",
    "                                   wavelet=PRNU_WAVELET, convert2ycbcr=False)\n",
    "        residual = gray - denoised\n",
    "        residual -= residual.mean()\n",
    "\n",
    "        # 估 per-file S\n",
    "        if PRNU_Q_MODE == \"per_file\":\n",
    "            vals = _sample_abs_vals(residual, PRNU_Q_SAMPLES)\n",
    "            S = _fast_percentile(vals, PRNU_Q_PERC)\n",
    "        else:\n",
    "            # 若要 global，可先掃一輪估 S，再放進這裡；簡化起見用保底 S\n",
    "            S = max(1e-6, float(np.std(residual)) * 6.0)\n",
    "\n",
    "        q = _prnu_quant_i8(residual, S)\n",
    "        return q\n",
    "    except Exception as e:\n",
    "        print(\"[PRNU] skip\", p.name, \"|\", e)\n",
    "        return None\n",
    "\n",
    "# ---------------- CLIP（→ float32 向量） ----------------\n",
    "# ---------------- CLIP（→ float32；倒數第二層；open_clip/LAION） ----------------\n",
    "# 取代原本的 \"import torch, clip\" 與 extract_clip_vec() 區塊\n",
    "\n",
    "# 設定：選 backbone 與對應的 LAION 權重\n",
    "CLIP_BACKBONE   = \"ViT-L-14\"         # 可改 \"ViT-B-32\"、\"ViT-L-14-336\" 等\n",
    "CLIP_PRETRAINED = {\n",
    "    \"ViT-L-14\":       \"laion2b_s32b_b82k\",\n",
    "    \"ViT-B-32\":       \"laion400m_e32\",\n",
    "    \"ViT-L-14-336\":   \"laion2b_s32b_b82k\"  # 336 變體若可用，維持同權重系列\n",
    "}.get(CLIP_BACKBONE, \"laion2b_s32b_b82k\")\n",
    "\n",
    "_openclip_model = None\n",
    "_openclip_pre   = None\n",
    "_openclip_dev   = \"cpu\"\n",
    "\n",
    "# ---------------- Fix: open_clip loader（相容 2/3 回傳值） ----------------\n",
    "_openclip_model = None\n",
    "_openclip_pre   = None\n",
    "_openclip_dev   = \"cpu\"\n",
    "\n",
    "def load_openclip():\n",
    "    global _openclip_model, _openclip_pre, _openclip_dev\n",
    "    if _openclip_model is None:\n",
    "        import torch, open_clip\n",
    "        _openclip_dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        res = open_clip.create_model_and_transforms(\n",
    "            CLIP_BACKBONE, pretrained=CLIP_PRETRAINED\n",
    "        )\n",
    "        # res 可能是 (model, pre_train, pre_val) 或 (model, pre)\n",
    "        if isinstance(res, tuple) and len(res) == 3:\n",
    "            model, pre_train, pre_val = res\n",
    "            pre = pre_val   # 推論用 eval 版變換\n",
    "        elif isinstance(res, tuple) and len(res) == 2:\n",
    "            model, pre = res\n",
    "        else:\n",
    "            # 極少數版本保底：直接用另一個 API\n",
    "            model, pre = open_clip.create_model_from_pretrained(\n",
    "                CLIP_BACKBONE, pretrained=CLIP_PRETRAINED\n",
    "            )\n",
    "\n",
    "        _openclip_model = model.to(_openclip_dev)\n",
    "        _openclip_model.eval()\n",
    "        _openclip_pre = pre\n",
    "\n",
    "    return _openclip_model, _openclip_pre, _openclip_dev\n",
    "\n",
    "\n",
    "def _encode_image_penultimate(model, image_tensor):\n",
    "    \"\"\"\n",
    "    回傳倒數第二層（pre-projection）CLS 特徵：\n",
    "    - 優先在 visual.ln_post 取得輸出（OpenAI/early open_clip ViT 結構）\n",
    "    - 退而求其次用 trunk.forward_features() 取 CLS，再經 ln_post（若存在）\n",
    "    - 最後的保底是用 encode_image（投影後），確保不會報錯\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    feats = {}\n",
    "    handle = None\n",
    "\n",
    "    # 1) 嘗試 hook 在 ln_post（投影前最後一層）\n",
    "    try:\n",
    "        target = getattr(model.visual, \"ln_post\", None)\n",
    "        if target is not None:\n",
    "            def _hook(_m, _inp, out):\n",
    "                feats[\"penult\"] = out.detach()\n",
    "            handle = target.register_forward_hook(_hook)\n",
    "            _ = model.encode_image(image_tensor)  # 觸發 forward\n",
    "            if handle is not None:\n",
    "                handle.remove()\n",
    "            if \"penult\" in feats:\n",
    "                return feats[\"penult\"]\n",
    "    except Exception:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "\n",
    "    # 2) timm trunk：直接拿 forward_features 的 CLS，並套 ln_post（若有）\n",
    "    try:\n",
    "        visual = model.visual\n",
    "        if hasattr(visual, \"trunk\") and hasattr(visual.trunk, \"forward_features\"):\n",
    "            x = visual.trunk.forward_features(image_tensor)\n",
    "            if isinstance(x, (tuple, list)):\n",
    "                x = x[0]\n",
    "            # 若仍是 token map，取 CLS\n",
    "            if x.ndim == 3:\n",
    "                x = x[:, 0, :]\n",
    "            if hasattr(visual, \"ln_post\") and visual.ln_post is not None:\n",
    "                x = visual.ln_post(x)\n",
    "            return x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) 保底：使用最終投影後向量（不是倒數第二層，但避免中斷流程）\n",
    "    with torch.no_grad():\n",
    "        return model.encode_image(image_tensor)\n",
    "\n",
    "def extract_clip_vec(p: Path) -> np.ndarray | None:\n",
    "    \"\"\"回傳 float32 向量 (D,)，D 取決於 backbone（例如 ViT-L-14 ≈ 1024 維倒數第二層）\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        model, pre, dev = load_openclip()\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        # 先做正方形中心裁切，之後交給 open_clip 的 preprocess 做尺寸/標準化\n",
    "        w, h = img.size\n",
    "        s = min(w, h)\n",
    "        img = img.crop(((w - s)//2, (h - s)//2, (w + s)//2, (h + s)//2))\n",
    "        im = pre(img).unsqueeze(0).to(dev)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            penult = _encode_image_penultimate(model, im).float()\n",
    "            # L2 normalize（與你原本一致）\n",
    "            penult = penult / penult.norm(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "            vec = penult.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "        return vec\n",
    "    except Exception as e:\n",
    "        print(\"[CLIP] skip\", p.name, \"|\", e)\n",
    "        return None\n",
    "\n",
    "def _ela_worker(args):\n",
    "    img_path, out_path = args\n",
    "    try:\n",
    "        if out_path.exists(): \n",
    "            return True\n",
    "        arr = extract_ela_i8(img_path)\n",
    "        if arr is None:\n",
    "            return False\n",
    "        _atomic_save_npy(out_path, arr)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _prnu_worker(args):\n",
    "    img_path, out_path = args\n",
    "    try:\n",
    "        if out_path.exists():\n",
    "            return True\n",
    "        arr = extract_prnu_i8(img_path)\n",
    "        if arr is None:\n",
    "            return False\n",
    "        _atomic_save_npy(out_path, arr)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf6ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _center_square(img: Image.Image) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    s = min(w, h)\n",
    "    return img.crop(((w - s)//2, (h - s)//2, (w + s)//2, (h + s)//2))\n",
    "\n",
    "class _ClipPathsDataset:\n",
    "    def __init__(self, paths, pre):\n",
    "        self.paths = paths\n",
    "        self.pre   = pre\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.paths[i]\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            img = _center_square(img)\n",
    "            t   = self.pre(img)\n",
    "            ok  = True\n",
    "        except Exception:\n",
    "            t, ok = None, False\n",
    "        return p, t, ok\n",
    "\n",
    "def _collate(batch):\n",
    "    # batch: list of (path, tensor, ok)\n",
    "    ps, ts = [], []\n",
    "    for p, t, ok in batch:\n",
    "        if ok and t is not None:\n",
    "            ps.append(p); ts.append(t)\n",
    "    if len(ts) == 0:\n",
    "        return [], None\n",
    "    import torch\n",
    "    return ps, torch.stack(ts, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5434d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Runner ----------------\n",
    "def _slug(s: str) -> str:\n",
    "    return \"\".join(c if (c.isalnum() or c in \"-_.\") else \"_\" for c in s)\n",
    "\n",
    "def make_id(p: Path, root: Path) -> str:\n",
    "    rel = p.relative_to(root)                 # a/b/c.jpg\n",
    "    base = \"_\".join(rel.with_suffix(\"\").parts)\n",
    "    dataset = _slug(root.name)                # e.g., \"unsplash\" or \"FLUX\"\n",
    "    return f\"{dataset}__{base}\"\n",
    "\n",
    "def run_extract_for_class(cls: str, root: Path):\n",
    "    files = all_images(root)\n",
    "    if MAX_PER_CLASS is not None and len(files) > MAX_PER_CLASS:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        idx = rng.choice(len(files), size=MAX_PER_CLASS, replace=False)\n",
    "        files = [files[i] for i in idx]\n",
    "    print(f\"→ {cls} ({len(files)} images) from {root}\")\n",
    "\n",
    "    # 準備輸出資料夾\n",
    "    dirs = {}\n",
    "    if \"ela\"  in SELECT_FEATURES: dirs[\"ela\"]  = OUT_ROOT / f\"ela_{cls}_npy\"\n",
    "    if \"prnu\" in SELECT_FEATURES: dirs[\"prnu\"] = OUT_ROOT / f\"prnu_{cls}_npy\"\n",
    "    if \"clip\" in SELECT_FEATURES: dirs[\"clip\"] = OUT_ROOT / f\"clip_{cls}_npy\"\n",
    "    for d in dirs.values(): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- 先跑 ELA/PRNU：CPU 多進程 ----\n",
    "    if \"ela\" in dirs:\n",
    "        tasks = []\n",
    "        for p in files:\n",
    "            fid  = make_id(p, root)\n",
    "            outp = dirs[\"ela\"] / f\"{fid}.npy\"\n",
    "            if not outp.exists():\n",
    "                tasks.append((p, outp))\n",
    "        if tasks:\n",
    "            print(f\"ELA (CPU x{N_WORKERS_CPU}) → {len(tasks)}\")\n",
    "            with ProcessPoolExecutor(max_workers=N_WORKERS_CPU) as ex:\n",
    "                futs = [ex.submit(_ela_worker, t) for t in tasks]\n",
    "                ok = 0\n",
    "                for f in tqdm(as_completed(futs), total=len(futs), desc=\"ELA\", **TQDM_KW):\n",
    "                    ok += 1 if f.result() else 0\n",
    "            print(f\"ELA saved: {ok}/{len(tasks)}\")\n",
    "\n",
    "    if \"prnu\" in dirs:\n",
    "        tasks = []\n",
    "        for p in files:\n",
    "            fid  = make_id(p, root)\n",
    "            outp = dirs[\"prnu\"] / f\"{fid}.npy\"\n",
    "            if not outp.exists():\n",
    "                tasks.append((p, outp))\n",
    "        if tasks:\n",
    "            print(f\"PRNU (CPU x{N_WORKERS_CPU}) → {len(tasks)}\")\n",
    "            with ProcessPoolExecutor(max_workers=N_WORKERS_CPU) as ex:\n",
    "                futs = [ex.submit(_prnu_worker, t) for t in tasks]\n",
    "                ok = 0\n",
    "                for f in tqdm(as_completed(futs), total=len(futs), desc=\"PRNU\", **TQDM_KW):\n",
    "                    ok += 1 if f.result() else 0\n",
    "            print(f\"PRNU saved: {ok}/{len(tasks)}\")\n",
    "\n",
    "    # ---- 再跑 CLIP：GPU 單進程 + DataLoader 多工載入 ----\n",
    "    if \"clip\" in dirs:\n",
    "        to_run = []\n",
    "        for p in files:\n",
    "            fid  = make_id(p, root)\n",
    "            outp = dirs[\"clip\"] / f\"{fid}.npy\"\n",
    "            if not outp.exists():\n",
    "                to_run.append((p, outp))\n",
    "        if to_run:\n",
    "            print(f\"CLIP (GPU batch={CLIP_BATCH}, loader_workers={DL_WORKERS}) → {len(to_run)}\")\n",
    "            # 準備資料集 / DataLoader\n",
    "            import torch\n",
    "            model, pre, dev = load_openclip()\n",
    "            ds_paths  = [p for p,_ in to_run]\n",
    "            out_paths = {p: outp for p, outp in to_run}\n",
    "            ds = _ClipPathsDataset(ds_paths, pre)\n",
    "            dl = torch.utils.data.DataLoader(\n",
    "                ds, batch_size=CLIP_BATCH, shuffle=False,\n",
    "                num_workers=DL_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                collate_fn=_collate, drop_last=False\n",
    "            )\n",
    "            n_ok = 0\n",
    "            pbar = tqdm(total=len(ds_paths), desc=\"CLIP\", **TQDM_KW)\n",
    "            with torch.no_grad():\n",
    "                for paths, batch in dl:\n",
    "                    if not paths:  # 全部壞圖\n",
    "                        continue\n",
    "                    batch = batch.to(dev, non_blocking=True)\n",
    "                    feats = _encode_image_penultimate(model, batch).float()\n",
    "                    feats = feats / feats.norm(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "                    vecs  = feats.cpu().numpy().astype(np.float32)\n",
    "                    # 寫檔\n",
    "                    for pth, vec in zip(paths, vecs):\n",
    "                        _atomic_save_npy(out_paths[pth], vec)\n",
    "                        n_ok += 1\n",
    "                    pbar.update(len(paths))\n",
    "            pbar.close()\n",
    "            print(f\"CLIP saved: {n_ok}/{len(ds_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e412270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ fake (3650 images) from /home/yaya/ai-detect-proj/data/6kflux\n",
      "ELA (CPU x7) → 3650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fda8b5ae5b4bda857df4c239c37ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ELA:   0%|          | 0/3650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELA saved: 3650/3650\n",
      "PRNU (CPU x7) → 3650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df135c6ecb8f4406946606eb840d4795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PRNU:   0%|          | 0/3650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRNU saved: 3650/3650\n",
      "CLIP (GPU batch=64, loader_workers=4) → 3650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fdebbfe74a4de1b7e753129c20a188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIP:   0%|          | 0/3650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP saved: 3650/3650\n",
      "✅ Done. Features saved under: /home/yaya/ai-detect-proj/Script/features_test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Go! ----------------\n",
    "def ensure_dirs():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    for feat in SELECT_FEATURES:\n",
    "        for cls in RUN_CLASSES:\n",
    "            (OUT_ROOT / f\"{feat}_{cls}_npy\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ensure_dirs()\n",
    "if \"real\" in RUN_CLASSES: run_extract_for_class(\"real\", REAL_DIR)\n",
    "if \"fake\" in RUN_CLASSES: run_extract_for_class(\"fake\", FAKE_DIR)\n",
    "print(\"✅ Done. Features saved under:\", OUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230acd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_tmp_npy(root: Path):\n",
    "    cnt = 0\n",
    "    for p in root.rglob(\"*.tmp.npy\"):\n",
    "        try:\n",
    "            p.unlink()\n",
    "            cnt += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"🧹 cleaned {cnt} orphan .tmp.npy files under {root}\")\n",
    "\n",
    "# 跑一次\n",
    "cleanup_tmp_npy(OUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84834c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, pre, dev = load_openclip()\n",
    "print(\"device:\", dev)\n",
    "p = next(iter(REAL_DIR.rglob(\"*.jpg\")), None) or next(iter(FAKE_DIR.rglob(\"*.png\")), None)\n",
    "print(\"test image:\", p)\n",
    "v = extract_clip_vec(p)\n",
    "print(\"vec shape:\", None if v is None else v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GPU 版 CLIP→SVM（一格可跑）=====\n",
    "from pathlib import Path\n",
    "import os, json, random, numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------- 基本參數 --------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "OUT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_256\")\n",
    "CLIP_REAL_DIR = OUT_ROOT / \"clip_real_npy\"\n",
    "CLIP_FAKE_DIR = OUT_ROOT / \"clip_fake_npy\"\n",
    "SPLIT_JSON = Path(\"/home/yaya/ai-detect-proj/Script/splits/combined_split.json\")  # 若無則隨機切\n",
    "N_PER_CLASS = 10000\n",
    "C = 1.0  # SVM 強度（越大越貼訓練集）\n",
    "\n",
    "# -------- 掃檔與切分 --------\n",
    "def list_npy(d): return sorted([p for p in d.glob(\"*.npy\")])\n",
    "def fid(p: Path): return p.stem\n",
    "\n",
    "real_files = list_npy(CLIP_REAL_DIR)\n",
    "fake_files = list_npy(CLIP_FAKE_DIR)\n",
    "assert real_files and fake_files, \"找不到 CLIP 特徵 .npy，請先完成特徵抽取。\"\n",
    "\n",
    "id2path = {\"real\": {fid(p): p for p in real_files},\n",
    "           \"fake\": {fid(p): p for p in fake_files}}\n",
    "\n",
    "def intersect_ids(need_ids, pool_dict): return [i for i in need_ids if i in pool_dict]\n",
    "\n",
    "def make_splits():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    if SPLIT_JSON.exists():\n",
    "        ids = json.loads(SPLIT_JSON.read_text())[\"ids\"]\n",
    "        def pick(sp, cls):\n",
    "            arr = intersect_ids(ids[sp], id2path[cls])\n",
    "            if N_PER_CLASS is not None:\n",
    "                arr = arr[:min(len(arr), N_PER_CLASS)]\n",
    "            return arr\n",
    "        splits = {\n",
    "            \"train\": [(i,0) for i in pick(\"train\",\"real\")] + [(i,1) for i in pick(\"train\",\"fake\")],\n",
    "            \"val\":   [(i,0) for i in pick(\"val\",\"real\")]   + [(i,1) for i in pick(\"val\",\"fake\")],\n",
    "            \"test\":  [(i,0) for i in pick(\"test\",\"real\")]  + [(i,1) for i in pick(\"test\",\"fake\")],\n",
    "        }\n",
    "        print(f\"使用 split.json：{SPLIT_JSON}\")\n",
    "    else:\n",
    "        real_ids = list(id2path[\"real\"].keys())\n",
    "        fake_ids = list(id2path[\"fake\"].keys())\n",
    "        random.shuffle(real_ids); random.shuffle(fake_ids)\n",
    "        if N_PER_CLASS is not None:\n",
    "            real_ids = real_ids[:min(len(real_ids), N_PER_CLASS)]\n",
    "            fake_ids = fake_ids[:min(len(fake_ids), N_PER_CLASS)]\n",
    "        # 8:1:1\n",
    "        r_tr, r_tmp = train_test_split(real_ids, test_size=0.2, random_state=SEED)\n",
    "        f_tr, f_tmp = train_test_split(fake_ids, test_size=0.2, random_state=SEED)\n",
    "        r_va, r_te  = train_test_split(r_tmp, test_size=0.5, random_state=SEED)\n",
    "        f_va, f_te  = train_test_split(f_tmp, test_size=0.5, random_state=SEED)\n",
    "        splits = {\n",
    "            \"train\": [(i,0) for i in r_tr] + [(i,1) for i in f_tr],\n",
    "            \"val\":   [(i,0) for i in r_va] + [(i,1) for i in f_va],\n",
    "            \"test\":  [(i,0) for i in r_te] + [(i,1) for i in f_te],\n",
    "        }\n",
    "        print(\"使用隨機切分（無 split.json）\")\n",
    "    for sp in splits:\n",
    "        random.shuffle(splits[sp])\n",
    "        n0 = sum(1 for _,y in splits[sp] if y==0); n1 = len(splits[sp])-n0\n",
    "        print(f\"{sp}: total={len(splits[sp])} | real={n0} fake={n1}\")\n",
    "    return splits\n",
    "splits = make_splits()\n",
    "\n",
    "def load_pairs(pairs):\n",
    "    X, y = [], []\n",
    "    for i, lab in pairs:\n",
    "        p = id2path[\"real\" if lab==0 else \"fake\"][i]\n",
    "        v = np.load(p, allow_pickle=False)\n",
    "        X.append(v.astype(np.float32, copy=False).reshape(-1))\n",
    "        y.append(lab)\n",
    "    return np.stack(X, 0), np.array(y, dtype=np.int32)\n",
    "\n",
    "X_train, y_train = load_pairs(splits[\"train\"])\n",
    "X_val,   y_val   = load_pairs(splits[\"val\"])\n",
    "X_test,  y_test  = load_pairs(splits[\"test\"])\n",
    "print(\"shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# -------- 嘗試 cuML（若不可用則退回 PyTorch 線性 SVM）--------\n",
    "USE_BACKEND = None\n",
    "try:\n",
    "    import cuml, cupy as cp\n",
    "    from cuml.svm import SVC\n",
    "    USE_BACKEND = \"cuml\"\n",
    "    print(\"✅ 使用 cuML GPU SVM\")\n",
    "except Exception as e:\n",
    "    USE_BACKEND = \"torch\"\n",
    "    print(\"⚠️ cuML 不可用，改用 PyTorch 線性 SVM（GPU）:\", e)\n",
    "\n",
    "def evaluate_scores(y_true, scores, name):\n",
    "    acc = accuracy_score(y_true, (scores>0).astype(np.int32))\n",
    "    auc = roc_auc_score(y_true, scores)\n",
    "    print(f\"[{name}] acc={acc:.4f} auc={auc:.4f}\")\n",
    "    print(confusion_matrix(y_true, (scores>0).astype(np.int32)))\n",
    "    print(classification_report(y_true, (scores>0).astype(np.int32), target_names=[\"real\",\"fake\"], digits=4))\n",
    "\n",
    "if USE_BACKEND == \"cuml\":\n",
    "    # ---- cuML 線性 SVM（GPU）----\n",
    "    Xtr = cp.asarray(X_train); ytr = cp.asarray(y_train)\n",
    "    Xva = cp.asarray(X_val);   yva = cp.asarray(y_val)\n",
    "    Xte = cp.asarray(X_test);  yte = cp.asarray(y_test)\n",
    "\n",
    "    clf = SVC(kernel=\"linear\", C=C, probability=False, max_iter=100000, tol=1e-3)\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    # decision_function > 0 視為 fake（label=1）\n",
    "    s_val = clf.decision_function(Xva).get()\n",
    "    s_te  = clf.decision_function(Xte).get()\n",
    "\n",
    "    evaluate_scores(y_val,  s_val,  \"val\")\n",
    "    evaluate_scores(y_test, s_te,   \"test\")\n",
    "\n",
    "    # 儲存\n",
    "    import joblib\n",
    "    joblib.dump({\"backend\":\"cuml\",\"model\":clf}, \"/home/yaya/ai-detect-proj/Script/saved_models/clip_svm_gpu.pkl\")\n",
    "    print(\"✅ saved: saved_models/clip_svm_gpu.pkl\")\n",
    "\n",
    "else:\n",
    "    # ---- PyTorch 線性 SVM（hinge）----\n",
    "    import torch, torch.nn as nn\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Xtr = torch.from_numpy(X_train).to(device)\n",
    "    Xva = torch.from_numpy(X_val).to(device)\n",
    "    Xte = torch.from_numpy(X_test).to(device)\n",
    "    # y in {0,1} → y' in {-1,+1}\n",
    "    ytr = torch.from_numpy(np.where(y_train==1, 1, -1).astype(np.float32)).to(device)\n",
    "    yva = torch.from_numpy(np.where(y_val==1,   1, -1).astype(np.float32)).to(device)\n",
    "    yte = torch.from_numpy(np.where(y_test==1,  1, -1).astype(np.float32)).to(device)\n",
    "\n",
    "    D = Xtr.shape[1]\n",
    "    model = nn.Linear(D, 1, bias=True).to(device)\n",
    "\n",
    "    # Hinge 損失 + L2（= SVM 的正則）：min 0.5*||w||^2 + C * Σ max(0, 1 - y*(Wx+b))\n",
    "    def hinge_loss(out, y):\n",
    "        # out: [N,1], y: [N] in {-1,+1}\n",
    "        m = 1 - y.unsqueeze(1) * out\n",
    "        return torch.clamp(m, min=0).mean()\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    EPOCHS, BATCH = 50, 1024\n",
    "    N = Xtr.shape[0]\n",
    "    best_val = float(\"inf\"); best = None\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        total = 0.0\n",
    "        for i in range(0, N, BATCH):\n",
    "            idx = perm[i:i+BATCH]\n",
    "            xb, yb = Xtr[idx], ytr[idx]\n",
    "            out = model(xb)                 # [B,1]\n",
    "            reg = 0.5 * (model.weight**2).sum()\n",
    "            loss = reg + C * hinge_loss(out, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        # 簡單 val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            s_val = model(Xva).squeeze(1)\n",
    "            val_loss = (0.5*(model.weight**2).sum() + C*hinge_loss(s_val, yva)).item()\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        if ep % 5 == 0 or ep == 1:\n",
    "            print(f\"[ep{ep:02d}] train_loss={total/N:.6f} val_obj={val_loss:.6f}\")\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        s_val = model(Xva).squeeze(1).detach().cpu().numpy()\n",
    "        s_te  = model(Xte).squeeze(1).detach().cpu().numpy()\n",
    "\n",
    "    evaluate_scores(y_val,  s_val, \"val\")\n",
    "    evaluate_scores(y_test, s_te,  \"test\")\n",
    "\n",
    "    # 儲存（Torch 權重）\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"D\": int(D), \"C\": C},\n",
    "               \"/home/yaya/ai-detect-proj/Script/saved_models/clip_svm_gpu_torch.pt\")\n",
    "    print(\"✅ saved: saved_models/clip_svm_gpu_torch.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88cfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PRNU(int8) → Fast CNN (speed-optimized) =====\n",
    "from pathlib import Path\n",
    "import os, json, random, math, time\n",
    "import numpy as np\n",
    "\n",
    "# tqdm（Notebook 友善）\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm.auto import tqdm\n",
    "TQDM_KW = dict(dynamic_ncols=True, leave=False)\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "FEA_ROOT    = SCRIPT_ROOT / \"features_256\"\n",
    "REAL_DIR    = FEA_ROOT / \"prnu_real_npy\"\n",
    "FAKE_DIR    = FEA_ROOT / \"prnu_fake_npy\"\n",
    "SPLIT_JSON  = SCRIPT_ROOT / \"splits/combined_split.json\"  # 若不存在自動隨機切\n",
    "SAVE_DIR    = SCRIPT_ROOT / \"saved_models\"; SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH   = SAVE_DIR / \"prnu_fastcnn_u8_best.pt\"\n",
    "\n",
    "# 資料與訓練\n",
    "N_PER_CLASS = 10000         # 每類最多取多少（你的情境：各 2000）\n",
    "BATCH       = 64          # 4060 建議 64~128，OOM 就降\n",
    "EPOCHS      = 15\n",
    "LR          = 2e-3\n",
    "WEIGHT_DECAY= 1e-4\n",
    "EARLY_STOP  = 5\n",
    "\n",
    "# DataLoader\n",
    "NUM_WORKERS     = min(8, os.cpu_count() or 4)\n",
    "PIN_MEMORY      = True\n",
    "PREFETCH_FACTOR = 4\n",
    "PERSISTENT      = True\n",
    "\n",
    "# Dataset 快取策略：\"ram\" | \"memmap\" | None\n",
    "CACHE_MODE = \"ram\"         # 4k 張 * 64KB ≈ 256MB，RAM 完全可承受 → 最快\n",
    "\n",
    "# ---------------- 檔案列表 & splits ----------------\n",
    "def list_npy(d: Path):\n",
    "    assert d.exists(), f\"Not found: {d}\"\n",
    "    return sorted([p for p in d.glob(\"*.npy\")])\n",
    "\n",
    "real_files = list_npy(REAL_DIR)\n",
    "fake_files = list_npy(FAKE_DIR)\n",
    "assert real_files and fake_files, \"找不到 PRNU 特徵 .npy，請先完成特徵抽取。\"\n",
    "\n",
    "def fid(p: Path): return p.stem\n",
    "id2path = {\"real\": {fid(p): p for p in real_files},\n",
    "           \"fake\": {fid(p): p for p in fake_files}}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "def intersect_ids(need_ids, pool_dict): return [i for i in need_ids if i in pool_dict]\n",
    "\n",
    "def make_splits():\n",
    "    if SPLIT_JSON.exists():\n",
    "        js  = json.loads(SPLIT_JSON.read_text())\n",
    "        ids = js[\"ids\"]\n",
    "        def pick(sp, cls):\n",
    "            arr = intersect_ids(ids[sp], id2path[cls])\n",
    "            if N_PER_CLASS is not None:\n",
    "                arr = arr[:min(len(arr), N_PER_CLASS)]\n",
    "            return arr\n",
    "        splits = {\n",
    "            \"train\": [(i,0) for i in pick(\"train\",\"real\")] + [(i,1) for i in pick(\"train\",\"fake\")],\n",
    "            \"val\":   [(i,0) for i in pick(\"val\",\"real\")]   + [(i,1) for i in pick(\"val\",\"fake\")],\n",
    "            \"test\":  [(i,0) for i in pick(\"test\",\"real\")]  + [(i,1) for i in pick(\"test\",\"fake\")],\n",
    "        }\n",
    "        print(f\"使用 split.json：{SPLIT_JSON}\")\n",
    "    else:\n",
    "        real_ids = list(id2path[\"real\"].keys())\n",
    "        fake_ids = list(id2path[\"fake\"].keys())\n",
    "        random.shuffle(real_ids); random.shuffle(fake_ids)\n",
    "        if N_PER_CLASS is not None:\n",
    "            real_ids = real_ids[:min(len(real_ids), N_PER_CLASS)]\n",
    "            fake_ids = fake_ids[:min(len(fake_ids), N_PER_CLASS)]\n",
    "        r_tr, r_tmp = train_test_split(real_ids, test_size=0.2, random_state=SEED)\n",
    "        f_tr, f_tmp = train_test_split(fake_ids, test_size=0.2, random_state=SEED)\n",
    "        r_va, r_te  = train_test_split(r_tmp, test_size=0.5, random_state=SEED)  # 0.1/0.1\n",
    "        f_va, f_te  = train_test_split(f_tmp, test_size=0.5, random_state=SEED)\n",
    "        splits = {\n",
    "            \"train\": [(i,0) for i in r_tr] + [(i,1) for i in f_tr],\n",
    "            \"val\":   [(i,0) for i in r_va] + [(i,1) for i in f_va],\n",
    "            \"test\":  [(i,0) for i in r_te] + [(i,1) for i in f_te],\n",
    "        }\n",
    "        print(\"使用隨機切分（無 split.json）\")\n",
    "    for sp in splits:\n",
    "        random.shuffle(splits[sp])\n",
    "        n0 = sum(1 for _,y in splits[sp] if y==0); n1 = len(splits[sp])-n0\n",
    "        print(f\"{sp}: total={len(splits[sp])} | real={n0} fake={n1}\")\n",
    "    return splits\n",
    "\n",
    "splits = make_splits()\n",
    "\n",
    "# ---------------- Dataset / DataLoader ----------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PRNUNPY(Dataset):\n",
    "    \"\"\"\n",
    "    PRNU int8 -> float32[-1,1]，per-sample 去均值；可 RAM 快取或 memmap。\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, id2path, augment=False, cache_mode=\"ram\"):\n",
    "        self.pairs = pairs\n",
    "        self.id2p  = id2path\n",
    "        self.augment = augment\n",
    "        self.cache_mode = cache_mode\n",
    "        self.cache = []\n",
    "\n",
    "        if cache_mode == \"ram\":\n",
    "            self.cache = [None]*len(pairs)\n",
    "            for i, (idx, lab) in enumerate(pairs):\n",
    "                d = \"real\" if lab==0 else \"fake\"\n",
    "                p = self.id2p[d][idx]\n",
    "                arr = np.load(p, allow_pickle=False)     # int8\n",
    "                self.cache[i] = arr.copy()\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx, lab = self.pairs[i]\n",
    "        if self.cache_mode == \"ram\":\n",
    "            arr = self.cache[i]\n",
    "        else:\n",
    "            d = \"real\" if lab==0 else \"fake\"\n",
    "            p = self.id2p[d][idx]\n",
    "            if self.cache_mode == \"memmap\":\n",
    "                arr = np.load(p, allow_pickle=False, mmap_mode='r')\n",
    "            else:\n",
    "                arr = np.load(p, allow_pickle=False)\n",
    "\n",
    "        x = (arr.astype(np.float32) / 127.0)\n",
    "        x = x - x.mean()\n",
    "        # 如需更快，可註解掉任何增強\n",
    "        if self.augment:\n",
    "            # x += np.random.normal(0.0, 0.02, size=x.shape).astype(np.float32)\n",
    "            pass\n",
    "        x = np.clip(x, -2.0, 2.0)\n",
    "        x = x[None, ...]  # [1,H,W]\n",
    "        return torch.from_numpy(x), torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "def make_loader(pairs, train=False):\n",
    "    ds = PRNUNPY(pairs, id2path, augment=train, cache_mode=CACHE_MODE)\n",
    "    return DataLoader(\n",
    "        ds, batch_size=BATCH, shuffle=train,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        prefetch_factor=PREFETCH_FACTOR, persistent_workers=PERSISTENT,\n",
    "        drop_last=train\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(splits[\"train\"], train=True)\n",
    "val_loader   = make_loader(splits[\"val\"],   train=False)\n",
    "test_loader  = make_loader(splits[\"test\"],  train=False)\n",
    "\n",
    "# ---------------- Fast CNN 模型 ----------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DSBlock(nn.Module):\n",
    "    \"\"\"Depthwise-Separable Conv：DW 3x3 + BN + ReLU → PW 1x1 + BN + ReLU\"\"\"\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(c_in, c_in, 3, stride=stride, padding=1, groups=c_in, bias=False)\n",
    "        self.bn1= nn.BatchNorm2d(c_in)\n",
    "        self.pw = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn2= nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.dw(x)))\n",
    "        x = self.act(self.bn2(self.pw(x)))\n",
    "        return x\n",
    "\n",
    "class PRNUFastCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1×256×256 → (stem 32) → DS(64,s=1) → DS(128,s=2) → DS(128,s=1)\n",
    "                 → DS(256,s=2) → DS(256,s=1) → GAP → FC\n",
    "    約 0.6M 參數，吞吐快，對紋理有效。\n",
    "    \"\"\"\n",
    "    def __init__(self, nc=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(nc, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.stage = nn.Sequential(\n",
    "            DSBlock(32, 64,  stride=1),   # 256\n",
    "            DSBlock(64, 128, stride=2),   # 128\n",
    "            DSBlock(128,128, stride=1),   # 128\n",
    "            DSBlock(128,256, stride=2),   # 64\n",
    "            DSBlock(256,256, stride=1),   # 64\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc   = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage(x)\n",
    "        x = self.head(x)\n",
    "        x = self.pool(x).squeeze(-1).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------- 訓練 & 評估 ----------------\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, logits = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            out = model(x)\n",
    "            logits.append(out.detach().cpu().numpy())\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    y_true = np.concatenate(ys, 0)\n",
    "    y_pred = logits.argmax(1)\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    prob1 = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[:,1]\n",
    "    auc = roc_auc_score(y_true, prob1)\n",
    "    return acc, auc, y_true, y_pred, prob1\n",
    "\n",
    "def train_prnu_fastcnn():\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    model  = PRNUFastCNN().to(device)\n",
    "    model  = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # bf16 > fp16（Ada 支援 bf16）\n",
    "    AMP_DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    USE_SCALER = (AMP_DTYPE == torch.float16)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_SCALER)\n",
    "\n",
    "    # 類別權重（避免不平衡）\n",
    "    n0 = sum(1 for _,y in splits[\"train\"] if y==0); n1 = len(splits[\"train\"])-n0\n",
    "    w = torch.tensor([1.0/n0, 1.0/n1], dtype=torch.float32, device=device)\n",
    "    w = w / w.mean()\n",
    "\n",
    "    crit  = nn.CrossEntropyLoss(weight=w)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCHS)\n",
    "\n",
    "    best_auc, best_state, no_improve = -1.0, None, 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(train_loader, desc=f\"train ep{ep}\", **TQDM_KW)\n",
    "        for x,y in pbar:\n",
    "            x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(dtype=AMP_DTYPE):\n",
    "                out  = model(x)\n",
    "                loss = crit(out, y)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if USE_SCALER:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=f\"{np.mean(losses):.4f}\")\n",
    "        sched.step()\n",
    "\n",
    "        # 驗證\n",
    "        val_acc, val_auc, *_ = evaluate(model, val_loader, device)\n",
    "        print(f\"[EP {ep:02d}] train_loss={np.mean(losses):.4f} | val acc={val_acc:.4f} auc={val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {\"model\": model.state_dict(),\n",
    "                          \"meta\": {\"arch\":\"PRNUFastCNN\",\"seed\":SEED,\"epochs_done\":ep,\n",
    "                                   \"val_auc\":float(val_auc),\"val_acc\":float(val_acc),\n",
    "                                   \"input\":\"PRNU int8 → float32/127, zero-mean\",\n",
    "                                   \"shape\":[1,256,256]}}\n",
    "            torch.save(best_state, BEST_PATH)\n",
    "            print(\"  ↳ saved best:\", BEST_PATH)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= EARLY_STOP:\n",
    "                print(f\"⏹ Early stop (no AUC improvement {EARLY_STOP} epochs).\")\n",
    "                break\n",
    "\n",
    "    # 載入最佳權重並在 test 評估\n",
    "    if best_state is None and BEST_PATH.exists():\n",
    "        best_state = torch.load(BEST_PATH, map_location=\"cpu\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "\n",
    "    test_acc, test_auc, y_true, y_pred, prob1 = evaluate(model, test_loader, device)\n",
    "    print(f\"[TEST] acc={test_acc:.4f} auc={test_auc:.4f}\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"real\",\"fake\"], digits=4))\n",
    "    return model\n",
    "\n",
    "model = train_prnu_fastcnn()\n",
    "\n",
    "# ---- 單張 .npy 推論 ----\n",
    "def predict_prnu_npy(npy_path: Path):\n",
    "    arr = np.load(npy_path, allow_pickle=False).astype(np.float32)\n",
    "    x = (arr/127.0); x = x - x.mean()\n",
    "    x = torch.from_numpy(x[None, None, ...])\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16):\n",
    "        logit = model(x.to(device).contiguous(memory_format=torch.channels_last))\n",
    "        prob  = torch.softmax(logit, dim=1)[0,1].item()\n",
    "        pred  = int(prob >= 0.5)  # 1=fake, 0=real\n",
    "    return pred, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16163d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ELA(int8) → Fast CNN（不使用 split 檔，隨機切 8:1:1）=====\n",
    "from pathlib import Path\n",
    "import os, random, re, json\n",
    "import numpy as np\n",
    "\n",
    "# tqdm（Notebook 友善）\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm.auto import tqdm\n",
    "TQDM_KW = dict(dynamic_ncols=True, leave=False)\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "FEA_ROOT    = SCRIPT_ROOT / \"features_npy\"\n",
    "ELA_REAL    = FEA_ROOT / \"ela_real_npy\"\n",
    "ELA_FAKE    = FEA_ROOT / \"ela_fake_npy\"\n",
    "\n",
    "SAVE_DIR    = SCRIPT_ROOT / \"saved_models\"; SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH   = SAVE_DIR / \"ela_fastcnn_u8_best.pt\"\n",
    "\n",
    "# 每類取多少；None = 全部可用\n",
    "N_PER_CLASS = 10000\n",
    "\n",
    "# 訓練設定\n",
    "BATCH       = 64          # 4060 上 64~128 較穩；OOM 就降\n",
    "EPOCHS      = 15\n",
    "LR          = 2e-3\n",
    "WEIGHT_DECAY= 1e-4\n",
    "EARLY_STOP  = 5\n",
    "\n",
    "# DataLoader\n",
    "NUM_WORKERS     = min(8, os.cpu_count() or 4)\n",
    "PIN_MEMORY      = True\n",
    "PREFETCH_FACTOR = 4\n",
    "PERSISTENT      = True\n",
    "\n",
    "# Dataset 快取：\"ram\" | \"memmap\" | None\n",
    "CACHE_MODE = \"ram\"\n",
    "\n",
    "# ELA 正規化：i8(-128..127) → (i8+128)/255 ∈ [0,1]，再做零均值\n",
    "ELA_ZERO_MEAN = True\n",
    "\n",
    "# ---------------- 掃檔（支援 __qXX，優先 q90） ----------------\n",
    "def list_npy(d: Path):\n",
    "    assert d.exists(), f\"Not found: {d}\"\n",
    "    return sorted([p for p in d.glob(\"*.npy\")])\n",
    "\n",
    "_q_pat = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id_from_stem(stem: str):\n",
    "    m = _q_pat.search(stem); return stem[:m.start()] if m else stem\n",
    "def quality_from_stem(stem: str):\n",
    "    m = _q_pat.search(stem); return int(m.group(1)) if m else None\n",
    "\n",
    "def build_id2path(files):\n",
    "    buckets = {}\n",
    "    for p in files:\n",
    "        b = base_id_from_stem(p.stem)\n",
    "        buckets.setdefault(b, []).append(p)\n",
    "    id2path = {}\n",
    "    for b, ps in buckets.items():\n",
    "        if len(ps) == 1:\n",
    "            id2path[b] = ps[0]\n",
    "        else:\n",
    "            # 優先 q90，其次挑離 90 最近\n",
    "            scored = []\n",
    "            for pp in ps:\n",
    "                q = quality_from_stem(pp.stem)\n",
    "                scored.append((0 if q == 90 else (abs(q-90) if q is not None else 999), pp))\n",
    "            scored.sort(key=lambda x: (x[0], str(x[1])))\n",
    "            id2path[b] = scored[0][1]\n",
    "    return id2path\n",
    "\n",
    "real_files = list_npy(ELA_REAL)\n",
    "fake_files = list_npy(ELA_FAKE)\n",
    "assert real_files and fake_files, \"找不到 ELA 特徵 .npy，請先完成特徵抽取。\"\n",
    "\n",
    "id2path = {\n",
    "    \"real\": build_id2path(real_files),\n",
    "    \"fake\": build_id2path(fake_files),\n",
    "}\n",
    "\n",
    "# ---------------- 隨機切 8:1:1（不使用 split 檔） ----------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cap(ids, k):\n",
    "    ids = list(ids); random.shuffle(ids)\n",
    "    return ids if (k is None or len(ids) <= k) else ids[:k]\n",
    "\n",
    "real_ids = cap(list(id2path[\"real\"].keys()), N_PER_CLASS)\n",
    "fake_ids = cap(list(id2path[\"fake\"].keys()), N_PER_CLASS)\n",
    "\n",
    "r_tr, r_tmp = train_test_split(real_ids, test_size=0.2, random_state=SEED)\n",
    "f_tr, f_tmp = train_test_split(fake_ids, test_size=0.2, random_state=SEED)\n",
    "r_va, r_te  = train_test_split(r_tmp, test_size=0.5, random_state=SEED)  # 0.1/0.1\n",
    "f_va, f_te  = train_test_split(f_tmp, test_size=0.5, random_state=SEED)\n",
    "\n",
    "splits = {\n",
    "    \"train\": [(i,0) for i in r_tr] + [(i,1) for i in f_tr],\n",
    "    \"val\":   [(i,0) for i in r_va] + [(i,1) for i in f_va],\n",
    "    \"test\":  [(i,0) for i in r_te] + [(i,1) for i in f_te],\n",
    "}\n",
    "for sp in splits:\n",
    "    random.shuffle(splits[sp])\n",
    "    n0 = sum(1 for _,y in splits[sp] if y==0); n1 = len(splits[sp])-n0\n",
    "    print(f\"{sp}: total={len(splits[sp])} | real={n0} fake={n1}\")\n",
    "\n",
    "# ---------------- Dataset / DataLoader ----------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ELAFromNPY(Dataset):\n",
    "    \"\"\"ELA int8(u8-128) → float32；(i8+128)/255 ∈ [0,1]，可選 zero-mean；RAM/memmap 快取。\"\"\"\n",
    "    def __init__(self, pairs, id2path, augment=False, cache_mode=\"ram\"):\n",
    "        self.pairs = pairs\n",
    "        self.id2p  = id2path\n",
    "        self.augment = augment\n",
    "        self.cache_mode = cache_mode\n",
    "        self.cache = []\n",
    "        if cache_mode == \"ram\":\n",
    "            self.cache = [None]*len(pairs)\n",
    "            for i, (idx, lab) in enumerate(pairs):\n",
    "                d = \"real\" if lab==0 else \"fake\"\n",
    "                p = self.id2p[d][idx]\n",
    "                self.cache[i] = np.load(p, allow_pickle=False).copy()  # int8\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx, lab = self.pairs[i]\n",
    "        if self.cache_mode == \"ram\":\n",
    "            arr = self.cache[i]\n",
    "        else:\n",
    "            d = \"real\" if lab==0 else \"fake\"\n",
    "            p = self.id2p[d][idx]\n",
    "            arr = np.load(p, allow_pickle=False, mmap_mode='r' if self.cache_mode==\"memmap\" else None)\n",
    "\n",
    "        x = (arr.astype(np.float32) + 128.0) / 255.0\n",
    "        if ELA_ZERO_MEAN:\n",
    "            x = x - x.mean()\n",
    "        x = np.clip(x, -2.0, 2.0)\n",
    "        x = x[None, ...]  # [1,H,W]\n",
    "        return torch.from_numpy(x), torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "def make_loader(pairs, train=False):\n",
    "    ds = ELAFromNPY(pairs, id2path, augment=train, cache_mode=CACHE_MODE)\n",
    "    return DataLoader(\n",
    "        ds, batch_size=BATCH, shuffle=train,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        prefetch_factor=PREFETCH_FACTOR, persistent_workers=PERSISTENT,\n",
    "        drop_last=train\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(splits[\"train\"], train=True)\n",
    "val_loader   = make_loader(splits[\"val\"],   train=False)\n",
    "test_loader  = make_loader(splits[\"test\"],  train=False)\n",
    "\n",
    "# ---------------- 模型：Depthwise-Separable CNN（快） ----------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DSBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(c_in, c_in, 3, stride=stride, padding=1, groups=c_in, bias=False)\n",
    "        self.bn1= nn.BatchNorm2d(c_in)\n",
    "        self.pw = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn2= nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.dw(x)))\n",
    "        x = self.act(self.bn2(self.pw(x)))\n",
    "        return x\n",
    "\n",
    "class ELAFastCNN(nn.Module):\n",
    "    def __init__(self, nc=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(nc, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.stage = nn.Sequential(\n",
    "            DSBlock(32, 64,  stride=1),   # H\n",
    "            DSBlock(64, 128, stride=2),   # H/2\n",
    "            DSBlock(128,128, stride=1),   # H/2\n",
    "            DSBlock(128,256, stride=2),   # H/4\n",
    "            DSBlock(256,256, stride=1),   # H/4\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc   = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x); x = self.stage(x); x = self.head(x)\n",
    "        x = self.pool(x).squeeze(-1).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------- 訓練 & 評估 ----------------\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, logits = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            out = model(x)\n",
    "            logits.append(out.detach().cpu().numpy())\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    y_true = np.concatenate(ys, 0)\n",
    "    y_pred = logits.argmax(1)\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    prob1 = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[:,1]\n",
    "    auc = roc_auc_score(y_true, prob1)\n",
    "    return acc, auc, y_true, y_pred, prob1\n",
    "\n",
    "import torch\n",
    "def train_ela_fastcnn():\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    model  = ELAFastCNN().to(device)\n",
    "    model  = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    AMP_DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    USE_SCALER = (AMP_DTYPE == torch.float16)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_SCALER)\n",
    "\n",
    "    # 類別權重\n",
    "    n0 = sum(1 for _,y in splits[\"train\"] if y==0); n1 = len(splits[\"train\"])-n0\n",
    "    w = torch.tensor([1.0/n0, 1.0/n1], dtype=torch.float32, device=device); w = w/w.mean()\n",
    "\n",
    "    crit  = torch.nn.CrossEntropyLoss(weight=w)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCHS)\n",
    "\n",
    "    best_auc, best_state, no_improve = -1.0, None, 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); losses = []\n",
    "        pbar = tqdm(train_loader, desc=f\"train ep{ep}\", **TQDM_KW)\n",
    "        for x,y in pbar:\n",
    "            x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(dtype=AMP_DTYPE):\n",
    "                out  = model(x)\n",
    "                loss = crit(out, y)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if USE_SCALER:\n",
    "                scaler.scale(loss).backward(); scaler.step(optim); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); optim.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=f\"{np.mean(losses):.4f}\")\n",
    "        sched.step()\n",
    "\n",
    "        val_acc, val_auc, *_ = evaluate(model, val_loader, device)\n",
    "        print(f\"[EP {ep:02d}] train_loss={np.mean(losses):.4f} | val acc={val_acc:.4f} auc={val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {\"model\": model.state_dict(),\n",
    "                          \"meta\": {\"arch\":\"ELAFastCNN\",\"seed\":SEED,\"epochs_done\":ep,\n",
    "                                   \"val_auc\":float(val_auc),\"val_acc\":float(val_acc),\n",
    "                                   \"input\":\"ELA int8→(i8+128)/255\"+(\"→zero-mean\" if ELA_ZERO_MEAN else \"\"),\n",
    "                                   \"shape\":\"[1,H,W]\"}}\n",
    "            torch.save(best_state, BEST_PATH); print(\"  ↳ saved best:\", BEST_PATH); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= EARLY_STOP:\n",
    "                print(f\"⏹ Early stop (no AUC improvement {EARLY_STOP} epochs).\"); break\n",
    "\n",
    "    if best_state is None and BEST_PATH.exists():\n",
    "        best_state = torch.load(BEST_PATH, map_location=\"cpu\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "\n",
    "    test_acc, test_auc, y_true, y_pred, prob1 = evaluate(model, test_loader, device)\n",
    "    print(f\"[TEST] acc={test_acc:.4f} auc={test_auc:.4f}\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"real\",\"fake\"], digits=4))\n",
    "    return model\n",
    "\n",
    "model = train_ela_fastcnn()\n",
    "\n",
    "# ---- 單張 .npy 推論 ----\n",
    "def predict_ela_npy(npy_path: Path):\n",
    "    arr = np.load(npy_path, allow_pickle=False).astype(np.float32)\n",
    "    x = (arr + 128.0) / 255.0\n",
    "    if ELA_ZERO_MEAN: x = x - x.mean()\n",
    "    x = torch.from_numpy(x[None, None, ...])\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16):\n",
    "        logit = model(x.to(device).contiguous(memory_format=torch.channels_last))\n",
    "        prob  = torch.softmax(logit, dim=1)[0,1].item()\n",
    "        pred  = int(prob >= 0.5)  # 1=fake, 0=real\n",
    "    return pred, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Export CLIP pooled(1024) per-split → .npy (memmap) ================\n",
    "# 不做訓練，只把資料準備好。支援：\n",
    "#  - 量化 .npz：優先讀 'pooled'；沒有就動態還原再 pool\n",
    "#  - 原始 .npy：形狀 [257,1024] 或 [1024]\n",
    "# 產物：exports/clip_pooled/<split>/{X.npy,y.npy,ids.txt,meta.json}\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, random, numpy as np\n",
    "\n",
    "# tqdm（console 版，避免 IProgress）\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kw): return x\n",
    "\n",
    "# ---------------- Config（改這裡） ----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "SPLIT_JSON  = SCRIPT_ROOT / \"splits/combined_split.json\"\n",
    "SPLIT_KEY   = \"iid\"          # \"iid\" / \"smoke_10p\" / \"ood_gen_strict.sd3\" ...\n",
    "EXPORT_ROOT = SCRIPT_ROOT / \"exports/clip_pooled\"  # 輸出根目錄\n",
    "\n",
    "# 特徵來源（兩種都會掃，.npz 優先）\n",
    "CLIP_REAL_DIRS = [\n",
    "    SCRIPT_ROOT / \"features_256q/clip_real_q\",   # 量化\n",
    "    SCRIPT_ROOT / \"features_256/clip_real_npy\",  # 原始\n",
    "]\n",
    "CLIP_FAKE_DIRS = [\n",
    "    SCRIPT_ROOT / \"features_256q/clip_fake_q\",\n",
    "    SCRIPT_ROOT / \"features_256/clip_fake_npy\",\n",
    "]\n",
    "\n",
    "SAVE_DTYPE   = \"float32\"   # 輸出向量精度：'float32'（訓練友善）或 'float16'（更省空間）\n",
    "SMOKE_FRAC   = None        # 若想只先匯出部分（例如 0.1 為 10%），預設 None=全量\n",
    "\n",
    "# ---------------- 掃檔 → id→path（.npz 優先） ----------------\n",
    "def scan_first_hit(dirs, exts=(\".npz\",\".npy\")):\n",
    "    lut = {}\n",
    "    for d in dirs:\n",
    "        if not d.exists(): continue\n",
    "        for ext in exts:\n",
    "            for p in sorted(d.glob(f\"*.{ext.lstrip('.')}\")):\n",
    "                k = p.stem\n",
    "                if k not in lut:\n",
    "                    lut[k] = p\n",
    "    return lut\n",
    "\n",
    "id2path = {\"real\": scan_first_hit(CLIP_REAL_DIRS),\n",
    "           \"fake\": scan_first_hit(CLIP_FAKE_DIRS)}\n",
    "assert id2path[\"real\"] and id2path[\"fake\"], \"❌ 找不到 CLIP 檔案，請檢查路徑\"\n",
    "\n",
    "# ---------------- 讀 unified split（支援 dot-path） ----------------\n",
    "def load_split_ids(json_path: Path, split_key: str):\n",
    "    data = json.loads(json_path.read_text())\n",
    "    node = data\n",
    "    if \"ids\" in node and split_key in (None,\"\",\"ids\"):\n",
    "        node = node[\"ids\"]\n",
    "    else:\n",
    "        for k in split_key.split(\".\"):\n",
    "            node = node[k]\n",
    "    assert all(k in node for k in (\"train\",\"val\",\"test\"))\n",
    "    return node[\"train\"], node[\"val\"], node[\"test\"]\n",
    "\n",
    "def attach(ids):\n",
    "    pairs, miss = [], 0\n",
    "    for i in ids:\n",
    "        if i in id2path[\"real\"]: pairs.append((i,0))\n",
    "        elif i in id2path[\"fake\"]: pairs.append((i,1))\n",
    "        else: miss += 1\n",
    "    if miss: print(f\"⚠️ split 有 {miss} 個 id 在磁碟找不到，已忽略。\")\n",
    "    return pairs\n",
    "\n",
    "def stratified_frac(pairs, frac, seed=SEED):\n",
    "    if not frac or frac>=1: return pairs\n",
    "    r = [(i,y) for (i,y) in pairs if y==0]\n",
    "    f = [(i,y) for (i,y) in pairs if y==1]\n",
    "    rnd = random.Random(seed); rnd.shuffle(r); rnd.shuffle(f)\n",
    "    return r[:max(1,int(len(r)*frac))] + f[:max(1,int(len(f)*frac))]\n",
    "\n",
    "tr_ids, va_ids, te_ids = load_split_ids(SPLIT_JSON, SPLIT_KEY)\n",
    "train_pairs, val_pairs, test_pairs = attach(tr_ids), attach(va_ids), attach(te_ids)\n",
    "if SMOKE_FRAC:\n",
    "    train_pairs = stratified_frac(train_pairs, SMOKE_FRAC, SEED)\n",
    "    val_pairs   = stratified_frac(val_pairs,   SMOKE_FRAC, SEED+1)\n",
    "    test_pairs  = stratified_frac(test_pairs,  SMOKE_FRAC, SEED+2)\n",
    "\n",
    "for name, pairs in [(\"train\",train_pairs),(\"val\",val_pairs),(\"test\",test_pairs)]:\n",
    "    n0 = sum(1 for _,y in pairs if y==0); n1 = len(pairs)-n0\n",
    "    print(f\"{name}: total={len(pairs)} | real={n0} fake={n1}\")\n",
    "\n",
    "# ---------------- 量化 .npz → 還原（如需） + pooling(mean_excl_cls) ----------------\n",
    "def _unpack_int4(packed, orig_size):\n",
    "    u = np.empty(orig_size + (orig_size % 2), dtype=np.uint8)\n",
    "    u[0::2] = packed & 0x0F\n",
    "    u[1::2] = (packed >> 4) & 0x0F\n",
    "    u = u[:orig_size]\n",
    "    return (u.astype(np.int16) - 8).astype(np.int8)\n",
    "\n",
    "def dequantize_npz(npz_path: Path):\n",
    "    z = np.load(npz_path, allow_pickle=False)\n",
    "    meta = json.loads(str(z[\"meta\"][()]))\n",
    "    mode = meta[\"mode\"]; shape = tuple(meta[\"shape\"])\n",
    "    if mode == \"fp16\":\n",
    "        return z[\"q\"].astype(np.float32).reshape(shape)\n",
    "    if \"int8\" in mode:\n",
    "        q = z[\"q\"].astype(np.int8)\n",
    "        if mode == \"int8_tensor\":\n",
    "            S = float(z[\"scales\"][0]); return (q.astype(np.float32)*S).reshape(shape)\n",
    "        if mode == \"int8_row\":\n",
    "            if len(shape)==1:\n",
    "                S = float(z[\"scales\"][0]); return (q.astype(np.float32)*S).reshape(shape)\n",
    "            T,D = shape; S = z[\"scales\"].astype(np.float32)\n",
    "            out = np.empty((T,D), np.float32)\n",
    "            for t in range(T): out[t] = q[t].astype(np.float32)*S[t]\n",
    "            return out\n",
    "        if mode == \"int8_block32\":\n",
    "            B = int(meta[\"block\"])\n",
    "            if len(shape)==1:\n",
    "                D = shape[0]; S = z[\"scales\"].astype(np.float32); out = np.empty((D,), np.float32)\n",
    "                nB = (D+B-1)//B\n",
    "                for b in range(nB):\n",
    "                    s = slice(b*B, min((b+1)*B,D)); out[s] = q[s].astype(np.float32)*S[b]\n",
    "                return out\n",
    "            else:\n",
    "                T,D = shape; S = z[\"scales\"].astype(np.float32); out = np.empty((T,D), np.float32)\n",
    "                nB = (D+B-1)//B\n",
    "                for t in range(T):\n",
    "                    for b in range(nB):\n",
    "                        s = slice(b*B, min((b+1)*B,D)); out[t,s] = q[t,s].astype(np.float32)*S[t,b]\n",
    "                return out\n",
    "    if \"int4\" in mode:\n",
    "        packed = z[\"q\"].astype(np.uint8); B = int(meta[\"block\"])\n",
    "        if len(shape)==1:\n",
    "            D = shape[0]; S = z[\"scales\"].astype(np.float32)\n",
    "            q = _unpack_int4(packed, D); out = np.empty((D,), np.float32)\n",
    "            nB = (D+B-1)//B\n",
    "            for b in range(nB):\n",
    "                s = slice(b*B, min((b+1)*B,D)); out[s] = q[s].astype(np.float32)*S[b]\n",
    "            return out\n",
    "        else:\n",
    "            T,D = shape; S = z[\"scales\"].astype(np.float32)\n",
    "            q = _unpack_int4(packed, T*D).reshape(T,D); out = np.empty((T,D), np.float32)\n",
    "            nB = (D+B-1)//B\n",
    "            for t in range(T):\n",
    "                for b in range(nB):\n",
    "                    s = slice(b*B, min((b+1)*B,D)); out[t,s] = q[t,s].astype(np.float32)*S[t,b]\n",
    "            return out\n",
    "    raise ValueError(f\"Unknown quant mode: {mode}\")\n",
    "\n",
    "def pooled_vec_from_file(p: Path):\n",
    "    if p.suffix == \".npz\":\n",
    "        z = np.load(p, allow_pickle=False)\n",
    "        if \"pooled\" in z and z[\"pooled\"].size > 0:   # 直接用預存 pooled（快）\n",
    "            v = z[\"pooled\"].astype(np.float32)\n",
    "        else:\n",
    "            X = dequantize_npz(p)                    # [T,D] 或 [D]\n",
    "            v = X if X.ndim==1 else X[1:].mean(axis=0)   # mean_excl_cls\n",
    "    else:\n",
    "        arr = np.load(p, allow_pickle=False)\n",
    "        v = arr.astype(np.float32) if arr.ndim==1 else arr[1:].astype(np.float32).mean(axis=0)\n",
    "    # L2 normalize\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "# ---------------- 輸出工具：.npy（open_memmap，邊寫邊落盤） ----------------\n",
    "def export_split(name, pairs):\n",
    "    if not pairs:\n",
    "        print(f\"{name}: 無資料，略過\"); return\n",
    "    out_dir = EXPORT_ROOT / SPLIT_KEY / name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 先 peek 一個向量決定 D\n",
    "    iid0, y0 = pairs[0]\n",
    "    p0 = id2path[\"real\" if y0==0 else \"fake\"][iid0]\n",
    "    v0 = pooled_vec_from_file(p0)\n",
    "    D = v0.shape[0]\n",
    "\n",
    "    # 建立 memmap .npy\n",
    "    from numpy.lib.format import open_memmap\n",
    "    X_path = out_dir / \"X.npy\"\n",
    "    X_mm   = open_memmap(X_path, mode=\"w+\", dtype=SAVE_DTYPE, shape=(len(pairs), D))\n",
    "    y_path = out_dir / \"y.npy\"\n",
    "    y_arr  = np.empty((len(pairs),), dtype=np.int32)\n",
    "\n",
    "    ids_txt = (out_dir / \"ids.txt\").open(\"w\")\n",
    "    print(f\"→ Export {name}: N={len(pairs)} D={D} → {X_path}\")\n",
    "\n",
    "    # 第 0 筆\n",
    "    X_mm[0] = v0.astype(SAVE_DTYPE, copy=False)\n",
    "    y_arr[0] = y0\n",
    "    ids_txt.write(f\"{iid0}\\t{y0}\\t{p0}\\n\")\n",
    "\n",
    "    # 其餘\n",
    "    for i,(iid,lab) in enumerate(tqdm(pairs[1:], total=len(pairs)-1, desc=f\"build {name}\")):\n",
    "        p = id2path[\"real\" if lab==0 else \"fake\"][iid]\n",
    "        v = pooled_vec_from_file(p)\n",
    "        X_mm[i+1] = v.astype(SAVE_DTYPE, copy=False)\n",
    "        y_arr[i+1] = lab\n",
    "        if i < 10:  # 前幾筆留下路徑方便除錯\n",
    "            ids_txt.write(f\"{iid}\\t{lab}\\t{p}\\n\")\n",
    "\n",
    "    # 寫出 y / meta\n",
    "    np.save(y_path, y_arr, allow_pickle=False)\n",
    "    meta = {\n",
    "        \"split_key\": SPLIT_KEY,\n",
    "        \"split\": name,\n",
    "        \"dtype\": SAVE_DTYPE,\n",
    "        \"N\": int(len(pairs)),\n",
    "        \"D\": int(D),\n",
    "        \"pool\": \"mean_excl_cls\",\n",
    "        \"l2norm\": True,\n",
    "        \"source_priority\": [str(d) for d in (CLIP_REAL_DIRS+CLIP_FAKE_DIRS)],\n",
    "    }\n",
    "    (out_dir / \"meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "    ids_txt.close()\n",
    "\n",
    "    # 小總結\n",
    "    szX = X_path.stat().st_size; szy = y_path.stat().st_size\n",
    "    def human(n): \n",
    "        u=[\"B\",\"KiB\",\"MiB\",\"GiB\",\"TiB\"]; i=0; f=float(n)\n",
    "        while f>=1024 and i<len(u)-1: f/=1024; i+=1\n",
    "        return f\"{f:.1f} {u[i]}\"\n",
    "    print(f\"{name} saved: X={human(szX)} y={human(szy)} → {out_dir}\")\n",
    "\n",
    "# ---------------- Run：逐 split 匯出 ----------------\n",
    "export_split(\"train\", train_pairs)\n",
    "export_split(\"val\",   val_pairs)\n",
    "export_split(\"test\",  test_pairs)\n",
    "\n",
    "print(\"✅ Done. Exports at:\", EXPORT_ROOT / SPLIT_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Step 1 + 2: 原始 CLIP 池化 → per-file；三模態交集 → 共用 splits =====================\n",
    "from pathlib import Path\n",
    "import json, random, time, re, numpy as np\n",
    "\n",
    "# ---- 基本設定（改這裡）----\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "FEA_ROOT    = SCRIPT_ROOT / \"features_256\"\n",
    "\n",
    "# 1) 原始 CLIP token map 的來源（每圖一檔 .npy）\n",
    "CLIP_RAW_REAL = FEA_ROOT / \"clip_real_npy\"\n",
    "CLIP_RAW_FAKE = FEA_ROOT / \"clip_fake_npy\"\n",
    "\n",
    "# 1) 池化後要存到哪（每圖一檔 .npy，float32 1024 維）\n",
    "CLIP_POOL_REAL = FEA_ROOT / \"clip_pooled_real_npy\"\n",
    "CLIP_POOL_FAKE = FEA_ROOT / \"clip_pooled_fake_npy\"\n",
    "\n",
    "# PRNU/ELA 來源（每圖一檔 .npy）\n",
    "PRNU_REAL = FEA_ROOT / \"prnu_real_npy\"\n",
    "PRNU_FAKE = FEA_ROOT / \"prnu_fake_npy\"\n",
    "ELA_REAL  = FEA_ROOT / \"ela_real_npy\"\n",
    "ELA_FAKE  = FEA_ROOT / \"ela_fake_npy\"\n",
    "\n",
    "# 2) Split 存放位置與名稱\n",
    "SPLIT_OUT   = SCRIPT_ROOT / \"splits\" / \"combined_split.json\"\n",
    "SMOKE_FRAC  = 0.10           # 10% smoke\n",
    "IID_RATIO   = (0.8, 0.1, 0.1)  # train/val/test\n",
    "\n",
    "# 要做 OOD 的假圖生成器代號（從檔名 stem 的前綴推斷）\n",
    "GEN_CANON = {\n",
    "    \"sd3\": [\"sd3\"],\n",
    "    \"midjourney\": [\"midjourney\", \"midjourney-v6\", \"midjourney-v6-llava\", \"mj\"],\n",
    "    \"flux\": [\"flux\", \"black-forest-labs\", \"flux-dev\", \"flux-1\"],\n",
    "    \"dalle3\": [\"dalle3\", \"dall-e-3\", \"dalle-3\"]\n",
    "}\n",
    "\n",
    "# ===================== 工具 =====================\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kw): return x\n",
    "\n",
    "def ensure_dir(d: Path):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_files(d: Path, ext=\".npy\"):\n",
    "    return sorted([p for p in d.glob(f\"*{ext}\")])\n",
    "\n",
    "def stem_set(d: Path, ext=\".npy\"):\n",
    "    return set(p.stem for p in d.glob(f\"*{ext}\"))\n",
    "\n",
    "def pool_clip_file(npy_path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    載入原始 CLIP 特徵：\n",
    "      - 若 shape=(257,1024)：採 mean_excl_cls（排除 CLS）→ 1024\n",
    "      - 若 shape=(1024,)   ：直接視為 pooled\n",
    "    之後做 L2 normalize，回傳 float32[1024]\n",
    "    \"\"\"\n",
    "    arr = np.load(npy_path, allow_pickle=False)\n",
    "    if arr.ndim == 2 and arr.shape[1] == 1024:\n",
    "        v = arr[1:].astype(np.float32).mean(axis=0)\n",
    "    elif arr.ndim == 1 and arr.shape[0] == 1024:\n",
    "        v = arr.astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported CLIP shape {arr.shape} @ {npy_path.name}\")\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "def atomic_save(path: Path, arr: np.ndarray):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    np.save(tmp, arr, allow_pickle=False)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def pool_all_clip(src_dir: Path, dst_dir: Path, limit=None):\n",
    "    ensure_dir(dst_dir)\n",
    "    files = list_files(src_dir, \".npy\")\n",
    "    if limit: files = files[:limit]\n",
    "    n_ok, n_skip = 0, 0\n",
    "    for p in tqdm(files, desc=f\"pool {src_dir.name} → {dst_dir.name}\"):\n",
    "        out = dst_dir / (p.stem + \".npy\")\n",
    "        if out.exists(): \n",
    "            n_skip += 1\n",
    "            continue\n",
    "        try:\n",
    "            v = pool_clip_file(p)\n",
    "            atomic_save(out, v)\n",
    "            n_ok += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[pool] skip {p.name} | {e}\")\n",
    "    print(f\"→ {dst_dir}: wrote={n_ok} skipped={n_skip}\")\n",
    "\n",
    "def guess_generator(img_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    依檔名 stem 的前綴（__ 前面的 dataset 名）推斷生成器：\n",
    "      e.g. 'sd3__xxxx', 'midjourney-v6-llava__xxxx', 'flux__xxxx', 'dalle3__xxxx'\n",
    "    回傳 'sd3'/'midjourney'/'flux'/'dalle3' 或 None（其他）\n",
    "    \"\"\"\n",
    "    # 取第一段前綴\n",
    "    prefix = img_id.split(\"__\", 1)[0].lower()\n",
    "    # 移除非字母數字與 - _\n",
    "    prefix = re.sub(r\"[^a-z0-9\\-_]+\", \"\", prefix)\n",
    "    for canon, aliases in GEN_CANON.items():\n",
    "        for a in aliases:\n",
    "            if prefix.startswith(a):\n",
    "                return canon\n",
    "    return None\n",
    "\n",
    "def stratified_split(real_ids, fake_ids, ratios=(0.8,0.1,0.1), seed=SEED):\n",
    "    \"\"\"不依賴 sklearn 的分層切分，回傳 dict: {'train':[...], 'val':[...], 'test':[...]}\"\"\"\n",
    "    assert abs(sum(ratios)-1.0) < 1e-6 and len(ratios)==3\n",
    "    r = list(real_ids); f = list(fake_ids)\n",
    "    rnd = random.Random(seed)\n",
    "    rnd.shuffle(r); rnd.shuffle(f)\n",
    "    def cut(arr):\n",
    "        n = len(arr)\n",
    "        n_tr = int(round(n*ratios[0]))\n",
    "        n_va = int(round(n*ratios[1]))\n",
    "        n_te = n - n_tr - n_va\n",
    "        return arr[:n_tr], arr[n_tr:n_tr+n_va], arr[n_tr+n_va:]\n",
    "    r_tr, r_va, r_te = cut(r)\n",
    "    f_tr, f_va, f_te = cut(f)\n",
    "    return {\n",
    "        \"train\": r_tr + f_tr,\n",
    "        \"val\":   r_va + f_va,\n",
    "        \"test\":  r_te + f_te,\n",
    "    }\n",
    "\n",
    "def summary(name, ids_dict):\n",
    "    for sp in [\"train\",\"val\",\"test\"]:\n",
    "        ids = ids_dict[sp]\n",
    "        n0 = sum(1 for i in ids if \"__\" in i and not guess_generator(i))  # 估 real by no-gen? (僅供參考)\n",
    "    # 真正 summary 我們用路徑存在性判定\n",
    "\n",
    "# ===================== Step 1：原始 CLIP → 池化 per-file =====================\n",
    "print(\"== Step 1: Pool original CLIP to per-file 1024 ==\")\n",
    "assert CLIP_RAW_REAL.exists() and CLIP_RAW_FAKE.exists(), \"找不到原始 CLIP .npy 資料夾\"\n",
    "pool_all_clip(CLIP_RAW_REAL, CLIP_POOL_REAL)\n",
    "pool_all_clip(CLIP_RAW_FAKE, CLIP_POOL_FAKE)\n",
    "\n",
    "# ===================== Step 2：三模態取交集 → 產出 splits =====================\n",
    "print(\"\\n== Step 2: Build unified splits (IID / OOD / smoke_10p) with tri-modal intersection ==\")\n",
    "\n",
    "# 交集（必須三模態都有）\n",
    "ids_real = stem_set(CLIP_POOL_REAL) & stem_set(PRNU_REAL) & stem_set(ELA_REAL)\n",
    "ids_fake = stem_set(CLIP_POOL_FAKE) & stem_set(PRNU_FAKE) & stem_set(ELA_FAKE)\n",
    "print(f\"交集數量 → real: {len(ids_real)} | fake: {len(ids_fake)}\")\n",
    "\n",
    "# --- IID：8/1/1 分層切分 ---\n",
    "iid = stratified_split(sorted(ids_real), sorted(ids_fake), ratios=IID_RATIO, seed=SEED)\n",
    "\n",
    "def count_split(ids_dict, ids_real_all, ids_fake_all):\n",
    "    def lab(i):\n",
    "        return 0 if i in ids_real_all else 1\n",
    "    for sp in [\"train\",\"val\",\"test\"]:\n",
    "        ids = ids_dict[sp]\n",
    "        n0 = sum(1 for i in ids if i in ids_real_all)\n",
    "        n1 = len(ids) - n0\n",
    "        print(f\"[{sp}] total={len(ids)} | real={n0} fake={n1}\")\n",
    "\n",
    "print(\"== IID summary ==\")\n",
    "count_split(iid, ids_real, ids_fake)\n",
    "\n",
    "# --- OOD：對每個生成器 g，train/val 不含 g，test 全部 g（+ real 的 test） ---\n",
    "# 先把 fake 依生成器分桶\n",
    "fake_by_gen = {}\n",
    "for i in ids_fake:\n",
    "    g = guess_generator(i) or \"other\"\n",
    "    fake_by_gen.setdefault(g, []).append(i)\n",
    "\n",
    "def ood_split_for(gen_key: str):\n",
    "    # real 跟 IID 使用同一個切分（確保各版本一致）\n",
    "    r_tr = [i for i in iid[\"train\"] if i in ids_real]\n",
    "    r_va = [i for i in iid[\"val\"]   if i in ids_real]\n",
    "    r_te = [i for i in iid[\"test\"]  if i in ids_real]\n",
    "    # 假圖：非 gen_key 的 → 按 IID 的分法放 train/val/test；gen_key 的 → 全部進 test\n",
    "    allowed = set().union(*[set(v) for k,v in fake_by_gen.items() if k != gen_key])\n",
    "    holdout = set(fake_by_gen.get(gen_key, []))\n",
    "    f_tr = [i for i in iid[\"train\"] if i in allowed]\n",
    "    f_va = [i for i in iid[\"val\"]   if i in allowed]\n",
    "    f_te = [i for i in iid[\"test\"]  if i in allowed] + sorted(list(holdout))\n",
    "    return {\"train\": r_tr + f_tr, \"val\": r_va + f_va, \"test\": r_te}\n",
    "\n",
    "ood_gen = {}\n",
    "for g in [\"sd3\",\"midjourney\",\"flux\",\"dalle3\"]:\n",
    "    sp = ood_split_for(g)\n",
    "    ood_gen[g] = sp\n",
    "    # 摘要\n",
    "    ntr = (sum(i in ids_real for i in sp[\"train\"]), sum(i in ids_fake for i in sp[\"train\"]))\n",
    "    nva = (sum(i in ids_real for i in sp[\"val\"]),   sum(i in ids_fake for i in sp[\"val\"]))\n",
    "    nte = (sum(i in ids_real for i in sp[\"test\"]),  sum(i in ids_fake for i in sp[\"test\"]))\n",
    "    print(f\"== OOD-{g} summary ==\")\n",
    "    print(f\"[train] total={len(sp['train'])} | real={ntr[0]} fake={ntr[1]}\")\n",
    "    print(f\"[val]   total={len(sp['val'])}   | real={nva[0]} fake={nva[1]}\")\n",
    "    # 顯示 test 裡 holdout g 的數量\n",
    "    n_g_test = sum(1 for i in sp[\"test\"] if guess_generator(i)==g)\n",
    "    print(f\"[test]  total={len(sp['test'])}  | real={nte[0]} fake={nte[1]} (holdout {g} in test: {n_g_test})\")\n",
    "\n",
    "# --- smoke_10p：從 IID 各 split 各自取 10%（分層隨機） ---\n",
    "def take_frac(ids_list, frac, seed):\n",
    "    rnd = random.Random(seed)\n",
    "    ids_r = [i for i in ids_list if i in ids_real]\n",
    "    ids_f = [i for i in ids_list if i in ids_fake]\n",
    "    rnd.shuffle(ids_r); rnd.shuffle(ids_f)\n",
    "    kr = max(1, int(round(len(ids_r)*frac))) if ids_r else 0\n",
    "    kf = max(1, int(round(len(ids_f)*frac))) if ids_f else 0\n",
    "    return ids_r[:kr] + ids_f[:kf]\n",
    "\n",
    "smoke_10p = {\n",
    "    \"train\": take_frac(iid[\"train\"], SMOKE_FRAC, SEED),\n",
    "    \"val\":   take_frac(iid[\"val\"],   SMOKE_FRAC, SEED+1),\n",
    "    \"test\":  take_frac(iid[\"test\"],  SMOKE_FRAC, SEED+2),\n",
    "}\n",
    "print(\"== smoke_10p summary ==\")\n",
    "count_split(smoke_10p, ids_real, ids_fake)\n",
    "\n",
    "# ===================== 寫出 JSON（會保留舊檔，再覆蓋） =====================\n",
    "out = {\n",
    "    \"meta\": {\n",
    "        \"seed\": SEED,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"tri_sync_modalities\": [\"clip_pooled(1024)\", \"prnu\", \"ela\"],\n",
    "        \"paths\": {\n",
    "            \"clip_pooled_real\": str(CLIP_POOL_REAL),\n",
    "            \"clip_pooled_fake\": str(CLIP_POOL_FAKE),\n",
    "            \"prnu_real\": str(PRNU_REAL), \"prnu_fake\": str(PRNU_FAKE),\n",
    "            \"ela_real\":  str(ELA_REAL),  \"ela_fake\":  str(ELA_FAKE),\n",
    "        },\n",
    "        \"iid_ratio\": IID_RATIO,\n",
    "        \"generators\": list(GEN_CANON.keys()),\n",
    "        \"note\": \"IDs are image stems without extension; splits contain tri-modal intersection only.\"\n",
    "    },\n",
    "    \"iid\": iid,\n",
    "    \"ood_gen\": ood_gen,\n",
    "    \"smoke_10p\": smoke_10p\n",
    "}\n",
    "\n",
    "ensure_dir(SPLIT_OUT.parent)\n",
    "# 備份舊檔\n",
    "if SPLIT_OUT.exists():\n",
    "    bk = SPLIT_OUT.with_suffix(f\".bak_{int(time.time())}.json\")\n",
    "    SPLIT_OUT.replace(bk)\n",
    "    print(\"↻ backup old split →\", bk)\n",
    "SPLIT_OUT.write_text(json.dumps(out, ensure_ascii=False, indent=2))\n",
    "print(\"✅ saved:\", SPLIT_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64e57ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clip_pooled_real_npy] fixed=0 removed_tmp=0\n",
      "[clip_pooled_fake_npy] fixed=60106 removed_tmp=0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 兩個資料夾都修\n",
    "DIRS = [\n",
    "    Path(\"/home/yaya/ai-detect-proj/Script/features_256/clip_pooled_real_npy\"),\n",
    "    Path(\"/home/yaya/ai-detect-proj/Script/features_256/clip_pooled_fake_npy\"),\n",
    "]\n",
    "\n",
    "for root in DIRS:\n",
    "    if not root.exists(): \n",
    "        print(\"skip (not found):\", root); \n",
    "        continue\n",
    "    n_fix = n_drop = 0\n",
    "    for tmpf in root.glob(\"*.npy.tmp.npy\"):\n",
    "        # 變回「正確檔名」：把 \".npy.tmp.npy\" → \".npy\"\n",
    "        dst = tmpf.with_name(tmpf.name.replace(\".npy.tmp.npy\", \".npy\"))\n",
    "        if dst.exists():\n",
    "            # 目標已存在：刪掉這個多餘的 .tmp 檔\n",
    "            tmpf.unlink()\n",
    "            n_drop += 1\n",
    "        else:\n",
    "            os.replace(tmpf, dst)\n",
    "            n_fix += 1\n",
    "    print(f\"[{root.name}] fixed={n_fix} removed_tmp={n_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e87e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLUX__0000_00018124.npy (1024,)\n",
      "SD3__0000_00049062.npy (1024,)\n",
      "SD3__0000_00021448.npy (1024,)\n",
      "dalle3__008127.npy (1024,)\n",
      "SD3__0000_00047005.npy (1024,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, random\n",
    "check_dirs = [Path(\"/home/yaya/ai-detect-proj/Script/features_256/clip_pooled_fake_npy\")]\n",
    "for d in check_dirs:\n",
    "    files = list(d.glob(\"*.npy\"))\n",
    "    for p in random.sample(files, min(5, len(files))):\n",
    "        arr = np.load(p, allow_pickle=False)\n",
    "        print(p.name, arr.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
