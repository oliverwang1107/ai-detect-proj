{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5927779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, math, json, uuid, tempfile\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# 建議的 CUDA / I/O 環境變數\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "os.environ.setdefault(\"PYTHONUTF8\",\"1\")\n",
    "os.environ.setdefault(\"PYTHONIOENCODING\",\"utf-8\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageChops, ImageOps, ImageDraw, ImageFont\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "# 主要根目錄：預設當前工作目錄（Colab 會是 /content）\n",
    "SCRIPT_ROOT = Path.cwd()\n",
    "SAVED_MODELS = SCRIPT_ROOT / \"saved_models\"\n",
    "SAVED_MODELS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 模型/融合器路徑\n",
    "PRNU_MODEL_PATH = SAVED_MODELS / \"prnu_fastcnn_best.pt\"\n",
    "ELA_MODEL_PATH  = SAVED_MODELS / \"ela_fastcnn_best.pt\"\n",
    "\n",
    "CLIP_LOGREG_PKL = SAVED_MODELS / \"clip_logreg_gpu.pkl\"    # （若有 RAPIDS 可用）\n",
    "CLIP_SVM_CUML   = SAVED_MODELS / \"clip_svm_gpu.pkl\"       # 回退\n",
    "CLIP_SVM_TORCH  = SAVED_MODELS / \"clip_svm_gpu_torch.pt\"  # 回退\n",
    "CLIP_PLATT_PKL  = SAVED_MODELS / \"clip_platt.pkl\"         # 可選\n",
    "\n",
    "FUSER_PKL       = SAVED_MODELS / \"fusion_lr.pkl\"\n",
    "FUSER_META      = SAVED_MODELS / \"fusion_lr_meta.json\"\n",
    "\n",
    "# 參數\n",
    "SEED            = 42\n",
    "TILE            = 256\n",
    "STRIDE          = 128\n",
    "ELA_QUALITY     = 90\n",
    "ELA_SCALE       = 15\n",
    "ELA_FEASZ       = 128\n",
    "PRNU_MODE       = \"soft\"\n",
    "PRNU_WAVELET    = \"db8\"\n",
    "PRNU_Q_MODE     = \"per_file\"\n",
    "PRNU_Q_PERC     = 0.999\n",
    "PRNU_Q_SAMPLES  = 4096\n",
    "\n",
    "FORCE_JPG_NONJPG = True\n",
    "JPEG_FORCE_QUALITY     = 95\n",
    "JPEG_FORCE_SUBSAMPLING = 0     # 4:4:4\n",
    "\n",
    "CLIP_BACKBONE   = \"ViT-L-14\"\n",
    "CLIP_PRETRAINED = {\"ViT-L-14\":\"laion2b_s32b_b82k\",\"ViT-B-32\":\"laion400m_e32\",\"ViT-L-14-336\":\"laion2b_s32b_b82k\"}.get(CLIP_BACKBONE, \"laion2b_s32b_b82k\")\n",
    "\n",
    "# 默認啟用模態（可在呼叫時覆寫）\n",
    "DEFAULT_ENABLED = {\"prnu\": True, \"ela\": True, \"clip\": True}\n",
    "\n",
    "# 裝置與 AMP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e592e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pillow-heif registered\n",
      "✅ pillow-avif (pillow-avif-python) registered\n"
     ]
    }
   ],
   "source": [
    "def _register_heif_avif():\n",
    "    ok = False\n",
    "    try:\n",
    "        import pillow_heif\n",
    "        pillow_heif.register_heif_opener()\n",
    "        ok = True\n",
    "        print(\"✅ pillow-heif registered\")\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ pillow-heif not available:\", e)\n",
    "    # avif 兩種常見套件命名，擇一成功即可\n",
    "    try:\n",
    "        import pillow_avif  # from pillow-avif-python\n",
    "        from pillow_avif import AvifImagePlugin  # noqa\n",
    "        ok = True\n",
    "        print(\"✅ pillow-avif (pillow-avif-python) registered\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            import avif  # from pillow-avif-plugin\n",
    "            ok = True\n",
    "            print(\"✅ pillow-avif-plugin registered\")\n",
    "        except Exception as e:\n",
    "            print(\"ℹ️ AVIF plugin not available:\", e)\n",
    "    return ok\n",
    "\n",
    "_register_heif_avif()\n",
    "\n",
    "def _open_image_any(p: Path) -> Image.Image:\n",
    "    p = Path(p)\n",
    "    try:\n",
    "        img = Image.open(p)\n",
    "        img.load()\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        if p.suffix.lower() in {\".heic\",\".heif\",\".heifs\",\".hif\",\".avif\"}:\n",
    "            try:\n",
    "                import pillow_heif\n",
    "                h = pillow_heif.read_heif(str(p))\n",
    "                img = Image.frombytes(h.mode, h.size, h.data, \"raw\")\n",
    "                return img\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(f\"HEIF/AVIF decode failed: {e2}\") from e2\n",
    "        raise\n",
    "\n",
    "def _to_rgb_no_alpha(img: Image.Image) -> Image.Image:\n",
    "    try:\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if img.mode in (\"RGBA\",\"LA\") or (img.mode==\"P\" and \"transparency\" in img.info):\n",
    "        bg = Image.new(\"RGB\", img.size, (255,255,255))\n",
    "        bg.paste(img.convert(\"RGBA\"), mask=img.convert(\"RGBA\").split()[-1])\n",
    "        return bg\n",
    "    if img.mode != \"RGB\":\n",
    "        return img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def as_jpg_if_needed(p: Path,\n",
    "                     quality: int = JPEG_FORCE_QUALITY,\n",
    "                     subsampling: int | str = JPEG_FORCE_SUBSAMPLING) -> Tuple[Path, str | None]:\n",
    "    p = Path(p)\n",
    "    if p.suffix.lower() in {\".jpg\",\".jpeg\"}:\n",
    "        return p, None\n",
    "    img = _to_rgb_no_alpha(_open_image_any(p))\n",
    "    tmp = p.with_suffix(f\".tmp_infer_{uuid.uuid4().hex[:8]}.jpg\")\n",
    "    buf = BytesIO(); img.save(buf, format=\"JPEG\", quality=int(quality), subsampling=subsampling, optimize=False)\n",
    "    with open(tmp, \"wb\") as f: f.write(buf.getvalue())\n",
    "    return tmp, str(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24660da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PRNU model loaded\n",
      "✅ ELA model loaded\n"
     ]
    }
   ],
   "source": [
    "class DSBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(c_in, c_in, 3, stride=stride, padding=1, groups=c_in, bias=False)\n",
    "        self.bn1= nn.BatchNorm2d(c_in)\n",
    "        self.pw = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn2= nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.dw(x)))\n",
    "        x = self.act(self.bn2(self.pw(x)))\n",
    "        return x\n",
    "\n",
    "class FastCNN_1ch(nn.Module):\n",
    "    def __init__(self, base=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(nn.Conv2d(1, base, 3, padding=1, bias=False),\n",
    "                                  nn.BatchNorm2d(base), nn.ReLU(inplace=True))\n",
    "        self.stage= nn.Sequential(DSBlock(base,base*2,1), DSBlock(base*2,base*4,2),\n",
    "                                  DSBlock(base*4,base*4,1), DSBlock(base*4,base*8,2),\n",
    "                                  DSBlock(base*8,base*8,1))\n",
    "        self.head = nn.Sequential(nn.Conv2d(base*8, base*8, 1, bias=False),\n",
    "                                  nn.BatchNorm2d(base*8), nn.ReLU(inplace=True))\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1); self.fc = nn.Linear(base*8,2)\n",
    "    def forward(self, x):\n",
    "        x=self.stem(x); x=self.stage(x); x=self.head(x); x=self.pool(x).flatten(1); return self.fc(x)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _safe_load(path, map_location=\"cpu\"):\n",
    "    try:\n",
    "        return torch.load(path, map_location=map_location, weights_only=True)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=map_location)\n",
    "\n",
    "def _extract_state_dict(blob):\n",
    "    if isinstance(blob, dict):\n",
    "        for k in (\"state_dict\",\"model\",\"weights\"):\n",
    "            if k in blob and isinstance(blob[k], dict):\n",
    "                sd = blob[k]; break\n",
    "        else:\n",
    "            sd = {k:v for k,v in blob.items() if torch.is_tensor(v)}\n",
    "            if not sd: raise RuntimeError(\"未知 checkpoint 格式\")\n",
    "    elif isinstance(blob, torch.nn.Module):\n",
    "        sd = blob.state_dict()\n",
    "    else:\n",
    "        sd = blob\n",
    "    if len(sd)>0 and next(iter(sd)).startswith(\"module.\"):\n",
    "        sd = OrderedDict((k[len(\"module.\"):], v) for k,v in sd.items())\n",
    "    return sd\n",
    "\n",
    "# 嘗試載入；缺檔時標示不可用，預設開關會自動關掉\n",
    "AVAILABLE = {\"prnu\": False, \"ela\": False, \"clip\": True}\n",
    "prnu_model = None; ela_model = None\n",
    "\n",
    "try:\n",
    "    prnu_model = FastCNN_1ch().to(device).eval()\n",
    "    prnu_model.load_state_dict(_extract_state_dict(_safe_load(PRNU_MODEL_PATH, device)))\n",
    "    AVAILABLE[\"prnu\"] = True\n",
    "    print(\"✅ PRNU model loaded\")\n",
    "except Exception as e:\n",
    "    print(\"ℹ️ PRNU model not available:\", e)\n",
    "\n",
    "try:\n",
    "    ela_model  = FastCNN_1ch().to(device).eval()\n",
    "    ela_model.load_state_dict(_extract_state_dict(_safe_load(ELA_MODEL_PATH,  device)))\n",
    "    AVAILABLE[\"ela\"] = True\n",
    "    print(\"✅ ELA model loaded\")\n",
    "except Exception as e:\n",
    "    print(\"ℹ️ ELA model not available:\", e)\n",
    "\n",
    "# 若缺少某模態，預設關閉它\n",
    "for k in list(DEFAULT_ENABLED):\n",
    "    if not AVAILABLE.get(k, False):\n",
    "        DEFAULT_ENABLED[k] = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def prnu_logit_from_i8(arr_i8: np.ndarray) -> float:\n",
    "    # channels_last + AMP\n",
    "    x = arr_i8\n",
    "    if x.dtype == np.int8:\n",
    "        x = x.astype(np.float32) / 127.0\n",
    "    elif x.dtype == np.uint8:\n",
    "        x = x.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        x = x.astype(np.float32); x = x/255.0 if x.max()>1.5 else x\n",
    "    x = x - x.mean()\n",
    "    t = torch.from_numpy(x[None,None,...]).to(device).contiguous(memory_format=torch.channels_last)\n",
    "    with torch.autocast(device_type=device.type, enabled=USE_AMP):\n",
    "        p1 = torch.softmax(prnu_model(t), dim=1)[0,1].item()\n",
    "    p1 = float(np.clip(p1, 1e-6, 1-1e-6))\n",
    "    z = math.log(p1) - math.log(1.0 - p1)\n",
    "    return float(np.clip(z, -20.0, 20.0))\n",
    "\n",
    "@torch.no_grad()\n",
    "def ela_logit_from_i8(arr_i8: np.ndarray) -> float:\n",
    "    a = arr_i8\n",
    "    if a.dtype == np.int8:\n",
    "        x = (a.astype(np.float32) + 128.0) / 255.0\n",
    "    elif a.dtype == np.uint8:\n",
    "        x = a.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        x = a.astype(np.float32); x = x/255.0 if x.max()>1.5 else x\n",
    "    x = x - x.mean()\n",
    "    t = torch.from_numpy(x[None,None,...]).to(device).contiguous(memory_format=torch.channels_last)\n",
    "    with torch.autocast(device_type=device.type, enabled=USE_AMP):\n",
    "        p1 = torch.softmax(ela_model(t), dim=1)[0,1].item()\n",
    "    p1 = float(np.clip(p1, 1e-6, 1-1e-6))\n",
    "    z = math.log(p1) - math.log(1.0 - p1)\n",
    "    return float(np.clip(z, -20.0, 20.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfa79b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CLIP: cuML LogisticRegression（logit）\n"
     ]
    }
   ],
   "source": [
    "_openclip = {\"model\":None,\"pre\":None,\"dev\":\"cpu\"}\n",
    "\n",
    "def _cuml_ok():\n",
    "    try:\n",
    "        import cuml, cupy  # noqa\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_openclip():\n",
    "    if _openclip[\"model\"] is None:\n",
    "        import open_clip\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model, _, pre = open_clip.create_model_and_transforms(CLIP_BACKBONE, pretrained=CLIP_PRETRAINED)\n",
    "        model = model.to(dev).eval()\n",
    "        _openclip.update(model=model, pre=pre, dev=dev)\n",
    "    return _openclip[\"model\"], _openclip[\"pre\"], _openclip[\"dev\"]\n",
    "\n",
    "class CLIPSingle:\n",
    "    def __init__(self):\n",
    "        self.mode=None; self.backend=None; self.model=None; self.D=None; self.platt=None\n",
    "        self._load()\n",
    "    def _load(self):\n",
    "        # cuML LR（優先）\n",
    "        if CLIP_LOGREG_PKL.exists():\n",
    "            import joblib\n",
    "            obj = joblib.load(CLIP_LOGREG_PKL)\n",
    "            self.model = obj[\"model\"] if isinstance(obj, dict) and \"model\" in obj else obj\n",
    "            self.mode, self.backend, self.D = \"logreg\", \"cuml\", 1024\n",
    "            print(\"✅ CLIP: cuML LogisticRegression（logit）\"); return\n",
    "        # cuML SVM（回退）\n",
    "        if CLIP_SVM_CUML.exists() and _cuml_ok():\n",
    "            import joblib\n",
    "            clf = joblib.load(CLIP_SVM_CUML)\n",
    "            self.model = clf[\"model\"] if isinstance(clf, dict) and \"model\" in clf else clf\n",
    "            self.mode, self.backend, self.D = \"svm\", \"cuml\", 1024\n",
    "            print(\"⚠️ CLIP: cuML SVM（margin）\")\n",
    "        # Torch 線性 SVM（回退）\n",
    "        elif CLIP_SVM_TORCH.exists():\n",
    "            sd = _safe_load(CLIP_SVM_TORCH, \"cpu\")\n",
    "            state = sd[\"state_dict\"] if isinstance(sd, dict) and \"state_dict\" in sd else sd\n",
    "            self.D = int(sd[\"D\"]) if isinstance(sd, dict) and \"D\" in sd else state[\"weight\"].shape[1]\n",
    "            lin = nn.Linear(self.D, 1, bias=True).eval(); lin.load_state_dict(state)\n",
    "            self.model = lin; self.mode, self.backend = \"svm\", \"torch\"\n",
    "            print(f\"⚠️ CLIP: Torch SVM（margin, D={self.D}）\")\n",
    "        else:\n",
    "            print(\"ℹ️ 未找到 CLIP 分類器（跳過 CLIP）\")\n",
    "            return\n",
    "        if self.mode == \"svm\" and CLIP_PLATT_PKL.exists():\n",
    "            import joblib\n",
    "            try:\n",
    "                self.platt = joblib.load(CLIP_PLATT_PKL)\n",
    "                print(\"✅ 載入 Platt 標定器（SVM → prob）\")\n",
    "            except Exception as e:\n",
    "                print(\"⚠️ Platt 載入失敗：\", e)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_logit(self, pil_tile_256: Image.Image) -> float | None:\n",
    "        if self.model is None: return None\n",
    "        tmp = Path(f\"/tmp/clip_tile_{uuid.uuid4().hex[:8]}.jpg\")\n",
    "        buf = BytesIO(); pil_tile_256.save(buf, format=\"JPEG\", quality=95, subsampling=0)\n",
    "        with open(tmp, \"wb\") as f: f.write(buf.getvalue())\n",
    "        try:\n",
    "            model, pre, dev = load_openclip()\n",
    "            img = Image.open(tmp).convert(\"RGB\")\n",
    "            im  = pre(img).unsqueeze(0).to(dev)\n",
    "            visual = model.visual\n",
    "\n",
    "            # 嘗試抓出 token map（最後一層）\n",
    "            tokens=None; ok=False\n",
    "            def _apply_norm(x):\n",
    "                if x.ndim==2: x=x.unsqueeze(0)\n",
    "                if hasattr(visual, \"ln_post\") and visual.ln_post is not None:\n",
    "                    x = visual.ln_post(x)\n",
    "                elif hasattr(visual, \"trunk\") and hasattr(visual.trunk, \"norm\") and visual.trunk.norm is not None:\n",
    "                    x = visual.trunk.norm(x)\n",
    "                return x.squeeze(0)\n",
    "\n",
    "            for try_path in (\"trunk.forward_features\",\"visual.forward_features\",\"hook\"):\n",
    "                try:\n",
    "                    if try_path==\"trunk.forward_features\" and hasattr(visual,\"trunk\") and hasattr(visual.trunk,\"forward_features\"):\n",
    "                        out = visual.trunk.forward_features(im)\n",
    "                        if isinstance(out,(tuple,list)): out=out[0]\n",
    "                        if isinstance(out,dict): out=out.get(\"x\", out.get(\"tokens\", out))\n",
    "                        if torch.is_tensor(out) and out.ndim==3:\n",
    "                            tokens=_apply_norm(out.detach()); ok=True; break\n",
    "                    if try_path==\"visual.forward_features\" and hasattr(visual,\"forward_features\"):\n",
    "                        out = visual.forward_features(im)\n",
    "                        if isinstance(out,(tuple,list)): out=out[0]\n",
    "                        if isinstance(out,dict): out=out.get(\"x\", out.get(\"tokens\", out))\n",
    "                        if torch.is_tensor(out) and out.ndim==3:\n",
    "                            tokens=_apply_norm(out.detach()); ok=True; break\n",
    "                    if try_path==\"hook\":\n",
    "                        feats={}; h=None\n",
    "                        try:\n",
    "                            if hasattr(visual,\"trunk\") and hasattr(visual.trunk,\"blocks\") and len(visual.trunk.blocks)>0:\n",
    "                                target=visual.trunk.blocks[-1]\n",
    "                                def _hook(_m,_i,o): feats[\"x\"]=o\n",
    "                                h=target.register_forward_hook(_hook)\n",
    "                                _ = getattr(visual.trunk, \"forward_features\", visual.trunk.forward)(im)\n",
    "                            elif hasattr(visual,\"transformer\") and hasattr(visual.transformer,\"resblocks\") and len(visual.transformer.resblocks)>0:\n",
    "                                target=visual.transformer.resblocks[-1]\n",
    "                                def _hook(_m,_i,o): feats[\"x\"]=o\n",
    "                                h=target.register_forward_hook(_hook); _=model.encode_image(im)\n",
    "                        finally:\n",
    "                            if h is not None:\n",
    "                                try: h.remove()\n",
    "                                except Exception: pass\n",
    "                        x=feats.get(\"x\",None)\n",
    "                        if torch.is_tensor(x) and x.ndim==3:\n",
    "                            tokens=_apply_norm(x.detach()); ok=True; break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not ok:\n",
    "                return None\n",
    "            tokens = tokens.float().cpu().numpy().astype(np.float32)\n",
    "            if tokens.shape[0] <= 1: return None\n",
    "            pooled = tokens[1:].mean(axis=0)\n",
    "            pooled /= (np.linalg.norm(pooled)+1e-12)\n",
    "\n",
    "            if self.backend == \"cuml\":\n",
    "                import cupy as cp\n",
    "                s = self.model.decision_function(cp.asarray(pooled[None,:])).get().astype(np.float32)[0]\n",
    "                return float(np.clip(s, -20.0, 20.0))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    s = self.model(torch.from_numpy(pooled[None,:]).float()).squeeze(1).numpy().astype(np.float32)[0]\n",
    "                if self.mode == \"svm\" and self.platt is not None:\n",
    "                    from sklearn.linear_model import LogisticRegression  # noqa\n",
    "                    import numpy as _np\n",
    "                    p = float(self.platt.predict_proba(_np.array(s, _np.float32).reshape(1,1))[:,1][0])\n",
    "                    z = math.log(max(p,1e-6)) - math.log(max(1-p,1e-6))\n",
    "                    return float(np.clip(z, -20.0, 20.0))\n",
    "                return float(np.clip(s, -20.0, 20.0))\n",
    "        finally:\n",
    "            try: os.unlink(tmp)\n",
    "            except Exception: pass\n",
    "\n",
    "clip_single = CLIPSingle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f56d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fusion_lr.pkl loaded (sklearn LR)\n",
      "✅ fusion_lr_meta.json loaded\n"
     ]
    }
   ],
   "source": [
    "def read_json_utf8(p: Path):\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_fuser_and_meta():\n",
    "    meta=None; kind=\"dummy\"; fuser=None\n",
    "    try:\n",
    "        if FUSER_PKL.exists():\n",
    "            import joblib\n",
    "            fuser = joblib.load(FUSER_PKL); kind=\"lr\"\n",
    "            print(\"✅ fusion_lr.pkl loaded (sklearn LR)\")\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ fusion_lr.pkl not available:\", e)\n",
    "    if FUSER_META.exists():\n",
    "        try:\n",
    "            meta = read_json_utf8(FUSER_META); print(\"✅ fusion_lr_meta.json loaded\")\n",
    "        except Exception as e:\n",
    "            print(\"ℹ️ fusion meta read fail:\", e)\n",
    "    return kind, fuser, meta\n",
    "\n",
    "fuser_kind, fuser, fmeta = load_fuser_and_meta()\n",
    "\n",
    "def fuse_logits(logits_dict: Dict[str, float | None],\n",
    "                enabled: List[str] | Tuple[str, ...] | None = None) -> float:\n",
    "    full_order = [\"prnu\", \"ela\", \"clip\"]\n",
    "    enabled = list(enabled) if enabled is not None else full_order\n",
    "    enabled = [k for k in full_order if k in enabled]\n",
    "\n",
    "    # 3模態全開 + 有 LR → 直接用\n",
    "    if fuser_kind == \"lr\" and enabled == full_order:\n",
    "        z = [float(np.nan_to_num(logits_dict.get(k, 0.0), nan=0.0, posinf=20.0, neginf=-20.0)) for k in full_order]\n",
    "        X = np.array([z], np.float32)\n",
    "        try:\n",
    "            proba = float(fuser.predict_proba(X)[:, 1][0])\n",
    "            return float(np.clip(proba, 1e-6, 1-1e-6))\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ LR fusion failed; fallback to weights:\", e)\n",
    "\n",
    "    # 備援權重\n",
    "    w = None\n",
    "    if fmeta and \"weights_if_no_sklearn\" in fmeta:\n",
    "        meta_order = [s.lower() for s in fmeta.get(\"order\", [\"PRNU\",\"ELA\",\"CLIP\"])]\n",
    "        meta_w_all = np.array(fmeta[\"weights_if_no_sklearn\"], np.float32)\n",
    "        pick = [meta_order.index(k) for k in enabled]\n",
    "        w = meta_w_all[pick]\n",
    "        s = float(w.sum()); w = (w/s) if s>0 else None\n",
    "    if w is None:\n",
    "        w = np.ones((len(enabled),), np.float32) / max(1, len(enabled))\n",
    "\n",
    "    z = [float(np.nan_to_num(logits_dict.get(k, 0.0), nan=0.0, posinf=20.0, neginf=-20.0)) for k in enabled]\n",
    "    zz = float(np.clip(np.dot(z, w), -20.0, 20.0))\n",
    "    return float(1.0/(1.0+np.exp(-zz)))\n",
    "\n",
    "# ---- 特徵抽取（tile 級）----\n",
    "def _to_int8_offset128_from_01(x01: np.ndarray) -> np.ndarray:\n",
    "    u8 = np.rint(np.clip(x01, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    return (u8.astype(np.int16) - 128).astype(np.int8)\n",
    "\n",
    "def ela_i8_from_tile(pil_tile_256: Image.Image) -> np.ndarray:\n",
    "    buf = BytesIO(); pil_tile_256.save(buf, format=\"JPEG\", quality=int(ELA_QUALITY), subsampling=0, optimize=False)\n",
    "    buf.seek(0)\n",
    "    diff = ImageChops.difference(pil_tile_256, Image.open(buf)).point(lambda x: x * ELA_SCALE)\n",
    "    diff = diff.convert(\"L\").resize((ELA_FEASZ, ELA_FEASZ))\n",
    "    arr01 = np.asarray(diff, dtype=np.float32) / 255.0\n",
    "    return _to_int8_offset128_from_01(arr01)\n",
    "\n",
    "def prnu_i8_from_tile(np_tile_rgb: np.ndarray) -> np.ndarray:\n",
    "    if np_tile_rgb.ndim == 2:\n",
    "        np_tile_rgb = np.repeat(np_tile_rgb[...,None], 3, axis=-1)\n",
    "    gray = np_tile_rgb.mean(axis=2, dtype=np.float32)\n",
    "    try:\n",
    "        den = denoise_wavelet(gray, channel_axis=None, mode=PRNU_MODE, wavelet=PRNU_WAVELET, convert2ycbcr=False)\n",
    "    except TypeError:\n",
    "        den = denoise_wavelet(gray, multichannel=False, mode=PRNU_MODE, wavelet=PRNU_WAVELET, convert2ycbcr=False)\n",
    "    residual = gray - den\n",
    "    residual -= residual.mean()\n",
    "    if PRNU_Q_MODE == \"per_file\":\n",
    "        v = residual.reshape(-1).astype(np.float32, copy=False)\n",
    "        if v.size > PRNU_Q_SAMPLES:\n",
    "            idx = rng.integers(0, v.size, size=PRNU_Q_SAMPLES, endpoint=False)\n",
    "            v = np.abs(v[idx])\n",
    "        else:\n",
    "            v = np.abs(v)\n",
    "        k = int(PRNU_Q_PERC * max(1, v.size-1))\n",
    "        S = float(max(1e-8, np.partition(v, k)[k]))\n",
    "    else:\n",
    "        S = max(1e-6, float(np.std(residual)) * 6.0)\n",
    "    x = np.clip(residual, -S, S) / S * 127.0\n",
    "    q = np.rint(x).astype(np.int16)\n",
    "    q = np.clip(q, -127, 127).astype(np.int8)\n",
    "    return q\n",
    "\n",
    "def make_grid(w: int, h: int, tile: int = TILE, stride: int = STRIDE) -> List[Tuple[int,int,int,int]]:\n",
    "    xs = list(range(0, max(1, w - tile + 1), stride))\n",
    "    ys = list(range(0, max(1, h - tile + 1), stride))\n",
    "    if xs[-1] != w - tile: xs.append(max(0, w - tile))\n",
    "    if ys[-1] != h - tile: ys.append(max(0, h - tile))\n",
    "    coords = [(x, y, tile, tile) for y in ys for x in xs]\n",
    "    return coords\n",
    "\n",
    "# ---- 可視化 ----\n",
    "def _text_size(draw, text, font):\n",
    "    if hasattr(draw, \"textbbox\"):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font); return (r - l), (b - t)\n",
    "    if hasattr(font, \"getbbox\"):\n",
    "        l, t, r, b = font.getbbox(text); return (r - l), (b - t)\n",
    "    if hasattr(font, \"getsize\"):\n",
    "        return font.getsize(text)\n",
    "    return (8 * len(text), 12)\n",
    "\n",
    "def overlay_tiles(base_img: Image.Image,\n",
    "                  tiles: list[dict],\n",
    "                  alpha: float = 0.35,\n",
    "                  draw_frame: bool = True,\n",
    "                  show_score: bool = True) -> Image.Image:\n",
    "    W, H = base_img.size\n",
    "    heat = np.zeros((H, W), dtype=np.float32)\n",
    "    cnt  = np.zeros((H, W), dtype=np.float32)\n",
    "    for t in tiles:\n",
    "        x, y, w, h = t['x'], t['y'], t['w'], t['h']\n",
    "        p = float(t['prob_fake'])\n",
    "        heat[y:y+h, x:x+w] += p\n",
    "        cnt [y:y+h, x:x+w] += 1.0\n",
    "    cnt[cnt == 0] = 1.0\n",
    "    heat = heat / cnt\n",
    "    heat_u8 = np.rint(np.clip(heat, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    heat_rgb = np.zeros((H, W, 3), dtype=np.uint8); heat_rgb[..., 0] = heat_u8\n",
    "    base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
    "    overlay = Image.blend(base_rgb, heat_img, alpha).convert('RGBA')\n",
    "    draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 16)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "    if draw_frame or show_score:\n",
    "        for t in tiles:\n",
    "            x, y, w, h = t['x'], t['y'], t['w'], t['h']\n",
    "            p  = float(t['prob_fake'])\n",
    "            lab = int(t['pred_label'])\n",
    "            color = (255, 0, 0, 255) if lab == 1 else (0, 255, 0, 255)\n",
    "            if draw_frame:\n",
    "                draw.rectangle([x, y, x + w, y + h], outline=color, width=2)\n",
    "            if show_score:\n",
    "                txt = f\"{p:.2f}\"; tw, th = _text_size(draw, txt, font)\n",
    "                draw.rectangle([x, y, x + tw + 6, y + th + 4], fill=(0, 0, 0, 127))\n",
    "                draw.text((x + 3, y + 2), txt, fill=(255, 255, 255, 255), font=font)\n",
    "    return overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e376a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_logit(p: float, eps: float = 1e-6, clamp: float = 20.0) -> float:\n",
    "    p = float(np.clip(p, eps, 1.0 - eps))\n",
    "    z = math.log(p) - math.log(1.0 - p)\n",
    "    return float(np.clip(z, -clamp, clamp))\n",
    "\n",
    "def infer_image_by_tiles(img_path: str | Path,\n",
    "                         tile: int = TILE,\n",
    "                         stride: int = STRIDE,\n",
    "                         aggregate: str = 'mean_prob',\n",
    "                         use_clip: bool = True,\n",
    "                         save_overlay: bool = True,\n",
    "                         overlay_alpha: float = 0.35,\n",
    "                         enable: Dict[str, bool] | None = None\n",
    "                         ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    enable: 例如 {\"prnu\":True,\"ela\":True,\"clip\":False}\n",
    "    若 None → 用 DEFAULT_ENABLED；且若 use_clip=False 會強制關閉 clip\n",
    "    \"\"\"\n",
    "    p = Path(img_path)\n",
    "    enable = dict(DEFAULT_ENABLED if enable is None else enable)\n",
    "    if not use_clip:\n",
    "        enable[\"clip\"] = False\n",
    "    # 若某模態模型不存在，自動關閉\n",
    "    if not AVAILABLE.get(\"prnu\", False):\n",
    "        enable[\"prnu\"] = False\n",
    "    if not AVAILABLE.get(\"ela\", False):\n",
    "        enable[\"ela\"]  = False\n",
    "\n",
    "    enabled_keys = [k for k,v in enable.items() if v]\n",
    "    if len(enabled_keys) == 0:\n",
    "        raise ValueError(\"至少需啟用一個模態（prnu/ela/clip）\")\n",
    "\n",
    "    tmp_files = []\n",
    "    # 只有需要 ELA 或 CLIP 時才轉 JPG\n",
    "    need_jpg = FORCE_JPG_NONJPG and (enable.get(\"ela\",False) or enable.get(\"clip\",False))\n",
    "    if need_jpg:\n",
    "        p_jpg, t1 = as_jpg_if_needed(p)\n",
    "        if t1: tmp_files.append(t1)\n",
    "    else:\n",
    "        p_jpg = p\n",
    "\n",
    "    # 讀圖\n",
    "    base_rgb = _to_rgb_no_alpha(_open_image_any(p))\n",
    "    img_pil = Image.open(p_jpg).convert('RGB')\n",
    "    W, H = img_pil.size\n",
    "    coords = make_grid(W, H, tile=tile, stride=stride)\n",
    "    tiles_out: List[Dict[str, Any]] = []\n",
    "\n",
    "    np_img = np.asarray(base_rgb, dtype=np.uint8)\n",
    "\n",
    "    # 逐 tile\n",
    "    for (x,y,w,h) in coords:\n",
    "        pil_tile = img_pil.crop((x,y,x+w,y+h))\n",
    "        np_tile  = np.asarray(np_img[y:y+h, x:x+w, :], dtype=np.uint8)\n",
    "\n",
    "        z_prnu = None\n",
    "        if enable.get(\"prnu\", False) and prnu_model is not None:\n",
    "            prnu_i8 = prnu_i8_from_tile(np_tile)\n",
    "            z_prnu = prnu_logit_from_i8(prnu_i8)\n",
    "\n",
    "        z_ela = None\n",
    "        if enable.get(\"ela\", False) and ela_model is not None:\n",
    "            ela_i8  = ela_i8_from_tile(pil_tile)\n",
    "            z_ela   = ela_logit_from_i8(ela_i8)\n",
    "\n",
    "        z_clip = None\n",
    "        if enable.get(\"clip\", False) and clip_single.model is not None:\n",
    "            z_clip = clip_single.predict_logit(pil_tile)\n",
    "\n",
    "        prob_fake = fuse_logits({\"prnu\": z_prnu, \"ela\": z_ela, \"clip\": z_clip}, enabled=enabled_keys)\n",
    "        pred = int(prob_fake >= 0.5)\n",
    "\n",
    "        tiles_out.append({\n",
    "            \"x\":x, \"y\":y, \"w\":w, \"h\":h,\n",
    "            \"prnu_logit\": (None if z_prnu is None else float(z_prnu)),\n",
    "            \"ela_logit\":  (None if z_ela  is None else float(z_ela)),\n",
    "            \"clip_logit\": (None if z_clip is None else float(z_clip)),\n",
    "            \"prob_fake\": float(prob_fake),\n",
    "            \"pred_label\": pred,\n",
    "        })\n",
    "\n",
    "    # 彙總\n",
    "    probs = np.array([t['prob_fake'] for t in tiles_out], np.float32)\n",
    "    if aggregate == 'mean_prob':\n",
    "        whole_prob = float(np.clip(probs.mean() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "    elif aggregate == 'max_prob':\n",
    "        whole_prob = float(np.clip(probs.max() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "    elif aggregate.startswith('topk_mean'):\n",
    "        try:\n",
    "            frac = float(aggregate.split(':',1)[1]) if ':' in aggregate else 0.5\n",
    "        except Exception:\n",
    "            frac = 0.5\n",
    "        k = max(1, int(len(probs) * frac))\n",
    "        idx = np.argsort(-probs)[:k]\n",
    "        whole_prob = float(np.clip(probs[idx].mean(), 1e-6, 1-1e-6))\n",
    "    elif aggregate == 'mean_logit':\n",
    "        lgs = np.array([_safe_logit(float(p)) for p in probs], np.float32)\n",
    "        lg  = float(np.clip(lgs.mean() if len(lgs)>0 else 0.0, -20.0, 20.0))\n",
    "        whole_prob = float(1.0/(1.0+np.exp(-lg)))\n",
    "    else:\n",
    "        whole_prob = float(np.clip(probs.mean() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "    whole_pred = int(whole_prob >= 0.5)\n",
    "\n",
    "    # 各模態平均 logit（跨 tiles）\n",
    "    def _avg_logit(key: str):\n",
    "        vals = [t[f\"{key}_logit\"] for t in tiles_out if t[f\"{key}_logit\"] is not None]\n",
    "        return (None if len(vals)==0 else float(np.mean(np.array(vals, np.float32))))\n",
    "    feature_avg_logits = {\"prnu\": _avg_logit(\"prnu\"),\n",
    "                          \"ela\":  _avg_logit(\"ela\"),\n",
    "                          \"clip\": _avg_logit(\"clip\")}\n",
    "\n",
    "    overlay_path = None\n",
    "    if save_overlay:\n",
    "        overlay_img = overlay_tiles(base_rgb, tiles_out, alpha=overlay_alpha,\n",
    "                                    draw_frame=True, show_score=True)\n",
    "        overlay_path = str(p.with_suffix(\".tiles_overlay.png\"))\n",
    "        overlay_img.save(overlay_path)\n",
    "\n",
    "    out = {\n",
    "        \"path\": str(p),\n",
    "        \"image_size\": [H, W],\n",
    "        \"tile\": tile,\n",
    "        \"stride\": stride,\n",
    "        \"aggregate\": aggregate,\n",
    "        \"enabled_modalities\": enable,\n",
    "        \"feature_avg_logits\": feature_avg_logits,\n",
    "        \"overall\": {\"prob_fake\": float(whole_prob), \"pred_label\": int(whole_pred)},\n",
    "        \"tiles\": tiles_out,\n",
    "        \"overlay_path\": overlay_path,\n",
    "    }\n",
    "\n",
    "    for t in tmp_files:\n",
    "        try: os.unlink(t)\n",
    "        except Exception: pass\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee24e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_probs_offline(tiles: List[dict], method: str) -> float:\n",
    "    probs = np.array([t['prob_fake'] for t in tiles], np.float32)\n",
    "    if probs.size == 0: \n",
    "        return 1e-6\n",
    "    if method == \"mean_prob\":\n",
    "        p = probs.mean()\n",
    "    elif method == \"max_prob\":\n",
    "        p = probs.max()\n",
    "    elif method.startswith(\"topk_mean\"):\n",
    "        try:\n",
    "            frac = float(method.split(\":\",1)[1]) if \":\" in method else 0.5\n",
    "        except Exception:\n",
    "            frac = 0.5\n",
    "        k = max(1, int(len(probs) * frac))\n",
    "        idx = np.argsort(-probs)[:k]\n",
    "        p = probs[idx].mean()\n",
    "    elif method == \"mean_logit\":\n",
    "        lgs = np.array([_safe_logit(float(x)) for x in probs], np.float32)\n",
    "        lg = float(np.clip(lgs.mean(), -20.0, 20.0))\n",
    "        p = float(1.0/(1.0+np.exp(-lg)))\n",
    "    else:\n",
    "        p = probs.mean()\n",
    "    return float(np.clip(p, 1e-6, 1-1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e179c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://12cfe6a533f3b00520.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://12cfe6a533f3b00520.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_28514/4227975594.py:128: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  base_rgb = base_img.convert('RGB'); heat_img = Image.fromarray(heat_rgb, mode='RGB')\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def run_once(img_path_or_pil, tile, stride, use_clip, enable_prnu, enable_ela, enable_clip, alpha, progress=gr.Progress()):\n",
    "    \"\"\"\n",
    "    跑一次完整推論：支援進度條（內部以階段更新），輸出 overlay 與 raw 結果。\n",
    "    \"\"\"\n",
    "    progress(0.0, desc=\"準備中…\")\n",
    "    # 準備臨時檔（Gradio 可能傳 PIL 或檔路徑）\n",
    "    if isinstance(img_path_or_pil, str):\n",
    "        p = Path(img_path_or_pil)\n",
    "    else:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f:\n",
    "            img_path_or_pil.save(f.name)\n",
    "            p = Path(f.name)\n",
    "\n",
    "    enable = {\"prnu\": bool(enable_prnu), \"ela\": bool(enable_ela), \"clip\": bool(enable_clip)}\n",
    "    if not use_clip:\n",
    "        enable[\"clip\"] = False\n",
    "\n",
    "    progress(0.2, desc=\"切 tile / 前處理…\")\n",
    "    res = infer_image_by_tiles(\n",
    "        p, tile=int(tile), stride=int(stride),\n",
    "        aggregate='mean_prob',              # 實際 tiles 的 prob_fake 與彙總無關；之後再事後重算\n",
    "        use_clip=bool(use_clip),\n",
    "        save_overlay=True,\n",
    "        overlay_alpha=float(alpha),\n",
    "        enable=enable\n",
    "    )\n",
    "    progress(0.85, desc=\"產生可視化…\")\n",
    "    overlay = None\n",
    "    if res.get(\"overlay_path\"):\n",
    "        overlay = Image.open(res[\"overlay_path\"]).convert(\"RGB\")\n",
    "\n",
    "    # 整理 caption 與平均 logit 顯示\n",
    "    flog = res.get(\"feature_avg_logits\", {})\n",
    "    msg = (f\"PRNU logit(avg)={flog.get('prnu')} | \"\n",
    "           f\"ELA logit(avg)={flog.get('ela')} | \"\n",
    "           f\"CLIP logit(avg)={flog.get('clip')}\")\n",
    "    progress(1.0, desc=\"完成\")\n",
    "    return msg, overlay, res\n",
    "\n",
    "def reaggregate(raw_result: dict, methods: list[str]):\n",
    "    if not raw_result or \"tiles\" not in raw_result:\n",
    "        return \"尚未有推論結果\", []\n",
    "    tiles = raw_result[\"tiles\"]\n",
    "    table = []\n",
    "    best = (\"\", -1.0, 0)\n",
    "    for m in (methods or []):\n",
    "        p = aggregate_probs_offline(tiles, m)\n",
    "        y = int(p >= 0.5)\n",
    "        table.append([m, round(float(p), 6), y])\n",
    "        if p > best[1]:\n",
    "            best = (m, p, y)\n",
    "    if not methods:\n",
    "        return \"請至少選擇一個彙總方式\", []\n",
    "    msg = f\"最佳：{best[0]}  prob={best[1]:.4f}  pred={best[2]}\"\n",
    "    return msg, table\n",
    "\n",
    "with gr.Blocks(title=\"Slide-256 Detector — PRNU/ELA/CLIP + Fusion (CUDA)\") as demo:\n",
    "    gr.Markdown(\"## Slide-256 Full-Image Inference — PRNU / ELA / CLIP + Fusion\\n上傳或拍照 → 跑一次 → 右側彙總（不重跑）\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            inp = gr.Image(type=\"filepath\", label=\"上傳或拍照\", sources=[\"upload\",\"webcam\",\"clipboard\"], height=320)\n",
    "            with gr.Row():\n",
    "                tile   = gr.Number(value=TILE, precision=0, label=\"Tile\")\n",
    "                stride = gr.Number(value=STRIDE, precision=0, label=\"Stride\")\n",
    "            use_clip   = gr.Checkbox(value=DEFAULT_ENABLED.get(\"clip\", True), label=\"Use CLIP head\")\n",
    "            with gr.Row():\n",
    "                enable_prnu = gr.Checkbox(value=DEFAULT_ENABLED.get(\"prnu\", True), label=\"Enable PRNU\")\n",
    "                enable_ela  = gr.Checkbox(value=DEFAULT_ENABLED.get(\"ela\",  True), label=\"Enable ELA\")\n",
    "                enable_clip = gr.Checkbox(value=DEFAULT_ENABLED.get(\"clip\", True), label=\"Enable CLIP\")\n",
    "            alpha      = gr.Slider(0.0, 1.0, value=0.35, step=0.05, label=\"Overlay alpha\")\n",
    "            run_btn    = gr.Button(\"開始推論\", variant=\"primary\")\n",
    "        with gr.Column(scale=1):\n",
    "            caption = gr.Label(label=\"平均 logit / 訊息\")\n",
    "            overlay = gr.Image(label=\"Overlay（熱度 + tile 分數）\")\n",
    "            raw     = gr.JSON(label=\"Raw 結果（tiles）\")\n",
    "\n",
    "            methods = gr.CheckboxGroup(\n",
    "                choices=[\"mean_prob\",\"max_prob\",\"topk_mean:0.5\",\"topk_mean:0.25\",\"mean_logit\"],\n",
    "                value=[\"topk_mean:0.5\",\"mean_logit\"],\n",
    "                label=\"選擇一個或多個彙總方式（不重跑）\"\n",
    "            )\n",
    "            recompute = gr.Button(\"重新計算彙總\")\n",
    "            table = gr.Dataframe(\n",
    "                headers=[\"method\",\"prob_fake\",\"pred_label\"],\n",
    "                datatype=[\"str\",\"number\",\"number\"],\n",
    "                interactive=False,\n",
    "                label=\"各彙總方式結果\"\n",
    "            )\n",
    "            out_msg = gr.Label(label=\"彙總訊息 / 最佳策略\")\n",
    "\n",
    "    run_btn.click(\n",
    "        run_once,\n",
    "        inputs=[inp, tile, stride, use_clip, enable_prnu, enable_ela, enable_clip, alpha],\n",
    "        outputs=[caption, overlay, raw]\n",
    "    )\n",
    "    recompute.click(\n",
    "        reaggregate,\n",
    "        inputs=[raw, methods],\n",
    "        outputs=[out_msg, table]\n",
    "    )\n",
    "\n",
    "demo.queue(max_size=16).launch(share=True, inline=True)  # share=True 方便手機示範\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
