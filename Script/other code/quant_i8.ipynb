{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fde50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 只做量化：PRNU float -> int8（無 zst 版）===\n",
    "# 特色：\n",
    "# 1) MODE='per_dataset'：每個資料集一個 scale S，兼顧精度與速度\n",
    "# 2) 取分位數用 np.partition 的近似法 + 子抽樣像素，極快且準\n",
    "# 3) 多進程平行處理\n",
    "# 4) 直接輸出 .npy（dtype=int8），並寫 meta.json 紀錄各 dataset 的 S\n",
    "\n",
    "import os, re, json, math, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ========= 路徑設定 =========\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "SRC_REAL_F32 = SCRIPT_ROOT/\"features_npy/prnu_real_npy\"    # 原始 float PRNU (real)\n",
    "SRC_FAKE_F32 = SCRIPT_ROOT/\"features_npy/prnu_fake_npy\"    # 原始 float PRNU (fake)\n",
    "\n",
    "DST_REAL_I8  = SCRIPT_ROOT/\"features_i8/prnu_real_i8_npy\"  # 量化 int8 輸出 (real)\n",
    "DST_FAKE_I8  = SCRIPT_ROOT/\"features_i8/prnu_fake_i8_npy\"  # 量化 int8 輸出 (fake)\n",
    "DST_REAL_I8.mkdir(parents=True, exist_ok=True)\n",
    "DST_FAKE_I8.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 主要參數 =========\n",
    "MODE = \"per_dataset\"       # 可選：'global'（最快）、'per_dataset'（推薦）、'per_file'（最保真但慢）\n",
    "PERCENTILE = 0.999         # p99.9\n",
    "SAMPLE_FILES_PER_DS = 500  # 每資料集抽幾個檔來估 S\n",
    "SAMPLE_PIXELS_PER_FILE = 4096  # 每檔抽幾個像素估分位數\n",
    "NPROC = max(1, (os.cpu_count() or 2) - 1)  # 進程數\n",
    "\n",
    "DATASET_KEYWORDS = [\n",
    "    'imagenet','flickr30k','unsplash','places365','coco2017','coco','div2k',\n",
    "    'sd3','midjourney-v6-llava','flux','dalle3','stablediffusion','midjourney'\n",
    "]\n",
    "\n",
    "# ========= 工具 =========\n",
    "def list_files(root: Path) -> List[Path]:\n",
    "    return sorted([p for p in root.rglob(\"*.npy\")] + [p for p in root.rglob(\"*.npz\")])\n",
    "\n",
    "def infer_dataset_from_stem(stem: str) -> str:\n",
    "    s = stem.lower()\n",
    "    for k in DATASET_KEYWORDS:\n",
    "        if k in s: return k\n",
    "    m = re.match(r'([a-z0-9\\-]+)[_\\-]', s)\n",
    "    return m.group(1) if m else \"unknown\"\n",
    "\n",
    "def load_prnu_2d(path: Path) -> np.ndarray:\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    if isinstance(z, np.lib.npyio.NpzFile):\n",
    "        for k in ('prnu','noise','arr','arr_0','data'):\n",
    "            if k in z.files:\n",
    "                a = z[k]; break\n",
    "        else:\n",
    "            a = z[z.files[0]]\n",
    "    else:\n",
    "        a = z\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[-1] in (1,3): a = a.mean(axis=2)\n",
    "        elif a.shape[0] in (1,3): a = a.mean(axis=0)\n",
    "        else: a = a.squeeze()\n",
    "    assert a.ndim == 2, f\"Expect 2D, got {a.shape} from {path}\"\n",
    "    return a\n",
    "\n",
    "def subsample_vals_abs(a: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"從 |a| 隨機抽 k 個像素（不足則全取），避免整張展平成新陣列造成大量記憶體。\"\"\"\n",
    "    v = a.reshape(-1)\n",
    "    n = v.size\n",
    "    if n <= k:\n",
    "        return np.abs(v.astype(np.float32, copy=False))\n",
    "    idx = np.random.default_rng(1337).integers(0, n, size=k, endpoint=False)\n",
    "    return np.abs(v[idx].astype(np.float32, copy=False))\n",
    "\n",
    "def fast_percentile_abs(v: np.ndarray, q: float) -> float:\n",
    "    if v.size == 0: return 1e-8\n",
    "    k = int(q * (v.size - 1))\n",
    "    vk = np.partition(v, k)[k]\n",
    "    return float(max(vk, 1e-8))\n",
    "\n",
    "def quantize_i8(a: np.ndarray, S: float) -> np.ndarray:\n",
    "    x = np.clip(a, -S, S) / S * 127.0\n",
    "    q = np.rint(x).astype(np.int16)\n",
    "    q = np.clip(q, -127, 127).astype(np.int8)\n",
    "    return q\n",
    "\n",
    "def save_npy(out_base: Path, q: np.ndarray):\n",
    "    out_path = out_base.with_suffix(\".npy\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # allow_pickle=False 以避免帶入非必要資訊\n",
    "    np.save(out_path, q, allow_pickle=False)\n",
    "    return out_path\n",
    "\n",
    "# ========= 估 scale S =========\n",
    "def estimate_global_S(files: List[Path], sample_files=20000, sample_pixels=4096, q=0.999) -> float:\n",
    "    pick = files if len(files) <= sample_files else list(np.random.default_rng(1337).choice(files, sample_files, replace=False))\n",
    "    vals = []\n",
    "    for p in tqdm(pick, desc=\"scan global S\"):\n",
    "        a = load_prnu_2d(p)\n",
    "        vals.append(subsample_vals_abs(a, sample_pixels))\n",
    "    v = np.concatenate(vals, axis=0) if len(vals) else np.array([1.0], dtype=np.float32)\n",
    "    return fast_percentile_abs(v, q)\n",
    "\n",
    "def estimate_S_per_dataset(files: List[Path], sample_files_per_ds=500, sample_pixels=4096, q=0.999) -> Dict[str, float]:\n",
    "    buckets: Dict[str, List[Path]] = {}\n",
    "    for p in files:\n",
    "        ds = infer_dataset_from_stem(p.stem)\n",
    "        buckets.setdefault(ds, []).append(p)\n",
    "    ds2S = {}\n",
    "    for ds, lst in buckets.items():\n",
    "        pick = lst if len(lst) <= sample_files_per_ds else list(np.random.default_rng(1337).choice(lst, sample_files_per_ds, replace=False))\n",
    "        vals = []\n",
    "        for p in tqdm(pick, desc=f\"scan S[{ds}]\"):\n",
    "            a = load_prnu_2d(p)\n",
    "            vals.append(subsample_vals_abs(a, sample_pixels))\n",
    "        v = np.concatenate(vals, 0) if len(vals) else np.array([1.0], dtype=np.float32)\n",
    "        ds2S[ds] = fast_percentile_abs(v, q)\n",
    "    return ds2S\n",
    "\n",
    "# ========= worker：單檔處理 =========\n",
    "def worker_convert_one(path: Path, out_dir: Path, mode: str, S_global: float, ds2S: Dict[str,float],\n",
    "                       skip_existing: bool=True) -> Tuple[str, str]:\n",
    "    try:\n",
    "        stem = path.stem\n",
    "        out_base = (out_dir / stem)\n",
    "        out_path = out_base.with_suffix(\".npy\")\n",
    "        if skip_existing and out_path.exists():\n",
    "            return (\"skip\", str(out_path))\n",
    "        a = load_prnu_2d(path)\n",
    "        if mode == \"per_file\":\n",
    "            v = subsample_vals_abs(a, SAMPLE_PIXELS_PER_FILE)\n",
    "            S = fast_percentile_abs(v, PERCENTILE)\n",
    "        elif mode == \"per_dataset\":\n",
    "            ds = infer_dataset_from_stem(stem)\n",
    "            S = ds2S.get(ds, S_global)\n",
    "        else:  # global\n",
    "            S = S_global\n",
    "        q = quantize_i8(a, S)\n",
    "        save_npy(out_base, q)\n",
    "        return (\"ok\", str(out_path))\n",
    "    except Exception as e:\n",
    "        return (\"err\", f\"{path} -> {e}\")\n",
    "\n",
    "# ========= 主流程：一側（real 或 fake）量化 =========\n",
    "def run_quantize_side(src_dir: Path, dst_dir: Path, mode=\"per_dataset\"):\n",
    "    files = list_files(src_dir)\n",
    "    if not files:\n",
    "        print(f\"[WARN] no files in {src_dir}\"); return\n",
    "\n",
    "    print(f\"MODE = {mode}\")\n",
    "    S_global = estimate_global_S(files, sample_files=min(20000, len(files)),\n",
    "                                 sample_pixels=SAMPLE_PIXELS_PER_FILE, q=PERCENTILE)\n",
    "    print(f\"Global S = {S_global:.6g}\")\n",
    "\n",
    "    ds2S = {}\n",
    "    if mode == \"per_dataset\":\n",
    "        ds2S = estimate_S_per_dataset(files, sample_files_per_ds=SAMPLE_FILES_PER_DS,\n",
    "                                      sample_pixels=SAMPLE_PIXELS_PER_FILE, q=PERCENTILE)\n",
    "        print(\"Per-dataset S:\", {k: round(v,6) for k,v in ds2S.items()})\n",
    "\n",
    "    # 存一份 meta（便於之後還原或追蹤）\n",
    "    meta = {\n",
    "        \"mode\": mode,\n",
    "        \"percentile\": PERCENTILE,\n",
    "        \"global_S\": S_global,\n",
    "        \"per_dataset_S\": ds2S,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"src\": str(src_dir), \"dst\": str(dst_dir),\n",
    "        \"output_format\": \"npy(int8)\"\n",
    "    }\n",
    "    (dst_dir.parent/\"prnu_i8_meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # 多進程平行處理\n",
    "    fn = partial(worker_convert_one, out_dir=dst_dir, mode=mode, S_global=S_global, ds2S=ds2S,\n",
    "                 skip_existing=True)\n",
    "    ok=skip=err=0\n",
    "    with mp.Pool(processes=NPROC, maxtasksperchild=200) as pool:\n",
    "        for status, msg in tqdm(pool.imap_unordered(fn, files, chunksize=64), total=len(files), desc=f\"quantize→{dst_dir.name}\"):\n",
    "            if status==\"ok\": ok+=1\n",
    "            elif status==\"skip\": skip+=1\n",
    "            else:\n",
    "                err+=1; print(\"[ERR]\", msg)\n",
    "    print(f\"完成：ok={ok}, skip={skip}, err={err}, out_dir={dst_dir}\")\n",
    "\n",
    "# ======== 執行（一次一側；你可先跑 real，再跑 fake）========\n",
    "run_quantize_side(SRC_REAL_F32, DST_REAL_I8, mode=MODE)\n",
    "run_quantize_side(SRC_FAKE_F32, DST_FAKE_I8, mode=MODE)\n",
    "\n",
    "print(\"✅ Done. 輸出為 .npy（int8）；meta 寫在：\", (DST_REAL_I8.parent/\"prnu_i8_meta.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137f599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 將處理 19000 個檔案，datasets = ['dalle3']\n",
      "MODE = per_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scan global S: 100%|██████████| 19000/19000 [00:37<00:00, 510.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global S (filtered) = 0.0596475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scan S[dalle3]: 100%|██████████| 500/500 [00:00<00:00, 3525.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-dataset S: {'dalle3': 0.046448}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "quantize→prnu_fake_i8_npy: 100%|██████████| 19000/19000 [00:04<00:00, 4230.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成：ok=19000, skip=0, err=0, out_dir=/home/yaya/ai-detect-proj/Script/features_i8/prnu_fake_i8_npy\n",
      "✅ Done. 只量化這些資料集： ['dalle3']\n",
      "   輸出為 int8 .npy；meta 寫在： /home/yaya/ai-detect-proj/Script/features_i8/prnu_i8_meta.json\n"
     ]
    }
   ],
   "source": [
    "# === PRNU float -> int8（單一資料集版；無 zst）===\n",
    "# 只針對指定資料集（例如 dalle3）做量化；其餘略過\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ========= 路徑設定（依你的環境調整）=========\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "\n",
    "# 來源（float PRNU）：如果要量化 fake/dalle3，通常在 prnu_fake_npy\n",
    "SRC_DIR = SCRIPT_ROOT / \"features_npy\" / \"prnu_fake_npy\"\n",
    "# 輸出（int8 PRNU）\n",
    "DST_DIR = SCRIPT_ROOT / \"features_i8\" / \"prnu_fake_i8_npy\"\n",
    "DST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 僅處理這些資料集（可放多個），預設只做 dalle3\n",
    "INCLUDE_TAGS = {\"dalle3\"}\n",
    "\n",
    "# ========= 量化參數 =========\n",
    "MODE = \"per_dataset\"           # 'global' | 'per_dataset' | 'per_file'\n",
    "PERCENTILE = 0.999             # p99.9\n",
    "SAMPLE_FILES_PER_DS = 500      # 估 S 時每資料集抽樣的檔數上限\n",
    "SAMPLE_PIXELS_PER_FILE = 4096  # 估分位數時，每檔抽樣像素數\n",
    "NPROC = max(1, (os.cpu_count() or 2) - 1)\n",
    "SEED = 1337\n",
    "\n",
    "# ========= 來源關鍵字/別名（含 dalle-3 -> dalle3）=========\n",
    "ALIASES = {\n",
    "    \"imagenet1k\":\"imagenet\", \"imgnet\":\"imagenet\", \"imagenet\":\"imagenet\",\n",
    "    \"unslpash\":\"unsplash\", \"unsplash\":\"unsplash\",\n",
    "    \"flicker30k\":\"flickr30k\", \"flickr30K\":\"flickr30k\", \"flickr30k\":\"flickr30k\",\n",
    "    \"places365\":\"places365\", \"coco2017\":\"coco2017\", \"div2k\":\"div2k\",\n",
    "    \"sd3\":\"sd3\", \"sdxl\":\"sd3\",\n",
    "    \"flux\":\"flux\", \"black-forest-labs\":\"flux\",\n",
    "    \"dalle-3\":\"dalle3\", \"dalle3\":\"dalle3\",\n",
    "    \"midjourney-v6-llava\":\"midjourney\", \"midjourney\":\"midjourney\",\n",
    "}\n",
    "DATASET_KEYWORDS = list(set(ALIASES.values()) | set(ALIASES.keys()))\n",
    "\n",
    "def canonical(tag:str)->str:\n",
    "    return ALIASES.get(tag.lower().strip(), tag.lower().strip())\n",
    "\n",
    "def infer_dataset_from_stem(stem: str) -> str:\n",
    "    s = stem.lower()\n",
    "    # 先匹配較長別名\n",
    "    for k in sorted(DATASET_KEYWORDS, key=len, reverse=True):\n",
    "        if k in s:\n",
    "            return canonical(k)\n",
    "    m = re.match(r'([a-z0-9\\-]+)[_\\-]', s)\n",
    "    return canonical(m.group(1)) if m else \"unknown\"\n",
    "\n",
    "# ========= IO / 讀檔 =========\n",
    "def list_files(root: Path) -> List[Path]:\n",
    "    return sorted([*root.rglob(\"*.npy\"), *root.rglob(\"*.npz\")])\n",
    "\n",
    "def load_prnu_2d(path: Path) -> np.ndarray:\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    if isinstance(z, np.lib.npyio.NpzFile):\n",
    "        for k in ('prnu','noise','arr','arr_0','data'):\n",
    "            if k in z.files:\n",
    "                a = z[k]; break\n",
    "        else:\n",
    "            a = z[z.files[0]]\n",
    "    else:\n",
    "        a = z\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[-1] in (1,3): a = a.mean(axis=2)\n",
    "        elif a.shape[0] in (1,3): a = a.mean(axis=0)\n",
    "        else: a = a.squeeze()\n",
    "    assert a.ndim == 2, f\"Expect 2D, got {a.shape} from {path}\"\n",
    "    return a\n",
    "\n",
    "def subsample_vals_abs(a: np.ndarray, k: int) -> np.ndarray:\n",
    "    v = a.reshape(-1)\n",
    "    n = v.size\n",
    "    if n <= k:\n",
    "        return np.abs(v.astype(np.float32, copy=False))\n",
    "    idx = np.random.default_rng(SEED).integers(0, n, size=k, endpoint=False)\n",
    "    return np.abs(v[idx].astype(np.float32, copy=False))\n",
    "\n",
    "def fast_percentile_abs(v: np.ndarray, q: float) -> float:\n",
    "    if v.size == 0: return 1e-8\n",
    "    k = int(q * (v.size - 1))\n",
    "    vk = np.partition(v, k)[k]\n",
    "    return float(max(vk, 1e-8))\n",
    "\n",
    "def quantize_i8(a: np.ndarray, S: float) -> np.ndarray:\n",
    "    x = np.clip(a, -S, S) / S * 127.0\n",
    "    q = np.rint(x).astype(np.int16)\n",
    "    q = np.clip(q, -127, 127).astype(np.int8)\n",
    "    return q\n",
    "\n",
    "def save_npy(out_base: Path, q: np.ndarray):\n",
    "    out_path = out_base.with_suffix(\".npy\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(out_path, q, allow_pickle=False)\n",
    "    return out_path\n",
    "\n",
    "# ========= 僅保留指定資料集的檔案 =========\n",
    "def filter_files_by_tags(files: List[Path], include_tags:set) -> List[Path]:\n",
    "    keep=[]\n",
    "    for p in files:\n",
    "        tag = infer_dataset_from_stem(p.stem)\n",
    "        if tag in include_tags:\n",
    "            keep.append(p)\n",
    "    return keep\n",
    "\n",
    "# ========= 估 S =========\n",
    "def estimate_global_S(files: List[Path], sample_files=20000, sample_pixels=4096, q=0.999) -> float:\n",
    "    pick = files if len(files) <= sample_files else list(np.random.default_rng(SEED).choice(files, sample_files, replace=False))\n",
    "    vals = []\n",
    "    for p in tqdm(pick, desc=\"scan global S\"):\n",
    "        a = load_prnu_2d(p)\n",
    "        vals.append(subsample_vals_abs(a, sample_pixels))\n",
    "    v = np.concatenate(vals, axis=0) if len(vals) else np.array([1.0], dtype=np.float32)\n",
    "    return fast_percentile_abs(v, q)\n",
    "\n",
    "def estimate_S_per_dataset(files: List[Path], sample_files_per_ds=500, sample_pixels=4096, q=0.999) -> Dict[str, float]:\n",
    "    buckets: Dict[str, List[Path]] = {}\n",
    "    for p in files:\n",
    "        ds = infer_dataset_from_stem(p.stem)\n",
    "        buckets.setdefault(ds, []).append(p)\n",
    "    ds2S = {}\n",
    "    for ds, lst in buckets.items():\n",
    "        pick = lst if len(lst) <= sample_files_per_ds else list(np.random.default_rng(SEED).choice(lst, sample_files_per_ds, replace=False))\n",
    "        vals = []\n",
    "        for p in tqdm(pick, desc=f\"scan S[{ds}]\"):\n",
    "            a = load_prnu_2d(p)\n",
    "            vals.append(subsample_vals_abs(a, sample_pixels))\n",
    "        v = np.concatenate(vals, 0) if len(vals) else np.array([1.0], dtype=np.float32)\n",
    "        ds2S[ds] = fast_percentile_abs(v, q)\n",
    "    return ds2S\n",
    "\n",
    "# ========= 單檔 worker =========\n",
    "def worker_convert_one(path: Path, out_dir: Path, mode: str, S_global: float, ds2S: Dict[str,float],\n",
    "                       skip_existing: bool=True) -> Tuple[str, str]:\n",
    "    try:\n",
    "        stem = path.stem\n",
    "        out_base = (out_dir / stem)\n",
    "        out_path = out_base.with_suffix(\".npy\")\n",
    "        if skip_existing and out_path.exists():\n",
    "            return (\"skip\", str(out_path))\n",
    "        a = load_prnu_2d(path)\n",
    "        if mode == \"per_file\":\n",
    "            v = subsample_vals_abs(a, SAMPLE_PIXELS_PER_FILE)\n",
    "            S = fast_percentile_abs(v, PERCENTILE)\n",
    "        elif mode == \"per_dataset\":\n",
    "            ds = infer_dataset_from_stem(stem)\n",
    "            S = ds2S.get(ds, S_global)\n",
    "        else:\n",
    "            S = S_global\n",
    "        q = quantize_i8(a, S)\n",
    "        save_npy(out_base, q)\n",
    "        return (\"ok\", str(out_path))\n",
    "    except Exception as e:\n",
    "        return (\"err\", f\"{path} -> {e}\")\n",
    "\n",
    "# ========= 主流程（僅單一/少量資料集）=========\n",
    "def run_quantize_single_dataset(src_dir: Path, dst_dir: Path, include_tags:set, mode=\"per_dataset\"):\n",
    "    all_files = list_files(src_dir)\n",
    "    files = filter_files_by_tags(all_files, include_tags)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"在 {src_dir} 找不到符合 {include_tags} 的 .npy/.npz 檔案\")\n",
    "    print(f\"[INFO] 將處理 {len(files)} 個檔案，datasets = {sorted(list(include_tags))}\")\n",
    "\n",
    "    print(f\"MODE = {mode}\")\n",
    "    S_global = estimate_global_S(files, sample_files=min(20000, len(files)),\n",
    "                                 sample_pixels=SAMPLE_PIXELS_PER_FILE, q=PERCENTILE)\n",
    "    print(f\"Global S (filtered) = {S_global:.6g}\")\n",
    "\n",
    "    ds2S = {}\n",
    "    if mode == \"per_dataset\":\n",
    "        ds2S = estimate_S_per_dataset(files, sample_files_per_ds=SAMPLE_FILES_PER_DS,\n",
    "                                      sample_pixels=SAMPLE_PIXELS_PER_FILE, q=PERCENTILE)\n",
    "        print(\"Per-dataset S:\", {k: round(v,6) for k,v in ds2S.items()})\n",
    "\n",
    "    meta = {\n",
    "        \"mode\": mode,\n",
    "        \"percentile\": PERCENTILE,\n",
    "        \"global_S_filtered\": S_global,\n",
    "        \"per_dataset_S\": ds2S,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"src\": str(src_dir), \"dst\": str(dst_dir),\n",
    "        \"include_tags\": sorted(list(include_tags)),\n",
    "        \"output_format\": \"npy(int8)\"\n",
    "    }\n",
    "    (dst_dir.parent/\"prnu_i8_meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "    fn = partial(worker_convert_one, out_dir=dst_dir, mode=mode, S_global=S_global, ds2S=ds2S,\n",
    "                 skip_existing=True)\n",
    "    ok=skip=err=0\n",
    "    with mp.Pool(processes=NPROC, maxtasksperchild=200) as pool:\n",
    "        for status, msg in tqdm(pool.imap_unordered(fn, files, chunksize=64), total=len(files), desc=f\"quantize→{dst_dir.name}\"):\n",
    "            if status==\"ok\": ok+=1\n",
    "            elif status==\"skip\": skip+=1\n",
    "            else:\n",
    "                err+=1; print(\"[ERR]\", msg)\n",
    "    print(f\"完成：ok={ok}, skip={skip}, err={err}, out_dir={dst_dir}\")\n",
    "\n",
    "# ======== 執行（僅 dalle3）=========\n",
    "run_quantize_single_dataset(SRC_DIR, DST_DIR, include_tags=INCLUDE_TAGS, mode=MODE)\n",
    "print(\"✅ Done. 只量化這些資料集：\", sorted(list(INCLUDE_TAGS)))\n",
    "print(\"   輸出為 int8 .npy；meta 寫在：\", (DST_DIR.parent/\"prnu_i8_meta.json\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
