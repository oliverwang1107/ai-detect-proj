{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d266030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "偵測到特徵： ['ela', 'clip', 'prnu']\n",
      "可操作（特徵交集）fake IDs： 89000\n",
      "\n",
      "來源現況（可用 → 保留 ≤ 25K）：\n",
      "  FLUX                    20000 →  20000\n",
      "  SD3                     50000 →  25000\n",
      "  dalle3                  19000 →  19000\n",
      "將移除的 ID 數： 25000\n",
      "\n",
      "Planned removals: 75000 files\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000003.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000007.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000012.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000013.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000015.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000016.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000018.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000019.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000022.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000024.npy\n",
      "🗑 已移到垃圾桶： 75000\n",
      "After cleanup → intersection fake IDs by source:\n",
      "  FLUX                    20000\n",
      "  SD3                     25000\n",
      "  dalle3                  19000\n",
      "Trash dir: /home/yaya/ai-detect-proj/Script/features_256/_trash_fake_pergen_20250821_121322\n"
     ]
    }
   ],
   "source": [
    "# ==== Downsample FAKE: each generator/source -> keep at most 25K IDs ====\n",
    "from pathlib import Path\n",
    "import re, random, time, shutil\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "FEA_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_256\")  # 你的特徵根目錄（會遞迴掃描）\n",
    "PER_GEN_TARGET = 25_000               # 每個生成器想保留的 ID 數上限\n",
    "SEED = 42; random.seed(SEED)\n",
    "\n",
    "DRY_RUN = False                        # 先預覽；OK 後改 False 真的動作\n",
    "MODE = \"trash\"                        # \"trash\"=移到垃圾桶 | \"delete\"=直接刪除\n",
    "KEEP_ALL_ELA_QUALS = True             # True=保留該ID所有 ELA 品質檔；False=只留一個（靠近 q90）\n",
    "\n",
    "TRASH_DIR = FEA_ROOT / f\"_trash_fake_pergen_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# ---------- 小工具 ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def ela_quality(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def list_feature_dirs(root: Path, feat: str, cls: str):\n",
    "    return [d for d in root.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_npy(d: Path):\n",
    "    return sorted(p for p in d.glob(\"*.npy\"))\n",
    "\n",
    "def dataset_of(img_id: str):\n",
    "    return img_id.split(\"__\", 1)[0]\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def move_to_trash(p: Path):\n",
    "    rel = p.relative_to(FEA_ROOT)\n",
    "    dst = TRASH_DIR / rel\n",
    "    ensure_dir(dst.parent)\n",
    "    shutil.move(str(p), str(dst))\n",
    "\n",
    "# ---------- 掃描 fake 的各特徵路徑 ----------\n",
    "dirs = {\n",
    "    (\"ela\",\"fake\"):  list_feature_dirs(FEA_ROOT, \"ela\",  \"fake\"),\n",
    "    (\"clip\",\"fake\"): list_feature_dirs(FEA_ROOT, \"clip\", \"fake\"),\n",
    "    (\"prnu\",\"fake\"): list_feature_dirs(FEA_ROOT, \"prnu\", \"fake\"),\n",
    "}\n",
    "present_feats = [feat for (feat,_), lst in dirs.items() if lst]\n",
    "assert present_feats, \"找不到任何 *_fake_npy 目錄，請確認 FEA_ROOT 設定。\"\n",
    "print(\"偵測到特徵：\", present_feats)\n",
    "\n",
    "# ---------- 建立各特徵的 fake ID 集合（ELA 用 base id） ----------\n",
    "id_sets = {}\n",
    "for feat in present_feats:\n",
    "    s = set()\n",
    "    for d in dirs[(feat,\"fake\")]:\n",
    "        for p in list_npy(d):\n",
    "            s.add(base_id(p.stem) if feat==\"ela\" else p.stem)\n",
    "    id_sets[feat] = s\n",
    "\n",
    "# 交集（僅在目前可同時取得之 IDs 中動作）\n",
    "ids_common = None\n",
    "for feat, s in id_sets.items():\n",
    "    ids_common = s if ids_common is None else (ids_common & s)\n",
    "print(\"可操作（特徵交集）fake IDs：\", len(ids_common))\n",
    "\n",
    "# ---------- 依生成器分桶，決定每個生成器保留到 25K ----------\n",
    "bucket = defaultdict(list)\n",
    "for i in ids_common:\n",
    "    bucket[dataset_of(i)].append(i)\n",
    "for k in bucket: random.shuffle(bucket[k])\n",
    "\n",
    "keep_ids = set()\n",
    "per_src_keep = {}\n",
    "per_src_avail = {src: len(lst) for src, lst in bucket.items()}\n",
    "for src, lst in bucket.items():\n",
    "    k = min(len(lst), PER_GEN_TARGET)\n",
    "    per_src_keep[src] = k\n",
    "    keep_ids.update(lst[:k])\n",
    "\n",
    "print(\"\\n來源現況（可用 → 保留 ≤ 25K）：\")\n",
    "for src in sorted(bucket.keys()):\n",
    "    print(f\"  {src:22s} {per_src_avail[src]:6d} → {per_src_keep[src]:6d}\")\n",
    "print(\"將移除的 ID 數：\", len(ids_common) - len(keep_ids))\n",
    "\n",
    "# ---------- 建立刪除清單 ----------\n",
    "to_remove = []           # 要刪/移的檔案路徑\n",
    "to_keep_one_ela = {}     # 當只保留單一品質時，記錄每個 base id 應保留的那個檔\n",
    "\n",
    "if not KEEP_ALL_ELA_QUALS:\n",
    "    # 先決定每個保留 ID 的唯一 ELA 檔（優先 q90，其次最接近 90）\n",
    "    for d in dirs.get((\"ela\",\"fake\"), []):\n",
    "        # 把該目錄所有檔按 base 分桶\n",
    "        by_base = defaultdict(list)\n",
    "        for p in list_npy(d):\n",
    "            b = base_id(p.stem)\n",
    "            if b in keep_ids:\n",
    "                by_base[b].append(p)\n",
    "        for b, lst in by_base.items():\n",
    "            best = None\n",
    "            best_score = (999, \"\")\n",
    "            for pp in lst:\n",
    "                q = ela_quality(pp.stem)\n",
    "                score = (0 if q == 90 else (abs(q-90) if q is not None else 999), pp.name)\n",
    "                if score < best_score:\n",
    "                    best_score, best = score, pp\n",
    "            to_keep_one_ela[b] = best\n",
    "\n",
    "# 只對「在交集內而不在 keep_ids」的檔動作（避免誤刪其他孤兒或未抽到的特徵）\n",
    "for feat in present_feats:\n",
    "    for d in dirs[(feat,\"fake\")]:\n",
    "        for p in list_npy(d):\n",
    "            id_base = base_id(p.stem) if feat==\"ela\" else p.stem\n",
    "            if id_base in ids_common:\n",
    "                if id_base not in keep_ids:\n",
    "                    to_remove.append(p)\n",
    "                elif feat==\"ela\" and (not KEEP_ALL_ELA_QUALS) and (to_keep_one_ela.get(id_base) != p):\n",
    "                    to_remove.append(p)\n",
    "\n",
    "print(f\"\\nPlanned removals: {len(to_remove)} files\")\n",
    "for p in to_remove[:10]:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "# ---------- 執行 ----------\n",
    "if DRY_RUN:\n",
    "    print(\"\\nDRY_RUN=True → 只預覽，不動作。確認 OK 後把 DRY_RUN=False 重新跑。\")\n",
    "else:\n",
    "    removed = 0\n",
    "    if MODE == \"trash\":\n",
    "        ensure_dir(TRASH_DIR)\n",
    "    for p in to_remove:\n",
    "        try:\n",
    "            if MODE == \"trash\":\n",
    "                move_to_trash(p)\n",
    "            else:\n",
    "                p.unlink(missing_ok=True)\n",
    "            removed += 1\n",
    "        except Exception as e:\n",
    "            print(\"skip (error):\", p, \"|\", e)\n",
    "    print((\"🗑 已移到垃圾桶：\" if MODE==\"trash\" else \"🗑 已刪除：\"), removed)\n",
    "\n",
    "    # 收尾統計（現在各來源的交集/保留狀況）\n",
    "    # 重新掃描交集\n",
    "    id_sets2 = {}\n",
    "    for feat in present_feats:\n",
    "        s2 = set()\n",
    "        for d in dirs[(feat,\"fake\")]:\n",
    "            for p in list_npy(d):\n",
    "                s2.add(base_id(p.stem) if feat==\"ela\" else p.stem)\n",
    "        id_sets2[feat] = s2\n",
    "    ids_common2 = None\n",
    "    for feat, s in id_sets2.items():\n",
    "        ids_common2 = s if ids_common2 is None else (ids_common2 & s)\n",
    "\n",
    "    by_src2 = Counter(dataset_of(i) for i in ids_common2)\n",
    "    print(\"After cleanup → intersection fake IDs by source:\")\n",
    "    for src in sorted(by_src2.keys()):\n",
    "        print(f\"  {src:22s} {by_src2[src]:6d}\")\n",
    "    if MODE==\"trash\":\n",
    "        print(\"Trash dir:\", TRASH_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9716a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交集數量 → real: 86772 | fake: 81720\n",
      "\n",
      "== IID summary ==\n",
      "[IID train] total=102398 | real=51198 fake=51200\n",
      "[IID val] total=12801 | real=6401 fake=6400\n",
      "[IID test] total=12801 | real=6401 fake=6400\n",
      "  real per-src: {'flickr30k': 22857, 'imagenet': 22857, 'unsplash': 18286}\n",
      "  fake per-src: {'dalle3': 19000, 'flux': 20000, 'sd3': 25000}\n",
      "\n",
      "== OOD summaries (by generator) ==\n",
      "[OOD-sd3 train] total=62399 | real=31199 fake=31200\n",
      "[OOD-sd3 val] total=7800 | real=3900 fake=3900\n",
      "[OOD-sd3 test] total=32801 | real=3901 fake=28900\n",
      "  holdout sd3 → 25000\n",
      "[OOD-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD-midjourney test] total=12801 | real=6401 fake=6400\n",
      "  holdout midjourney → 0\n",
      "[OOD-flux train] total=70399 | real=35199 fake=35200\n",
      "[OOD-flux val] total=8799 | real=4399 fake=4400\n",
      "[OOD-flux test] total=28802 | real=4402 fake=24400\n",
      "  holdout flux → 20000\n",
      "[OOD-dalle3 train] total=71998 | real=35998 fake=36000\n",
      "[OOD-dalle3 val] total=9000 | real=4500 fake=4500\n",
      "[OOD-dalle3 test] total=28002 | real=4502 fake=23500\n",
      "  holdout dalle3 → 19000\n",
      "\n",
      "✅ saved: /home/yaya/ai-detect-proj/Script/splits_iid_ood.json\n"
     ]
    }
   ],
   "source": [
    "# ===== Build IID + OOD splits (complete version, with your quotas) =====\n",
    "from pathlib import Path\n",
    "import re, json, random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- 基本設定 ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "FEA_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_256\")            # ← 改成你的 features 根目錄\n",
    "OUT_JSON = Path(\"/home/yaya/ai-detect-proj/Script/splits_iid_ood.json\") # ← 輸出 JSON 路徑\n",
    "\n",
    "# 用哪些特徵做交集（建議三者；只想快一點可暫時改 ['ela']）\n",
    "INTERSECT_FEATURES = ['ela','clip','prnu']\n",
    "\n",
    "# 你的配額（固定）\n",
    "FAKE_QUOTAS = {\"sd3\":25_000, \"midjourney\":30_000, \"flux\":20_000, \"dalle3\":19_000}   # 94k\n",
    "REAL_QUOTAS = {\"imagenet\":30_000, \"flickr30k\":30_000, \"unsplash\":24_000}            # 84k\n",
    "\n",
    "# 切分比例\n",
    "RATIOS = {\"train\":0.8, \"val\":0.1, \"test\":0.1}\n",
    "\n",
    "# ---------- 來源別名（前綴 -> 標準名）----------\n",
    "ALIAS = {\n",
    "    # real\n",
    "    \"flick\":\"flickr30k\",\"flicker\":\"flickr30k\",\"flickr\":\"flickr30k\",\"flickr30k\":\"flickr30k\",\n",
    "    \"unsplash\":\"unsplash\",\"imagenet\":\"imagenet\",\n",
    "    # fake\n",
    "    \"sd3\":\"sd3\",\"stable-diffusion-3\":\"sd3\",\"sd3.5\":\"sd3\",\n",
    "    \"midjourney\":\"midjourney\",\"midjourney-v6-llava\":\"midjourney\",\"mj\":\"midjourney\",\n",
    "    \"flux\":\"flux\",\"FLUX\":\"flux\",\n",
    "    \"dalle\":\"dalle3\",\"dalle3\":\"dalle3\",\"dall-e-3\":\"dalle3\",\n",
    "}\n",
    "\n",
    "# ---------- 小工具 ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def norm_source(raw: str):\n",
    "    return ALIAS.get(raw.lower(), raw.lower())\n",
    "\n",
    "def dataset_of(img_id: str):\n",
    "    return norm_source(img_id.split(\"__\",1)[0])\n",
    "\n",
    "def list_feature_dirs(feat: str, cls: str):\n",
    "    return [d for d in FEA_ROOT.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_ids_for(feat: str, cls: str):\n",
    "    ids = set()\n",
    "    for d in list_feature_dirs(feat, cls):\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            s = p.stem\n",
    "            s = base_id(s) if feat == \"ela\" else s\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "def bucket_by_source(ids):\n",
    "    b = defaultdict(list)\n",
    "    for i in ids:\n",
    "        b[dataset_of(i)].append(i)\n",
    "    for k in b: random.shuffle(b[k])\n",
    "    return b\n",
    "\n",
    "def sample_strict_quota(bucket, quotas):\n",
    "    \"\"\"嚴格依配額抽樣；不足就盡量拿。回傳 keep 與各來源可用量。\"\"\"\n",
    "    keep = []\n",
    "    avail = {k: len(v) for k,v in bucket.items()}\n",
    "    for src, q in quotas.items():\n",
    "        got = bucket.get(src, [])[:q]\n",
    "        keep.extend(got)\n",
    "    return keep, avail\n",
    "\n",
    "def scale_real_quotas_to(total_target, avail_per_src, base_quotas):\n",
    "    \"\"\"把 real 配額按比例縮放到 total_target；不超過可用量；不足就輪詢補齊。\"\"\"\n",
    "    s = sum(base_quotas.values())\n",
    "    if s == 0: return {}\n",
    "    quotas = {}\n",
    "    acc = 0\n",
    "    keys = sorted(base_quotas.keys())\n",
    "    for k in keys[:-1]:\n",
    "        q = int(round(total_target * base_quotas[k] / s))\n",
    "        quotas[k] = q; acc += q\n",
    "    quotas[keys[-1]] = max(0, total_target - acc)\n",
    "    # 截斷到可用\n",
    "    for k in list(quotas.keys()):\n",
    "        quotas[k] = min(quotas[k], avail_per_src.get(k, 0))\n",
    "    # 若總量仍不足，從有餘量的來源輪詢補\n",
    "    need = total_target - sum(quotas.values())\n",
    "    if need > 0:\n",
    "        remain = {k: max(0, avail_per_src.get(k,0) - quotas.get(k,0)) for k in avail_per_src}\n",
    "        srcs = sorted(remain.keys())\n",
    "        while need > 0:\n",
    "            progressed = False\n",
    "            for sname in srcs:\n",
    "                if remain[sname] > 0:\n",
    "                    quotas[sname] = quotas.get(sname,0) + 1\n",
    "                    remain[sname] -= 1\n",
    "                    need -= 1\n",
    "                    progressed = True\n",
    "                    if need == 0: break\n",
    "            if not progressed:\n",
    "                break\n",
    "    return quotas\n",
    "\n",
    "def split_8_1_1_per_source(selected_ids):\n",
    "    \"\"\"每來源各自 8/1/1，再合併；保持來源比例。\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    out = {\"train\":[], \"val\":[], \"test\":[]}\n",
    "    per_src = bucket_by_source(selected_ids)\n",
    "    for src, ids in per_src.items():\n",
    "        if len(ids) < 10:\n",
    "            n_tr = int(len(ids) * RATIOS[\"train\"])\n",
    "            n_va = int(len(ids) * RATIOS[\"val\"])\n",
    "            out[\"train\"].extend(ids[:n_tr])\n",
    "            out[\"val\"].extend(ids[n_tr:n_tr+n_va])\n",
    "            out[\"test\"].extend(ids[n_tr+n_va:])\n",
    "        else:\n",
    "            tr, tmp = train_test_split(ids, test_size=(1-RATIOS[\"train\"]), random_state=SEED)\n",
    "            va, te  = train_test_split(tmp, test_size=RATIOS[\"test\"]/(RATIOS[\"test\"]+RATIOS[\"val\"]), random_state=SEED)\n",
    "            out[\"train\"].extend(tr); out[\"val\"].extend(va); out[\"test\"].extend(te)\n",
    "    for k in out: random.shuffle(out[k])\n",
    "    return out\n",
    "\n",
    "def report_counts(ids):\n",
    "    c = Counter(dataset_of(i) for i in ids)\n",
    "    return {\"total\": len(ids), \"by_source\": dict(sorted(c.items()))}\n",
    "\n",
    "# ---------- 掃描 & 特徵交集 ----------\n",
    "present = {}\n",
    "for feat in INTERSECT_FEATURES:\n",
    "    present[feat] = {\"real\": list_ids_for(feat,\"real\"),\n",
    "                     \"fake\": list_ids_for(feat,\"fake\")}\n",
    "sets_real = [present[feat][\"real\"] for feat in INTERSECT_FEATURES if present[feat][\"real\"]]\n",
    "sets_fake = [present[feat][\"fake\"] for feat in INTERSECT_FEATURES if present[feat][\"fake\"]]\n",
    "assert sets_real and sets_fake, \"找不到可用的 real 或 fake IDs，請檢查特徵路徑與 INTERSECT_FEATURES。\"\n",
    "\n",
    "common = {\n",
    "    \"real\": set.intersection(*sets_real) if len(sets_real)>1 else sets_real[0],\n",
    "    \"fake\": set.intersection(*sets_fake) if len(sets_fake)>1 else sets_fake[0],\n",
    "}\n",
    "print(\"交集數量 → real:\", len(common[\"real\"]), \"| fake:\", len(common[\"fake\"]))\n",
    "\n",
    "real_bkt = bucket_by_source(common[\"real\"])\n",
    "fake_bkt = bucket_by_source(common[\"fake\"])\n",
    "\n",
    "# ---------- IID ----------\n",
    "# fake：嚴格依配額\n",
    "fake_keep_iid, fake_avail = sample_strict_quota(fake_bkt, FAKE_QUOTAS)\n",
    "\n",
    "# real：嘗試縮放到與 fake 同總量（如不足就拿到上限）\n",
    "avail_real = {k: len(v) for k,v in real_bkt.items()}\n",
    "real_quota_scaled = scale_real_quotas_to(len(fake_keep_iid), avail_real, REAL_QUOTAS)\n",
    "real_keep_iid, _ = sample_strict_quota(real_bkt, real_quota_scaled)\n",
    "\n",
    "iid_real = split_8_1_1_per_source(real_keep_iid)\n",
    "iid_fake = split_8_1_1_per_source(fake_keep_iid)\n",
    "iid = { \"train\": iid_real[\"train\"] + iid_fake[\"train\"],\n",
    "        \"val\":   iid_real[\"val\"]   + iid_fake[\"val\"],\n",
    "        \"test\":  iid_real[\"test\"]  + iid_fake[\"test\"] }\n",
    "for k in iid: random.shuffle(iid[k])\n",
    "\n",
    "# ---------- OOD-by-Generator（四個） ----------\n",
    "def build_ood_for(hold_src: str):\n",
    "    hold_src = hold_src.lower()\n",
    "    # holdout：整個來源，按配額上限截斷\n",
    "    hold_quota = FAKE_QUOTAS.get(hold_src, len(fake_bkt.get(hold_src, [])))\n",
    "    hold_ids   = fake_bkt.get(hold_src, [])[:min(hold_quota, len(fake_bkt.get(hold_src, [])))]\n",
    "    # 其餘 fake：照配額抽樣\n",
    "    remain_quota = {s:q for s,q in FAKE_QUOTAS.items() if s != hold_src}\n",
    "    fake_keep, _ = sample_strict_quota(fake_bkt, remain_quota)\n",
    "    # real：縮放到與「訓練用 fake」同總量\n",
    "    real_quota_scaled = scale_real_quotas_to(len(fake_keep), avail_real, REAL_QUOTAS)\n",
    "    real_keep, _ = sample_strict_quota(real_bkt, real_quota_scaled)\n",
    "    # split\n",
    "    r = split_8_1_1_per_source(real_keep)\n",
    "    f = split_8_1_1_per_source(fake_keep)\n",
    "    ood = { \"train\": r[\"train\"] + f[\"train\"],\n",
    "            \"val\":   r[\"val\"]   + f[\"val\"],\n",
    "            \"test\":  r[\"test\"]  + f[\"test\"] + hold_ids }\n",
    "    for k in ood: random.shuffle(ood[k])\n",
    "    rep = {\n",
    "        \"train\": report_counts(ood[\"train\"]),\n",
    "        \"val\":   report_counts(ood[\"val\"]),\n",
    "        \"test\":  report_counts(ood[\"test\"]),\n",
    "        \"holdout_source\": hold_src,\n",
    "        \"holdout_size\": len(hold_ids)\n",
    "    }\n",
    "    return ood, rep\n",
    "\n",
    "ood_all = {}\n",
    "ood_reports = {}\n",
    "for src in [\"sd3\",\"midjourney\",\"flux\",\"dalle3\"]:\n",
    "    ood_all[src], ood_reports[src] = build_ood_for(src)\n",
    "\n",
    "# ---------- 報告 & 存檔 ----------\n",
    "def quick_report(name, split, real_pool):\n",
    "    rset = set(real_pool[\"train\"] + real_pool[\"val\"] + real_pool[\"test\"])\n",
    "    for sp in (\"train\",\"val\",\"test\"):\n",
    "        both = split[sp]\n",
    "        n_total = len(both)\n",
    "        n_real  = sum(1 for i in both if i in rset)\n",
    "        n_fake  = n_total - n_real\n",
    "        print(f\"[{name} {sp}] total={n_total} | real={n_real} fake={n_fake}\")\n",
    "\n",
    "print(\"\\n== IID summary ==\")\n",
    "quick_report(\"IID\", iid, iid_real)\n",
    "print(\"  real per-src:\", report_counts(iid_real[\"train\"]+iid_real[\"val\"]+iid_real[\"test\"])[\"by_source\"])\n",
    "print(\"  fake per-src:\", report_counts(iid_fake[\"train\"]+iid_fake[\"val\"]+iid_fake[\"test\"])[\"by_source\"])\n",
    "\n",
    "print(\"\\n== OOD summaries (by generator) ==\")\n",
    "for src in ood_all:\n",
    "    quick_report(f\"OOD-{src}\", ood_all[src], iid_real)  # 只是用來區分 real/fake\n",
    "    print(f\"  holdout {src} →\", ood_reports[src][\"holdout_size\"])\n",
    "\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"intersect_features\": INTERSECT_FEATURES,\n",
    "    \"fake_quotas\": FAKE_QUOTAS,\n",
    "    \"real_quotas\": REAL_QUOTAS,\n",
    "    \"avail\": {\n",
    "        \"real_by_src\": {k:len(v) for k,v in real_bkt.items()},\n",
    "        \"fake_by_src\": {k:len(v) for k,v in fake_bkt.items()},\n",
    "        \"real_intersection_total\": len(common[\"real\"]),\n",
    "        \"fake_intersection_total\": len(common[\"fake\"]),\n",
    "    },\n",
    "    \"ood_reports\": ood_reports,\n",
    "}\n",
    "payload = {\"meta\": meta, \"iid\": iid, \"ood_gen\": ood_all}\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "print(\"\\n✅ saved:\", OUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c690e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交集數量 → real: 86772 | fake: 81720\n",
      "\n",
      "== IID summary ==\n",
      "[IID train] total=132576 | real=67200 fake=65376\n",
      "[IID val] total=16572 | real=8400 fake=8172\n",
      "[IID test] total=16572 | real=8400 fake=8172\n",
      "\n",
      "== IID_balanced summary ==\n",
      "[IID_BAL train] total=127128 | real=67200 fake=59928\n",
      "[IID_BAL val] total=15891 | real=8400 fake=7491\n",
      "[IID_BAL test] total=15892 | real=8400 fake=7492\n",
      "\n",
      "== OOD summaries ==\n",
      "[OOD-sd3 train] total=90750 | real=45374 fake=45376\n",
      "[OOD-sd3 val] total=11345 | real=5673 fake=5672\n",
      "[OOD-sd3 test] total=36345 | real=5673 fake=30672\n",
      "  holdout sd3 → 25000\n",
      "[OOD-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD-midjourney test] total=30521 | real=6401 fake=24120\n",
      "  holdout midjourney → 17720\n",
      "[OOD-flux train] total=98751 | real=49375 fake=49376\n",
      "[OOD-flux val] total=12343 | real=6171 fake=6172\n",
      "[OOD-flux test] total=32346 | real=6174 fake=26172\n",
      "  holdout flux → 20000\n",
      "[OOD-dalle3 train] total=100352 | real=50176 fake=50176\n",
      "[OOD-dalle3 val] total=12544 | real=6272 fake=6272\n",
      "[OOD-dalle3 test] total=31544 | real=6272 fake=25272\n",
      "  holdout dalle3 → 19000\n",
      "\n",
      "== OOD_strict summaries ==\n",
      "[OOD_STRICT-sd3 train] total=90750 | real=45374 fake=45376\n",
      "[OOD_STRICT-sd3 val] total=11345 | real=5673 fake=5672\n",
      "[OOD_STRICT-sd3 test] total=30673 | real=5673 fake=25000\n",
      "[OOD_STRICT-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD_STRICT-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD_STRICT-midjourney test] total=24121 | real=6401 fake=17720\n",
      "[OOD_STRICT-flux train] total=98751 | real=49375 fake=49376\n",
      "[OOD_STRICT-flux val] total=12343 | real=6171 fake=6172\n",
      "[OOD_STRICT-flux test] total=26174 | real=6174 fake=20000\n",
      "[OOD_STRICT-dalle3 train] total=100352 | real=50176 fake=50176\n",
      "[OOD_STRICT-dalle3 val] total=12544 | real=6272 fake=6272\n",
      "[OOD_STRICT-dalle3 test] total=25272 | real=6272 fake=19000\n",
      "\n",
      "✅ saved: /home/yaya/ai-detect-proj/Script/splits/combined_split.json\n"
     ]
    }
   ],
   "source": [
    "# ===== Unified split builder: IID / IID_balanced / OOD(by generator) / OOD_strict / smoke_10p =====\n",
    "from pathlib import Path\n",
    "import re, json, random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- 基本設定（改這裡） ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "FEA_ROOT    = SCRIPT_ROOT / \"features_256\"                         # features 根\n",
    "OUT_JSON    = SCRIPT_ROOT / \"splits/combined_split.json\"           # 輸出檔\n",
    "\n",
    "# 用哪些特徵做交集（建議三者；要加速可改成 ['ela']）\n",
    "INTERSECT_FEATURES = ['ela','clip','prnu']\n",
    "\n",
    "# 你的配額（固定）\n",
    "FAKE_QUOTAS = {\"sd3\":25_000, \"midjourney\":30_000, \"flux\":20_000, \"dalle3\":19_000}  # 94k\n",
    "REAL_QUOTAS = {\"imagenet\":30_000, \"flickr30k\":30_000, \"unsplash\":24_000}           # 84k\n",
    "\n",
    "# 切分比例\n",
    "RATIOS = {\"train\":0.8, \"val\":0.1, \"test\":0.1}\n",
    "\n",
    "# 產生一份 10% 的 smoke，從哪個 split 抽\n",
    "SMOKE_BASE_KEY = \"iid\"   # 可改成 \"ood_gen.sd3\" / \"ood_gen.midjourney\" / \"ood_gen.flux\" / \"ood_gen.dalle3\"\n",
    "\n",
    "# ---------- 來源別名（前綴 -> 標準名）----------\n",
    "ALIAS = {\n",
    "    # real\n",
    "    \"flick\":\"flickr30k\",\"flicker\":\"flickr30k\",\"flickr\":\"flickr30k\",\"flickr30k\":\"flickr30k\",\n",
    "    \"unsplash\":\"unsplash\",\"imagenet\":\"imagenet\",\n",
    "    # fake\n",
    "    \"sd3\":\"sd3\",\"stable-diffusion-3\":\"sd3\",\"sd3.5\":\"sd3\",\n",
    "    \"midjourney\":\"midjourney\",\"midjourney-v6\":\"midjourney\",\"mj\":\"midjourney\",\n",
    "    \"flux\":\"flux\",\"FLUX\":\"flux\",\n",
    "    \"dalle\":\"dalle3\",\"dalle3\":\"dalle3\",\"dall-e-3\":\"dalle3\",\n",
    "}\n",
    "\n",
    "REAL_SOURCES = {\"imagenet\",\"flickr30k\",\"unsplash\"}\n",
    "\n",
    "# ---------- 小工具 ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def norm_source(raw: str): return ALIAS.get(raw.lower(), raw.lower())\n",
    "def dataset_of(img_id: str): return norm_source(img_id.split(\"__\",1)[0])\n",
    "def is_real_id(img_id: str): return dataset_of(img_id) in REAL_SOURCES\n",
    "\n",
    "def list_feature_dirs(feat: str, cls: str):\n",
    "    return [d for d in FEA_ROOT.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_ids_for(feat: str, cls: str):\n",
    "    ids = set()\n",
    "    for d in list_feature_dirs(feat, cls):\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            s = p.stem\n",
    "            s = base_id(s) if feat == \"ela\" else s\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "def bucket_by_source(ids):\n",
    "    b = defaultdict(list)\n",
    "    for i in ids:\n",
    "        b[dataset_of(i)].append(i)\n",
    "    for k in b: random.shuffle(b[k])\n",
    "    return b\n",
    "\n",
    "def sample_quota(bucket, quotas):\n",
    "    \"\"\"依配額抽樣；不足就拿可用量。回傳 keep 與各來源可用量\"\"\"\n",
    "    keep = []\n",
    "    avail = {k: len(v) for k,v in bucket.items()}\n",
    "    for src, q in quotas.items():\n",
    "        got = bucket.get(src, [])[:q]\n",
    "        keep.extend(got)\n",
    "    return keep, avail\n",
    "\n",
    "def scale_real_quotas_to(total_target, avail_per_src, base_quotas):\n",
    "    \"\"\"把 real 配額按比例縮放到 total_target；不超過可用量；不足就輪詢補齊。\"\"\"\n",
    "    s = sum(base_quotas.values())\n",
    "    if s == 0: return {}\n",
    "    quotas = {}\n",
    "    acc = 0\n",
    "    keys = sorted(base_quotas.keys())\n",
    "    for k in keys[:-1]:\n",
    "        q = int(round(total_target * base_quotas[k] / s))\n",
    "        quotas[k] = q; acc += q\n",
    "    quotas[keys[-1]] = max(0, total_target - acc)\n",
    "    # 截到可用\n",
    "    for k in list(quotas.keys()):\n",
    "        quotas[k] = min(quotas[k], avail_per_src.get(k, 0))\n",
    "    # 若仍不足，輪詢補\n",
    "    need = total_target - sum(quotas.values())\n",
    "    if need > 0:\n",
    "        remain = {k: max(0, avail_per_src.get(k,0) - quotas.get(k,0)) for k in avail_per_src}\n",
    "        srcs = sorted(remain.keys())\n",
    "        while need > 0:\n",
    "            progressed = False\n",
    "            for sname in srcs:\n",
    "                if remain[sname] > 0:\n",
    "                    quotas[sname] = quotas.get(sname,0) + 1\n",
    "                    remain[sname] -= 1\n",
    "                    need -= 1\n",
    "                    progressed = True\n",
    "                    if need == 0: break\n",
    "            if not progressed: break\n",
    "    return quotas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_8_1_1_per_source(selected_ids):\n",
    "    \"\"\"各來源 8/1/1，再合併，保持來源比例。\"\"\"\n",
    "    out = {\"train\":[], \"val\":[], \"test\":[]}\n",
    "    per_src = bucket_by_source(selected_ids)\n",
    "    for src, ids in per_src.items():\n",
    "        if len(ids) < 10:\n",
    "            n_tr = int(len(ids) * RATIOS[\"train\"])\n",
    "            n_va = int(len(ids) * RATIOS[\"val\"])\n",
    "            out[\"train\"].extend(ids[:n_tr])\n",
    "            out[\"val\"].extend(ids[n_tr:n_tr+n_va])\n",
    "            out[\"test\"].extend(ids[n_tr+n_va:])\n",
    "        else:\n",
    "            tr, tmp = train_test_split(ids, test_size=(1-RATIOS[\"train\"]), random_state=SEED, shuffle=True)\n",
    "            va, te  = train_test_split(tmp, test_size=RATIOS[\"test\"]/(RATIOS[\"test\"]+RATIOS[\"val\"]), random_state=SEED, shuffle=True)\n",
    "            out[\"train\"].extend(tr); out[\"val\"].extend(va); out[\"test\"].extend(te)\n",
    "    for k in out: random.shuffle(out[k])\n",
    "    return out\n",
    "\n",
    "def report_counts(ids):\n",
    "    c = Counter(dataset_of(i) for i in ids)\n",
    "    return {\"total\": len(ids), \"by_source\": dict(sorted(c.items()))}\n",
    "\n",
    "def summarize(name, split, real_pool):\n",
    "    rset = set(real_pool[\"train\"] + real_pool[\"val\"] + real_pool[\"test\"])\n",
    "    for sp in (\"train\",\"val\",\"test\"):\n",
    "        both = split[sp]\n",
    "        n_total = len(both)\n",
    "        n_real  = sum(1 for i in both if i in rset)\n",
    "        n_fake  = n_total - n_real\n",
    "        print(f\"[{name} {sp}] total={n_total} | real={n_real} fake={n_fake}\")\n",
    "\n",
    "# ---------- 掃描 & 特徵交集 ----------\n",
    "present = {}\n",
    "for feat in INTERSECT_FEATURES:\n",
    "    present[feat] = {\"real\": list_ids_for(feat,\"real\"),\n",
    "                     \"fake\": list_ids_for(feat,\"fake\")}\n",
    "sets_real = [present[feat][\"real\"] for feat in INTERSECT_FEATURES if present[feat][\"real\"]]\n",
    "sets_fake = [present[feat][\"fake\"] for feat in INTERSECT_FEATURES if present[feat][\"fake\"]]\n",
    "assert sets_real and sets_fake, \"找不到可用的 real 或 fake IDs，請檢查特徵路徑與 INTERSECT_FEATURES。\"\n",
    "\n",
    "common = {\n",
    "    \"real\": set.intersection(*sets_real) if len(sets_real)>1 else sets_real[0],\n",
    "    \"fake\": set.intersection(*sets_fake) if len(sets_fake)>1 else sets_fake[0],\n",
    "}\n",
    "print(\"交集數量 → real:\", len(common[\"real\"]), \"| fake:\", len(common[\"fake\"]))\n",
    "\n",
    "real_bkt = bucket_by_source(common[\"real\"])\n",
    "fake_bkt = bucket_by_source(common[\"fake\"])\n",
    "avail_real = {k: len(v) for k,v in real_bkt.items()}\n",
    "\n",
    "# ---------- IID ----------\n",
    "# fake：嚴格依配額\n",
    "iid_fake_keep, fake_avail = sample_quota(fake_bkt, FAKE_QUOTAS)\n",
    "# real：固定用你的 REAL_QUOTAS（不縮放）\n",
    "iid_real_keep, _ = sample_quota(real_bkt, REAL_QUOTAS)\n",
    "\n",
    "iid_real = split_8_1_1_per_source(iid_real_keep)\n",
    "iid_fake = split_8_1_1_per_source(iid_fake_keep)\n",
    "iid = { \"train\": iid_real[\"train\"] + iid_fake[\"train\"],\n",
    "        \"val\":   iid_real[\"val\"]   + iid_fake[\"val\"],\n",
    "        \"test\":  iid_real[\"test\"]  + iid_fake[\"test\"] }\n",
    "for k in iid: random.shuffle(iid[k])\n",
    "\n",
    "# ---------- IID_balanced（讓 fake 下採樣到 real 的總量） ----------\n",
    "total_real_target = sum(len(v) for v in iid_real.values())\n",
    "# 這裡用各生成器的配額占比來決定 fake 下採樣比例\n",
    "sum_fake_quota = sum(FAKE_QUOTAS.values())\n",
    "bal_fake_quota = {src: int(round(total_real_target * q / sum_fake_quota)) for src, q in FAKE_QUOTAS.items()}\n",
    "bal_fake_keep, _ = sample_quota(fake_bkt, bal_fake_quota)\n",
    "bal_fake = split_8_1_1_per_source(bal_fake_keep)\n",
    "iid_balanced = { \"train\": iid_real[\"train\"] + bal_fake[\"train\"],\n",
    "                 \"val\":   iid_real[\"val\"]   + bal_fake[\"val\"],\n",
    "                 \"test\":  iid_real[\"test\"]  + bal_fake[\"test\"] }\n",
    "for k in iid_balanced: random.shuffle(iid_balanced[k])\n",
    "\n",
    "# ---------- OOD-by-Generator（混合 test） ----------\n",
    "def build_ood_for(hold_src: str, strict=False):\n",
    "    hold_src = hold_src.lower()\n",
    "    # holdout：整個來源，按配額截斷\n",
    "    hold_quota = FAKE_QUOTAS.get(hold_src, len(fake_bkt.get(hold_src, [])))\n",
    "    hold_ids   = fake_bkt.get(hold_src, [])[:min(hold_quota, len(fake_bkt.get(hold_src, [])))]\n",
    "    # 其餘 fake：照配額抽樣\n",
    "    remain_quota = {s:q for s,q in FAKE_QUOTAS.items() if s != hold_src}\n",
    "    fake_keep, _ = sample_quota(fake_bkt, remain_quota)\n",
    "    # real：縮放到與「訓練用 fake」同總量\n",
    "    real_quota_scaled = scale_real_quotas_to(len(fake_keep), avail_real, REAL_QUOTAS)\n",
    "    real_keep, _ = sample_quota(real_bkt, real_quota_scaled)\n",
    "    # split\n",
    "    r = split_8_1_1_per_source(real_keep)\n",
    "    f = split_8_1_1_per_source(fake_keep)\n",
    "    if strict:\n",
    "        test_fake = hold_ids                      # 只放 holdout\n",
    "    else:\n",
    "        test_fake = f[\"test\"] + hold_ids          # 混合 test\n",
    "    ood = { \"train\": r[\"train\"] + f[\"train\"],\n",
    "            \"val\":   r[\"val\"]   + f[\"val\"],\n",
    "            \"test\":  r[\"test\"]  + test_fake }\n",
    "    for k in ood: random.shuffle(ood[k])\n",
    "    rep = {\n",
    "        \"train\": report_counts(ood[\"train\"]),\n",
    "        \"val\":   report_counts(ood[\"val\"]),\n",
    "        \"test\":  report_counts(ood[\"test\"]),\n",
    "        \"holdout_source\": hold_src,\n",
    "        \"holdout_size\": len(hold_ids)\n",
    "    }\n",
    "    return ood, rep\n",
    "\n",
    "ood_gen = {}\n",
    "ood_gen_strict = {}\n",
    "ood_reports = {}\n",
    "for src in [\"sd3\",\"midjourney\",\"flux\",\"dalle3\"]:\n",
    "    ood_gen[src], ood_reports[src] = build_ood_for(src, strict=False)\n",
    "    ood_gen_strict[src], _         = build_ood_for(src, strict=True)\n",
    "\n",
    "# ---------- 10% smoke（從 SMOKE_BASE_KEY 抽） ----------\n",
    "def get_by_key(tree, key: str):\n",
    "    node = tree\n",
    "    for k in key.split(\".\"):\n",
    "        node = node[k]\n",
    "    return node\n",
    "\n",
    "def subsample_by_class_source(ids, frac=0.1, min_each=1, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    buckets = defaultdict(list)\n",
    "    for i in ids:\n",
    "        buckets[( \"real\" if is_real_id(i) else \"fake\", dataset_of(i) )].append(i)\n",
    "    keep = []\n",
    "    for (_c,_s), arr in buckets.items():\n",
    "        random.shuffle(arr)\n",
    "        k = max(min_each, int(round(len(arr)*frac)))\n",
    "        keep.extend(arr[:k])\n",
    "    random.shuffle(keep)\n",
    "    return keep\n",
    "\n",
    "tmp_all = {\n",
    "    \"iid\": iid,\n",
    "    \"iid_balanced\": iid_balanced,\n",
    "    \"ood_gen\": ood_gen,\n",
    "    \"ood_gen_strict\": ood_gen_strict\n",
    "}\n",
    "base = get_by_key(tmp_all, SMOKE_BASE_KEY)\n",
    "smoke_10p = {\n",
    "    \"train\": subsample_by_class_source(base[\"train\"], 0.10, seed=SEED+1),\n",
    "    \"val\":   subsample_by_class_source(base[\"val\"],   0.10, seed=SEED+2),\n",
    "    \"test\":  subsample_by_class_source(base[\"test\"],  0.10, seed=SEED+3),\n",
    "}\n",
    "\n",
    "# ---------- 簡報 ----------\n",
    "print(\"\\n== IID summary ==\"); summarize(\"IID\", iid, iid_real)\n",
    "print(\"\\n== IID_balanced summary ==\"); summarize(\"IID_BAL\", iid_balanced, iid_real)\n",
    "print(\"\\n== OOD summaries ==\")\n",
    "for src in ood_gen:\n",
    "    summarize(f\"OOD-{src}\", ood_gen[src], iid_real)\n",
    "    print(f\"  holdout {src} →\", ood_reports[src][\"holdout_size\"])\n",
    "print(\"\\n== OOD_strict summaries ==\")\n",
    "for src in ood_gen_strict:\n",
    "    summarize(f\"OOD_STRICT-{src}\", ood_gen_strict[src], iid_real)\n",
    "\n",
    "# ---------- 存檔 ----------\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "payload = {\n",
    "    \"meta\": {\n",
    "        \"seed\": SEED,\n",
    "        \"intersect_features\": INTERSECT_FEATURES,\n",
    "        \"fake_quotas\": FAKE_QUOTAS,\n",
    "        \"real_quotas\": REAL_QUOTAS,\n",
    "        \"avail\": {\n",
    "            \"real_by_src\": {k:len(v) for k,v in real_bkt.items()},\n",
    "            \"fake_by_src\": {k:len(v) for k,v in fake_bkt.items()},\n",
    "            \"real_intersection_total\": len(common[\"real\"]),\n",
    "            \"fake_intersection_total\": len(common[\"fake\"]),\n",
    "        },\n",
    "        \"notes\": \"iid: 固定配額；iid_balanced: fake 下採樣到 real 總量；ood_gen: 混合 test；ood_gen_strict: 嚴格 OOD\",\n",
    "    },\n",
    "    \"iid\": iid,\n",
    "    \"iid_balanced\": iid_balanced,\n",
    "    \"ood_gen\": ood_gen,\n",
    "    \"ood_gen_strict\": ood_gen_strict,\n",
    "    \"smoke_10p\": smoke_10p,\n",
    "}\n",
    "OUT_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "print(\"\\n✅ saved:\", OUT_JSON)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
