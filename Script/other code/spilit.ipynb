{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d266030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åµæ¸¬åˆ°ç‰¹å¾µï¼š ['ela', 'clip', 'prnu']\n",
      "å¯æ“ä½œï¼ˆç‰¹å¾µäº¤é›†ï¼‰fake IDsï¼š 89000\n",
      "\n",
      "ä¾†æºç¾æ³ï¼ˆå¯ç”¨ â†’ ä¿ç•™ â‰¤ 25Kï¼‰ï¼š\n",
      "  FLUX                    20000 â†’  20000\n",
      "  SD3                     50000 â†’  25000\n",
      "  dalle3                  19000 â†’  19000\n",
      "å°‡ç§»é™¤çš„ ID æ•¸ï¼š 25000\n",
      "\n",
      "Planned removals: 75000 files\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000003.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000007.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000012.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000013.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000015.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000016.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000018.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000019.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000022.npy\n",
      "  - /home/yaya/ai-detect-proj/Script/features_256/ela_fake_npy/SD3__0000_00000024.npy\n",
      "ğŸ—‘ å·²ç§»åˆ°åƒåœ¾æ¡¶ï¼š 75000\n",
      "After cleanup â†’ intersection fake IDs by source:\n",
      "  FLUX                    20000\n",
      "  SD3                     25000\n",
      "  dalle3                  19000\n",
      "Trash dir: /home/yaya/ai-detect-proj/Script/features_256/_trash_fake_pergen_20250821_121322\n"
     ]
    }
   ],
   "source": [
    "# ==== Downsample FAKE: each generator/source -> keep at most 25K IDs ====\n",
    "from pathlib import Path\n",
    "import re, random, time, shutil\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- è¨­å®š ----------\n",
    "FEA_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_256\")  # ä½ çš„ç‰¹å¾µæ ¹ç›®éŒ„ï¼ˆæœƒéè¿´æƒæï¼‰\n",
    "PER_GEN_TARGET = 25_000               # æ¯å€‹ç”Ÿæˆå™¨æƒ³ä¿ç•™çš„ ID æ•¸ä¸Šé™\n",
    "SEED = 42; random.seed(SEED)\n",
    "\n",
    "DRY_RUN = False                        # å…ˆé è¦½ï¼›OK å¾Œæ”¹ False çœŸçš„å‹•ä½œ\n",
    "MODE = \"trash\"                        # \"trash\"=ç§»åˆ°åƒåœ¾æ¡¶ | \"delete\"=ç›´æ¥åˆªé™¤\n",
    "KEEP_ALL_ELA_QUALS = True             # True=ä¿ç•™è©²IDæ‰€æœ‰ ELA å“è³ªæª”ï¼›False=åªç•™ä¸€å€‹ï¼ˆé è¿‘ q90ï¼‰\n",
    "\n",
    "TRASH_DIR = FEA_ROOT / f\"_trash_fake_pergen_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# ---------- å°å·¥å…· ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def ela_quality(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def list_feature_dirs(root: Path, feat: str, cls: str):\n",
    "    return [d for d in root.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_npy(d: Path):\n",
    "    return sorted(p for p in d.glob(\"*.npy\"))\n",
    "\n",
    "def dataset_of(img_id: str):\n",
    "    return img_id.split(\"__\", 1)[0]\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def move_to_trash(p: Path):\n",
    "    rel = p.relative_to(FEA_ROOT)\n",
    "    dst = TRASH_DIR / rel\n",
    "    ensure_dir(dst.parent)\n",
    "    shutil.move(str(p), str(dst))\n",
    "\n",
    "# ---------- æƒæ fake çš„å„ç‰¹å¾µè·¯å¾‘ ----------\n",
    "dirs = {\n",
    "    (\"ela\",\"fake\"):  list_feature_dirs(FEA_ROOT, \"ela\",  \"fake\"),\n",
    "    (\"clip\",\"fake\"): list_feature_dirs(FEA_ROOT, \"clip\", \"fake\"),\n",
    "    (\"prnu\",\"fake\"): list_feature_dirs(FEA_ROOT, \"prnu\", \"fake\"),\n",
    "}\n",
    "present_feats = [feat for (feat,_), lst in dirs.items() if lst]\n",
    "assert present_feats, \"æ‰¾ä¸åˆ°ä»»ä½• *_fake_npy ç›®éŒ„ï¼Œè«‹ç¢ºèª FEA_ROOT è¨­å®šã€‚\"\n",
    "print(\"åµæ¸¬åˆ°ç‰¹å¾µï¼š\", present_feats)\n",
    "\n",
    "# ---------- å»ºç«‹å„ç‰¹å¾µçš„ fake ID é›†åˆï¼ˆELA ç”¨ base idï¼‰ ----------\n",
    "id_sets = {}\n",
    "for feat in present_feats:\n",
    "    s = set()\n",
    "    for d in dirs[(feat,\"fake\")]:\n",
    "        for p in list_npy(d):\n",
    "            s.add(base_id(p.stem) if feat==\"ela\" else p.stem)\n",
    "    id_sets[feat] = s\n",
    "\n",
    "# äº¤é›†ï¼ˆåƒ…åœ¨ç›®å‰å¯åŒæ™‚å–å¾—ä¹‹ IDs ä¸­å‹•ä½œï¼‰\n",
    "ids_common = None\n",
    "for feat, s in id_sets.items():\n",
    "    ids_common = s if ids_common is None else (ids_common & s)\n",
    "print(\"å¯æ“ä½œï¼ˆç‰¹å¾µäº¤é›†ï¼‰fake IDsï¼š\", len(ids_common))\n",
    "\n",
    "# ---------- ä¾ç”Ÿæˆå™¨åˆ†æ¡¶ï¼Œæ±ºå®šæ¯å€‹ç”Ÿæˆå™¨ä¿ç•™åˆ° 25K ----------\n",
    "bucket = defaultdict(list)\n",
    "for i in ids_common:\n",
    "    bucket[dataset_of(i)].append(i)\n",
    "for k in bucket: random.shuffle(bucket[k])\n",
    "\n",
    "keep_ids = set()\n",
    "per_src_keep = {}\n",
    "per_src_avail = {src: len(lst) for src, lst in bucket.items()}\n",
    "for src, lst in bucket.items():\n",
    "    k = min(len(lst), PER_GEN_TARGET)\n",
    "    per_src_keep[src] = k\n",
    "    keep_ids.update(lst[:k])\n",
    "\n",
    "print(\"\\nä¾†æºç¾æ³ï¼ˆå¯ç”¨ â†’ ä¿ç•™ â‰¤ 25Kï¼‰ï¼š\")\n",
    "for src in sorted(bucket.keys()):\n",
    "    print(f\"  {src:22s} {per_src_avail[src]:6d} â†’ {per_src_keep[src]:6d}\")\n",
    "print(\"å°‡ç§»é™¤çš„ ID æ•¸ï¼š\", len(ids_common) - len(keep_ids))\n",
    "\n",
    "# ---------- å»ºç«‹åˆªé™¤æ¸…å–® ----------\n",
    "to_remove = []           # è¦åˆª/ç§»çš„æª”æ¡ˆè·¯å¾‘\n",
    "to_keep_one_ela = {}     # ç•¶åªä¿ç•™å–®ä¸€å“è³ªæ™‚ï¼Œè¨˜éŒ„æ¯å€‹ base id æ‡‰ä¿ç•™çš„é‚£å€‹æª”\n",
    "\n",
    "if not KEEP_ALL_ELA_QUALS:\n",
    "    # å…ˆæ±ºå®šæ¯å€‹ä¿ç•™ ID çš„å”¯ä¸€ ELA æª”ï¼ˆå„ªå…ˆ q90ï¼Œå…¶æ¬¡æœ€æ¥è¿‘ 90ï¼‰\n",
    "    for d in dirs.get((\"ela\",\"fake\"), []):\n",
    "        # æŠŠè©²ç›®éŒ„æ‰€æœ‰æª”æŒ‰ base åˆ†æ¡¶\n",
    "        by_base = defaultdict(list)\n",
    "        for p in list_npy(d):\n",
    "            b = base_id(p.stem)\n",
    "            if b in keep_ids:\n",
    "                by_base[b].append(p)\n",
    "        for b, lst in by_base.items():\n",
    "            best = None\n",
    "            best_score = (999, \"\")\n",
    "            for pp in lst:\n",
    "                q = ela_quality(pp.stem)\n",
    "                score = (0 if q == 90 else (abs(q-90) if q is not None else 999), pp.name)\n",
    "                if score < best_score:\n",
    "                    best_score, best = score, pp\n",
    "            to_keep_one_ela[b] = best\n",
    "\n",
    "# åªå°ã€Œåœ¨äº¤é›†å…§è€Œä¸åœ¨ keep_idsã€çš„æª”å‹•ä½œï¼ˆé¿å…èª¤åˆªå…¶ä»–å­¤å…’æˆ–æœªæŠ½åˆ°çš„ç‰¹å¾µï¼‰\n",
    "for feat in present_feats:\n",
    "    for d in dirs[(feat,\"fake\")]:\n",
    "        for p in list_npy(d):\n",
    "            id_base = base_id(p.stem) if feat==\"ela\" else p.stem\n",
    "            if id_base in ids_common:\n",
    "                if id_base not in keep_ids:\n",
    "                    to_remove.append(p)\n",
    "                elif feat==\"ela\" and (not KEEP_ALL_ELA_QUALS) and (to_keep_one_ela.get(id_base) != p):\n",
    "                    to_remove.append(p)\n",
    "\n",
    "print(f\"\\nPlanned removals: {len(to_remove)} files\")\n",
    "for p in to_remove[:10]:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "# ---------- åŸ·è¡Œ ----------\n",
    "if DRY_RUN:\n",
    "    print(\"\\nDRY_RUN=True â†’ åªé è¦½ï¼Œä¸å‹•ä½œã€‚ç¢ºèª OK å¾ŒæŠŠ DRY_RUN=False é‡æ–°è·‘ã€‚\")\n",
    "else:\n",
    "    removed = 0\n",
    "    if MODE == \"trash\":\n",
    "        ensure_dir(TRASH_DIR)\n",
    "    for p in to_remove:\n",
    "        try:\n",
    "            if MODE == \"trash\":\n",
    "                move_to_trash(p)\n",
    "            else:\n",
    "                p.unlink(missing_ok=True)\n",
    "            removed += 1\n",
    "        except Exception as e:\n",
    "            print(\"skip (error):\", p, \"|\", e)\n",
    "    print((\"ğŸ—‘ å·²ç§»åˆ°åƒåœ¾æ¡¶ï¼š\" if MODE==\"trash\" else \"ğŸ—‘ å·²åˆªé™¤ï¼š\"), removed)\n",
    "\n",
    "    # æ”¶å°¾çµ±è¨ˆï¼ˆç¾åœ¨å„ä¾†æºçš„äº¤é›†/ä¿ç•™ç‹€æ³ï¼‰\n",
    "    # é‡æ–°æƒæäº¤é›†\n",
    "    id_sets2 = {}\n",
    "    for feat in present_feats:\n",
    "        s2 = set()\n",
    "        for d in dirs[(feat,\"fake\")]:\n",
    "            for p in list_npy(d):\n",
    "                s2.add(base_id(p.stem) if feat==\"ela\" else p.stem)\n",
    "        id_sets2[feat] = s2\n",
    "    ids_common2 = None\n",
    "    for feat, s in id_sets2.items():\n",
    "        ids_common2 = s if ids_common2 is None else (ids_common2 & s)\n",
    "\n",
    "    by_src2 = Counter(dataset_of(i) for i in ids_common2)\n",
    "    print(\"After cleanup â†’ intersection fake IDs by source:\")\n",
    "    for src in sorted(by_src2.keys()):\n",
    "        print(f\"  {src:22s} {by_src2[src]:6d}\")\n",
    "    if MODE==\"trash\":\n",
    "        print(\"Trash dir:\", TRASH_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9716a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äº¤é›†æ•¸é‡ â†’ real: 86772 | fake: 81720\n",
      "\n",
      "== IID summary ==\n",
      "[IID train] total=102398 | real=51198 fake=51200\n",
      "[IID val] total=12801 | real=6401 fake=6400\n",
      "[IID test] total=12801 | real=6401 fake=6400\n",
      "  real per-src: {'flickr30k': 22857, 'imagenet': 22857, 'unsplash': 18286}\n",
      "  fake per-src: {'dalle3': 19000, 'flux': 20000, 'sd3': 25000}\n",
      "\n",
      "== OOD summaries (by generator) ==\n",
      "[OOD-sd3 train] total=62399 | real=31199 fake=31200\n",
      "[OOD-sd3 val] total=7800 | real=3900 fake=3900\n",
      "[OOD-sd3 test] total=32801 | real=3901 fake=28900\n",
      "  holdout sd3 â†’ 25000\n",
      "[OOD-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD-midjourney test] total=12801 | real=6401 fake=6400\n",
      "  holdout midjourney â†’ 0\n",
      "[OOD-flux train] total=70399 | real=35199 fake=35200\n",
      "[OOD-flux val] total=8799 | real=4399 fake=4400\n",
      "[OOD-flux test] total=28802 | real=4402 fake=24400\n",
      "  holdout flux â†’ 20000\n",
      "[OOD-dalle3 train] total=71998 | real=35998 fake=36000\n",
      "[OOD-dalle3 val] total=9000 | real=4500 fake=4500\n",
      "[OOD-dalle3 test] total=28002 | real=4502 fake=23500\n",
      "  holdout dalle3 â†’ 19000\n",
      "\n",
      "âœ… saved: /home/yaya/ai-detect-proj/Script/splits_iid_ood.json\n"
     ]
    }
   ],
   "source": [
    "# ===== Build IID + OOD splits (complete version, with your quotas) =====\n",
    "from pathlib import Path\n",
    "import re, json, random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- åŸºæœ¬è¨­å®š ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "FEA_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_256\")            # â† æ”¹æˆä½ çš„ features æ ¹ç›®éŒ„\n",
    "OUT_JSON = Path(\"/home/yaya/ai-detect-proj/Script/splits_iid_ood.json\") # â† è¼¸å‡º JSON è·¯å¾‘\n",
    "\n",
    "# ç”¨å“ªäº›ç‰¹å¾µåšäº¤é›†ï¼ˆå»ºè­°ä¸‰è€…ï¼›åªæƒ³å¿«ä¸€é»å¯æš«æ™‚æ”¹ ['ela']ï¼‰\n",
    "INTERSECT_FEATURES = ['ela','clip','prnu']\n",
    "\n",
    "# ä½ çš„é…é¡ï¼ˆå›ºå®šï¼‰\n",
    "FAKE_QUOTAS = {\"sd3\":25_000, \"midjourney\":30_000, \"flux\":20_000, \"dalle3\":19_000}   # 94k\n",
    "REAL_QUOTAS = {\"imagenet\":30_000, \"flickr30k\":30_000, \"unsplash\":24_000}            # 84k\n",
    "\n",
    "# åˆ‡åˆ†æ¯”ä¾‹\n",
    "RATIOS = {\"train\":0.8, \"val\":0.1, \"test\":0.1}\n",
    "\n",
    "# ---------- ä¾†æºåˆ¥åï¼ˆå‰ç¶´ -> æ¨™æº–åï¼‰----------\n",
    "ALIAS = {\n",
    "    # real\n",
    "    \"flick\":\"flickr30k\",\"flicker\":\"flickr30k\",\"flickr\":\"flickr30k\",\"flickr30k\":\"flickr30k\",\n",
    "    \"unsplash\":\"unsplash\",\"imagenet\":\"imagenet\",\n",
    "    # fake\n",
    "    \"sd3\":\"sd3\",\"stable-diffusion-3\":\"sd3\",\"sd3.5\":\"sd3\",\n",
    "    \"midjourney\":\"midjourney\",\"midjourney-v6-llava\":\"midjourney\",\"mj\":\"midjourney\",\n",
    "    \"flux\":\"flux\",\"FLUX\":\"flux\",\n",
    "    \"dalle\":\"dalle3\",\"dalle3\":\"dalle3\",\"dall-e-3\":\"dalle3\",\n",
    "}\n",
    "\n",
    "# ---------- å°å·¥å…· ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def norm_source(raw: str):\n",
    "    return ALIAS.get(raw.lower(), raw.lower())\n",
    "\n",
    "def dataset_of(img_id: str):\n",
    "    return norm_source(img_id.split(\"__\",1)[0])\n",
    "\n",
    "def list_feature_dirs(feat: str, cls: str):\n",
    "    return [d for d in FEA_ROOT.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_ids_for(feat: str, cls: str):\n",
    "    ids = set()\n",
    "    for d in list_feature_dirs(feat, cls):\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            s = p.stem\n",
    "            s = base_id(s) if feat == \"ela\" else s\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "def bucket_by_source(ids):\n",
    "    b = defaultdict(list)\n",
    "    for i in ids:\n",
    "        b[dataset_of(i)].append(i)\n",
    "    for k in b: random.shuffle(b[k])\n",
    "    return b\n",
    "\n",
    "def sample_strict_quota(bucket, quotas):\n",
    "    \"\"\"åš´æ ¼ä¾é…é¡æŠ½æ¨£ï¼›ä¸è¶³å°±ç›¡é‡æ‹¿ã€‚å›å‚³ keep èˆ‡å„ä¾†æºå¯ç”¨é‡ã€‚\"\"\"\n",
    "    keep = []\n",
    "    avail = {k: len(v) for k,v in bucket.items()}\n",
    "    for src, q in quotas.items():\n",
    "        got = bucket.get(src, [])[:q]\n",
    "        keep.extend(got)\n",
    "    return keep, avail\n",
    "\n",
    "def scale_real_quotas_to(total_target, avail_per_src, base_quotas):\n",
    "    \"\"\"æŠŠ real é…é¡æŒ‰æ¯”ä¾‹ç¸®æ”¾åˆ° total_targetï¼›ä¸è¶…éå¯ç”¨é‡ï¼›ä¸è¶³å°±è¼ªè©¢è£œé½Šã€‚\"\"\"\n",
    "    s = sum(base_quotas.values())\n",
    "    if s == 0: return {}\n",
    "    quotas = {}\n",
    "    acc = 0\n",
    "    keys = sorted(base_quotas.keys())\n",
    "    for k in keys[:-1]:\n",
    "        q = int(round(total_target * base_quotas[k] / s))\n",
    "        quotas[k] = q; acc += q\n",
    "    quotas[keys[-1]] = max(0, total_target - acc)\n",
    "    # æˆªæ–·åˆ°å¯ç”¨\n",
    "    for k in list(quotas.keys()):\n",
    "        quotas[k] = min(quotas[k], avail_per_src.get(k, 0))\n",
    "    # è‹¥ç¸½é‡ä»ä¸è¶³ï¼Œå¾æœ‰é¤˜é‡çš„ä¾†æºè¼ªè©¢è£œ\n",
    "    need = total_target - sum(quotas.values())\n",
    "    if need > 0:\n",
    "        remain = {k: max(0, avail_per_src.get(k,0) - quotas.get(k,0)) for k in avail_per_src}\n",
    "        srcs = sorted(remain.keys())\n",
    "        while need > 0:\n",
    "            progressed = False\n",
    "            for sname in srcs:\n",
    "                if remain[sname] > 0:\n",
    "                    quotas[sname] = quotas.get(sname,0) + 1\n",
    "                    remain[sname] -= 1\n",
    "                    need -= 1\n",
    "                    progressed = True\n",
    "                    if need == 0: break\n",
    "            if not progressed:\n",
    "                break\n",
    "    return quotas\n",
    "\n",
    "def split_8_1_1_per_source(selected_ids):\n",
    "    \"\"\"æ¯ä¾†æºå„è‡ª 8/1/1ï¼Œå†åˆä½µï¼›ä¿æŒä¾†æºæ¯”ä¾‹ã€‚\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    out = {\"train\":[], \"val\":[], \"test\":[]}\n",
    "    per_src = bucket_by_source(selected_ids)\n",
    "    for src, ids in per_src.items():\n",
    "        if len(ids) < 10:\n",
    "            n_tr = int(len(ids) * RATIOS[\"train\"])\n",
    "            n_va = int(len(ids) * RATIOS[\"val\"])\n",
    "            out[\"train\"].extend(ids[:n_tr])\n",
    "            out[\"val\"].extend(ids[n_tr:n_tr+n_va])\n",
    "            out[\"test\"].extend(ids[n_tr+n_va:])\n",
    "        else:\n",
    "            tr, tmp = train_test_split(ids, test_size=(1-RATIOS[\"train\"]), random_state=SEED)\n",
    "            va, te  = train_test_split(tmp, test_size=RATIOS[\"test\"]/(RATIOS[\"test\"]+RATIOS[\"val\"]), random_state=SEED)\n",
    "            out[\"train\"].extend(tr); out[\"val\"].extend(va); out[\"test\"].extend(te)\n",
    "    for k in out: random.shuffle(out[k])\n",
    "    return out\n",
    "\n",
    "def report_counts(ids):\n",
    "    c = Counter(dataset_of(i) for i in ids)\n",
    "    return {\"total\": len(ids), \"by_source\": dict(sorted(c.items()))}\n",
    "\n",
    "# ---------- æƒæ & ç‰¹å¾µäº¤é›† ----------\n",
    "present = {}\n",
    "for feat in INTERSECT_FEATURES:\n",
    "    present[feat] = {\"real\": list_ids_for(feat,\"real\"),\n",
    "                     \"fake\": list_ids_for(feat,\"fake\")}\n",
    "sets_real = [present[feat][\"real\"] for feat in INTERSECT_FEATURES if present[feat][\"real\"]]\n",
    "sets_fake = [present[feat][\"fake\"] for feat in INTERSECT_FEATURES if present[feat][\"fake\"]]\n",
    "assert sets_real and sets_fake, \"æ‰¾ä¸åˆ°å¯ç”¨çš„ real æˆ– fake IDsï¼Œè«‹æª¢æŸ¥ç‰¹å¾µè·¯å¾‘èˆ‡ INTERSECT_FEATURESã€‚\"\n",
    "\n",
    "common = {\n",
    "    \"real\": set.intersection(*sets_real) if len(sets_real)>1 else sets_real[0],\n",
    "    \"fake\": set.intersection(*sets_fake) if len(sets_fake)>1 else sets_fake[0],\n",
    "}\n",
    "print(\"äº¤é›†æ•¸é‡ â†’ real:\", len(common[\"real\"]), \"| fake:\", len(common[\"fake\"]))\n",
    "\n",
    "real_bkt = bucket_by_source(common[\"real\"])\n",
    "fake_bkt = bucket_by_source(common[\"fake\"])\n",
    "\n",
    "# ---------- IID ----------\n",
    "# fakeï¼šåš´æ ¼ä¾é…é¡\n",
    "fake_keep_iid, fake_avail = sample_strict_quota(fake_bkt, FAKE_QUOTAS)\n",
    "\n",
    "# realï¼šå˜—è©¦ç¸®æ”¾åˆ°èˆ‡ fake åŒç¸½é‡ï¼ˆå¦‚ä¸è¶³å°±æ‹¿åˆ°ä¸Šé™ï¼‰\n",
    "avail_real = {k: len(v) for k,v in real_bkt.items()}\n",
    "real_quota_scaled = scale_real_quotas_to(len(fake_keep_iid), avail_real, REAL_QUOTAS)\n",
    "real_keep_iid, _ = sample_strict_quota(real_bkt, real_quota_scaled)\n",
    "\n",
    "iid_real = split_8_1_1_per_source(real_keep_iid)\n",
    "iid_fake = split_8_1_1_per_source(fake_keep_iid)\n",
    "iid = { \"train\": iid_real[\"train\"] + iid_fake[\"train\"],\n",
    "        \"val\":   iid_real[\"val\"]   + iid_fake[\"val\"],\n",
    "        \"test\":  iid_real[\"test\"]  + iid_fake[\"test\"] }\n",
    "for k in iid: random.shuffle(iid[k])\n",
    "\n",
    "# ---------- OOD-by-Generatorï¼ˆå››å€‹ï¼‰ ----------\n",
    "def build_ood_for(hold_src: str):\n",
    "    hold_src = hold_src.lower()\n",
    "    # holdoutï¼šæ•´å€‹ä¾†æºï¼ŒæŒ‰é…é¡ä¸Šé™æˆªæ–·\n",
    "    hold_quota = FAKE_QUOTAS.get(hold_src, len(fake_bkt.get(hold_src, [])))\n",
    "    hold_ids   = fake_bkt.get(hold_src, [])[:min(hold_quota, len(fake_bkt.get(hold_src, [])))]\n",
    "    # å…¶é¤˜ fakeï¼šç…§é…é¡æŠ½æ¨£\n",
    "    remain_quota = {s:q for s,q in FAKE_QUOTAS.items() if s != hold_src}\n",
    "    fake_keep, _ = sample_strict_quota(fake_bkt, remain_quota)\n",
    "    # realï¼šç¸®æ”¾åˆ°èˆ‡ã€Œè¨“ç·´ç”¨ fakeã€åŒç¸½é‡\n",
    "    real_quota_scaled = scale_real_quotas_to(len(fake_keep), avail_real, REAL_QUOTAS)\n",
    "    real_keep, _ = sample_strict_quota(real_bkt, real_quota_scaled)\n",
    "    # split\n",
    "    r = split_8_1_1_per_source(real_keep)\n",
    "    f = split_8_1_1_per_source(fake_keep)\n",
    "    ood = { \"train\": r[\"train\"] + f[\"train\"],\n",
    "            \"val\":   r[\"val\"]   + f[\"val\"],\n",
    "            \"test\":  r[\"test\"]  + f[\"test\"] + hold_ids }\n",
    "    for k in ood: random.shuffle(ood[k])\n",
    "    rep = {\n",
    "        \"train\": report_counts(ood[\"train\"]),\n",
    "        \"val\":   report_counts(ood[\"val\"]),\n",
    "        \"test\":  report_counts(ood[\"test\"]),\n",
    "        \"holdout_source\": hold_src,\n",
    "        \"holdout_size\": len(hold_ids)\n",
    "    }\n",
    "    return ood, rep\n",
    "\n",
    "ood_all = {}\n",
    "ood_reports = {}\n",
    "for src in [\"sd3\",\"midjourney\",\"flux\",\"dalle3\"]:\n",
    "    ood_all[src], ood_reports[src] = build_ood_for(src)\n",
    "\n",
    "# ---------- å ±å‘Š & å­˜æª” ----------\n",
    "def quick_report(name, split, real_pool):\n",
    "    rset = set(real_pool[\"train\"] + real_pool[\"val\"] + real_pool[\"test\"])\n",
    "    for sp in (\"train\",\"val\",\"test\"):\n",
    "        both = split[sp]\n",
    "        n_total = len(both)\n",
    "        n_real  = sum(1 for i in both if i in rset)\n",
    "        n_fake  = n_total - n_real\n",
    "        print(f\"[{name} {sp}] total={n_total} | real={n_real} fake={n_fake}\")\n",
    "\n",
    "print(\"\\n== IID summary ==\")\n",
    "quick_report(\"IID\", iid, iid_real)\n",
    "print(\"  real per-src:\", report_counts(iid_real[\"train\"]+iid_real[\"val\"]+iid_real[\"test\"])[\"by_source\"])\n",
    "print(\"  fake per-src:\", report_counts(iid_fake[\"train\"]+iid_fake[\"val\"]+iid_fake[\"test\"])[\"by_source\"])\n",
    "\n",
    "print(\"\\n== OOD summaries (by generator) ==\")\n",
    "for src in ood_all:\n",
    "    quick_report(f\"OOD-{src}\", ood_all[src], iid_real)  # åªæ˜¯ç”¨ä¾†å€åˆ† real/fake\n",
    "    print(f\"  holdout {src} â†’\", ood_reports[src][\"holdout_size\"])\n",
    "\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"intersect_features\": INTERSECT_FEATURES,\n",
    "    \"fake_quotas\": FAKE_QUOTAS,\n",
    "    \"real_quotas\": REAL_QUOTAS,\n",
    "    \"avail\": {\n",
    "        \"real_by_src\": {k:len(v) for k,v in real_bkt.items()},\n",
    "        \"fake_by_src\": {k:len(v) for k,v in fake_bkt.items()},\n",
    "        \"real_intersection_total\": len(common[\"real\"]),\n",
    "        \"fake_intersection_total\": len(common[\"fake\"]),\n",
    "    },\n",
    "    \"ood_reports\": ood_reports,\n",
    "}\n",
    "payload = {\"meta\": meta, \"iid\": iid, \"ood_gen\": ood_all}\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "print(\"\\nâœ… saved:\", OUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c690e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äº¤é›†æ•¸é‡ â†’ real: 86772 | fake: 81720\n",
      "\n",
      "== IID summary ==\n",
      "[IID train] total=132576 | real=67200 fake=65376\n",
      "[IID val] total=16572 | real=8400 fake=8172\n",
      "[IID test] total=16572 | real=8400 fake=8172\n",
      "\n",
      "== IID_balanced summary ==\n",
      "[IID_BAL train] total=127128 | real=67200 fake=59928\n",
      "[IID_BAL val] total=15891 | real=8400 fake=7491\n",
      "[IID_BAL test] total=15892 | real=8400 fake=7492\n",
      "\n",
      "== OOD summaries ==\n",
      "[OOD-sd3 train] total=90750 | real=45374 fake=45376\n",
      "[OOD-sd3 val] total=11345 | real=5673 fake=5672\n",
      "[OOD-sd3 test] total=36345 | real=5673 fake=30672\n",
      "  holdout sd3 â†’ 25000\n",
      "[OOD-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD-midjourney test] total=30521 | real=6401 fake=24120\n",
      "  holdout midjourney â†’ 17720\n",
      "[OOD-flux train] total=98751 | real=49375 fake=49376\n",
      "[OOD-flux val] total=12343 | real=6171 fake=6172\n",
      "[OOD-flux test] total=32346 | real=6174 fake=26172\n",
      "  holdout flux â†’ 20000\n",
      "[OOD-dalle3 train] total=100352 | real=50176 fake=50176\n",
      "[OOD-dalle3 val] total=12544 | real=6272 fake=6272\n",
      "[OOD-dalle3 test] total=31544 | real=6272 fake=25272\n",
      "  holdout dalle3 â†’ 19000\n",
      "\n",
      "== OOD_strict summaries ==\n",
      "[OOD_STRICT-sd3 train] total=90750 | real=45374 fake=45376\n",
      "[OOD_STRICT-sd3 val] total=11345 | real=5673 fake=5672\n",
      "[OOD_STRICT-sd3 test] total=30673 | real=5673 fake=25000\n",
      "[OOD_STRICT-midjourney train] total=102398 | real=51198 fake=51200\n",
      "[OOD_STRICT-midjourney val] total=12801 | real=6401 fake=6400\n",
      "[OOD_STRICT-midjourney test] total=24121 | real=6401 fake=17720\n",
      "[OOD_STRICT-flux train] total=98751 | real=49375 fake=49376\n",
      "[OOD_STRICT-flux val] total=12343 | real=6171 fake=6172\n",
      "[OOD_STRICT-flux test] total=26174 | real=6174 fake=20000\n",
      "[OOD_STRICT-dalle3 train] total=100352 | real=50176 fake=50176\n",
      "[OOD_STRICT-dalle3 val] total=12544 | real=6272 fake=6272\n",
      "[OOD_STRICT-dalle3 test] total=25272 | real=6272 fake=19000\n",
      "\n",
      "âœ… saved: /home/yaya/ai-detect-proj/Script/splits/combined_split.json\n"
     ]
    }
   ],
   "source": [
    "# ===== Unified split builder: IID / IID_balanced / OOD(by generator) / OOD_strict / smoke_10p =====\n",
    "from pathlib import Path\n",
    "import re, json, random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------- åŸºæœ¬è¨­å®šï¼ˆæ”¹é€™è£¡ï¼‰ ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "SCRIPT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "FEA_ROOT    = SCRIPT_ROOT / \"features_256\"                         # features æ ¹\n",
    "OUT_JSON    = SCRIPT_ROOT / \"splits/combined_split.json\"           # è¼¸å‡ºæª”\n",
    "\n",
    "# ç”¨å“ªäº›ç‰¹å¾µåšäº¤é›†ï¼ˆå»ºè­°ä¸‰è€…ï¼›è¦åŠ é€Ÿå¯æ”¹æˆ ['ela']ï¼‰\n",
    "INTERSECT_FEATURES = ['ela','clip','prnu']\n",
    "\n",
    "# ä½ çš„é…é¡ï¼ˆå›ºå®šï¼‰\n",
    "FAKE_QUOTAS = {\"sd3\":25_000, \"midjourney\":30_000, \"flux\":20_000, \"dalle3\":19_000}  # 94k\n",
    "REAL_QUOTAS = {\"imagenet\":30_000, \"flickr30k\":30_000, \"unsplash\":24_000}           # 84k\n",
    "\n",
    "# åˆ‡åˆ†æ¯”ä¾‹\n",
    "RATIOS = {\"train\":0.8, \"val\":0.1, \"test\":0.1}\n",
    "\n",
    "# ç”¢ç”Ÿä¸€ä»½ 10% çš„ smokeï¼Œå¾å“ªå€‹ split æŠ½\n",
    "SMOKE_BASE_KEY = \"iid\"   # å¯æ”¹æˆ \"ood_gen.sd3\" / \"ood_gen.midjourney\" / \"ood_gen.flux\" / \"ood_gen.dalle3\"\n",
    "\n",
    "# ---------- ä¾†æºåˆ¥åï¼ˆå‰ç¶´ -> æ¨™æº–åï¼‰----------\n",
    "ALIAS = {\n",
    "    # real\n",
    "    \"flick\":\"flickr30k\",\"flicker\":\"flickr30k\",\"flickr\":\"flickr30k\",\"flickr30k\":\"flickr30k\",\n",
    "    \"unsplash\":\"unsplash\",\"imagenet\":\"imagenet\",\n",
    "    # fake\n",
    "    \"sd3\":\"sd3\",\"stable-diffusion-3\":\"sd3\",\"sd3.5\":\"sd3\",\n",
    "    \"midjourney\":\"midjourney\",\"midjourney-v6\":\"midjourney\",\"mj\":\"midjourney\",\n",
    "    \"flux\":\"flux\",\"FLUX\":\"flux\",\n",
    "    \"dalle\":\"dalle3\",\"dalle3\":\"dalle3\",\"dall-e-3\":\"dalle3\",\n",
    "}\n",
    "\n",
    "REAL_SOURCES = {\"imagenet\",\"flickr30k\",\"unsplash\"}\n",
    "\n",
    "# ---------- å°å·¥å…· ----------\n",
    "ELA_Q_PAT = re.compile(r\"__q(\\d+)$\")\n",
    "def base_id(stem: str):\n",
    "    m = ELA_Q_PAT.search(stem)\n",
    "    return stem[:m.start()] if m else stem\n",
    "\n",
    "def norm_source(raw: str): return ALIAS.get(raw.lower(), raw.lower())\n",
    "def dataset_of(img_id: str): return norm_source(img_id.split(\"__\",1)[0])\n",
    "def is_real_id(img_id: str): return dataset_of(img_id) in REAL_SOURCES\n",
    "\n",
    "def list_feature_dirs(feat: str, cls: str):\n",
    "    return [d for d in FEA_ROOT.glob(f\"**/{feat}_{cls}_npy\") if d.is_dir()]\n",
    "\n",
    "def list_ids_for(feat: str, cls: str):\n",
    "    ids = set()\n",
    "    for d in list_feature_dirs(feat, cls):\n",
    "        for p in d.glob(\"*.npy\"):\n",
    "            s = p.stem\n",
    "            s = base_id(s) if feat == \"ela\" else s\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "def bucket_by_source(ids):\n",
    "    b = defaultdict(list)\n",
    "    for i in ids:\n",
    "        b[dataset_of(i)].append(i)\n",
    "    for k in b: random.shuffle(b[k])\n",
    "    return b\n",
    "\n",
    "def sample_quota(bucket, quotas):\n",
    "    \"\"\"ä¾é…é¡æŠ½æ¨£ï¼›ä¸è¶³å°±æ‹¿å¯ç”¨é‡ã€‚å›å‚³ keep èˆ‡å„ä¾†æºå¯ç”¨é‡\"\"\"\n",
    "    keep = []\n",
    "    avail = {k: len(v) for k,v in bucket.items()}\n",
    "    for src, q in quotas.items():\n",
    "        got = bucket.get(src, [])[:q]\n",
    "        keep.extend(got)\n",
    "    return keep, avail\n",
    "\n",
    "def scale_real_quotas_to(total_target, avail_per_src, base_quotas):\n",
    "    \"\"\"æŠŠ real é…é¡æŒ‰æ¯”ä¾‹ç¸®æ”¾åˆ° total_targetï¼›ä¸è¶…éå¯ç”¨é‡ï¼›ä¸è¶³å°±è¼ªè©¢è£œé½Šã€‚\"\"\"\n",
    "    s = sum(base_quotas.values())\n",
    "    if s == 0: return {}\n",
    "    quotas = {}\n",
    "    acc = 0\n",
    "    keys = sorted(base_quotas.keys())\n",
    "    for k in keys[:-1]:\n",
    "        q = int(round(total_target * base_quotas[k] / s))\n",
    "        quotas[k] = q; acc += q\n",
    "    quotas[keys[-1]] = max(0, total_target - acc)\n",
    "    # æˆªåˆ°å¯ç”¨\n",
    "    for k in list(quotas.keys()):\n",
    "        quotas[k] = min(quotas[k], avail_per_src.get(k, 0))\n",
    "    # è‹¥ä»ä¸è¶³ï¼Œè¼ªè©¢è£œ\n",
    "    need = total_target - sum(quotas.values())\n",
    "    if need > 0:\n",
    "        remain = {k: max(0, avail_per_src.get(k,0) - quotas.get(k,0)) for k in avail_per_src}\n",
    "        srcs = sorted(remain.keys())\n",
    "        while need > 0:\n",
    "            progressed = False\n",
    "            for sname in srcs:\n",
    "                if remain[sname] > 0:\n",
    "                    quotas[sname] = quotas.get(sname,0) + 1\n",
    "                    remain[sname] -= 1\n",
    "                    need -= 1\n",
    "                    progressed = True\n",
    "                    if need == 0: break\n",
    "            if not progressed: break\n",
    "    return quotas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_8_1_1_per_source(selected_ids):\n",
    "    \"\"\"å„ä¾†æº 8/1/1ï¼Œå†åˆä½µï¼Œä¿æŒä¾†æºæ¯”ä¾‹ã€‚\"\"\"\n",
    "    out = {\"train\":[], \"val\":[], \"test\":[]}\n",
    "    per_src = bucket_by_source(selected_ids)\n",
    "    for src, ids in per_src.items():\n",
    "        if len(ids) < 10:\n",
    "            n_tr = int(len(ids) * RATIOS[\"train\"])\n",
    "            n_va = int(len(ids) * RATIOS[\"val\"])\n",
    "            out[\"train\"].extend(ids[:n_tr])\n",
    "            out[\"val\"].extend(ids[n_tr:n_tr+n_va])\n",
    "            out[\"test\"].extend(ids[n_tr+n_va:])\n",
    "        else:\n",
    "            tr, tmp = train_test_split(ids, test_size=(1-RATIOS[\"train\"]), random_state=SEED, shuffle=True)\n",
    "            va, te  = train_test_split(tmp, test_size=RATIOS[\"test\"]/(RATIOS[\"test\"]+RATIOS[\"val\"]), random_state=SEED, shuffle=True)\n",
    "            out[\"train\"].extend(tr); out[\"val\"].extend(va); out[\"test\"].extend(te)\n",
    "    for k in out: random.shuffle(out[k])\n",
    "    return out\n",
    "\n",
    "def report_counts(ids):\n",
    "    c = Counter(dataset_of(i) for i in ids)\n",
    "    return {\"total\": len(ids), \"by_source\": dict(sorted(c.items()))}\n",
    "\n",
    "def summarize(name, split, real_pool):\n",
    "    rset = set(real_pool[\"train\"] + real_pool[\"val\"] + real_pool[\"test\"])\n",
    "    for sp in (\"train\",\"val\",\"test\"):\n",
    "        both = split[sp]\n",
    "        n_total = len(both)\n",
    "        n_real  = sum(1 for i in both if i in rset)\n",
    "        n_fake  = n_total - n_real\n",
    "        print(f\"[{name} {sp}] total={n_total} | real={n_real} fake={n_fake}\")\n",
    "\n",
    "# ---------- æƒæ & ç‰¹å¾µäº¤é›† ----------\n",
    "present = {}\n",
    "for feat in INTERSECT_FEATURES:\n",
    "    present[feat] = {\"real\": list_ids_for(feat,\"real\"),\n",
    "                     \"fake\": list_ids_for(feat,\"fake\")}\n",
    "sets_real = [present[feat][\"real\"] for feat in INTERSECT_FEATURES if present[feat][\"real\"]]\n",
    "sets_fake = [present[feat][\"fake\"] for feat in INTERSECT_FEATURES if present[feat][\"fake\"]]\n",
    "assert sets_real and sets_fake, \"æ‰¾ä¸åˆ°å¯ç”¨çš„ real æˆ– fake IDsï¼Œè«‹æª¢æŸ¥ç‰¹å¾µè·¯å¾‘èˆ‡ INTERSECT_FEATURESã€‚\"\n",
    "\n",
    "common = {\n",
    "    \"real\": set.intersection(*sets_real) if len(sets_real)>1 else sets_real[0],\n",
    "    \"fake\": set.intersection(*sets_fake) if len(sets_fake)>1 else sets_fake[0],\n",
    "}\n",
    "print(\"äº¤é›†æ•¸é‡ â†’ real:\", len(common[\"real\"]), \"| fake:\", len(common[\"fake\"]))\n",
    "\n",
    "real_bkt = bucket_by_source(common[\"real\"])\n",
    "fake_bkt = bucket_by_source(common[\"fake\"])\n",
    "avail_real = {k: len(v) for k,v in real_bkt.items()}\n",
    "\n",
    "# ---------- IID ----------\n",
    "# fakeï¼šåš´æ ¼ä¾é…é¡\n",
    "iid_fake_keep, fake_avail = sample_quota(fake_bkt, FAKE_QUOTAS)\n",
    "# realï¼šå›ºå®šç”¨ä½ çš„ REAL_QUOTASï¼ˆä¸ç¸®æ”¾ï¼‰\n",
    "iid_real_keep, _ = sample_quota(real_bkt, REAL_QUOTAS)\n",
    "\n",
    "iid_real = split_8_1_1_per_source(iid_real_keep)\n",
    "iid_fake = split_8_1_1_per_source(iid_fake_keep)\n",
    "iid = { \"train\": iid_real[\"train\"] + iid_fake[\"train\"],\n",
    "        \"val\":   iid_real[\"val\"]   + iid_fake[\"val\"],\n",
    "        \"test\":  iid_real[\"test\"]  + iid_fake[\"test\"] }\n",
    "for k in iid: random.shuffle(iid[k])\n",
    "\n",
    "# ---------- IID_balancedï¼ˆè®“ fake ä¸‹æ¡æ¨£åˆ° real çš„ç¸½é‡ï¼‰ ----------\n",
    "total_real_target = sum(len(v) for v in iid_real.values())\n",
    "# é€™è£¡ç”¨å„ç”Ÿæˆå™¨çš„é…é¡å æ¯”ä¾†æ±ºå®š fake ä¸‹æ¡æ¨£æ¯”ä¾‹\n",
    "sum_fake_quota = sum(FAKE_QUOTAS.values())\n",
    "bal_fake_quota = {src: int(round(total_real_target * q / sum_fake_quota)) for src, q in FAKE_QUOTAS.items()}\n",
    "bal_fake_keep, _ = sample_quota(fake_bkt, bal_fake_quota)\n",
    "bal_fake = split_8_1_1_per_source(bal_fake_keep)\n",
    "iid_balanced = { \"train\": iid_real[\"train\"] + bal_fake[\"train\"],\n",
    "                 \"val\":   iid_real[\"val\"]   + bal_fake[\"val\"],\n",
    "                 \"test\":  iid_real[\"test\"]  + bal_fake[\"test\"] }\n",
    "for k in iid_balanced: random.shuffle(iid_balanced[k])\n",
    "\n",
    "# ---------- OOD-by-Generatorï¼ˆæ··åˆ testï¼‰ ----------\n",
    "def build_ood_for(hold_src: str, strict=False):\n",
    "    hold_src = hold_src.lower()\n",
    "    # holdoutï¼šæ•´å€‹ä¾†æºï¼ŒæŒ‰é…é¡æˆªæ–·\n",
    "    hold_quota = FAKE_QUOTAS.get(hold_src, len(fake_bkt.get(hold_src, [])))\n",
    "    hold_ids   = fake_bkt.get(hold_src, [])[:min(hold_quota, len(fake_bkt.get(hold_src, [])))]\n",
    "    # å…¶é¤˜ fakeï¼šç…§é…é¡æŠ½æ¨£\n",
    "    remain_quota = {s:q for s,q in FAKE_QUOTAS.items() if s != hold_src}\n",
    "    fake_keep, _ = sample_quota(fake_bkt, remain_quota)\n",
    "    # realï¼šç¸®æ”¾åˆ°èˆ‡ã€Œè¨“ç·´ç”¨ fakeã€åŒç¸½é‡\n",
    "    real_quota_scaled = scale_real_quotas_to(len(fake_keep), avail_real, REAL_QUOTAS)\n",
    "    real_keep, _ = sample_quota(real_bkt, real_quota_scaled)\n",
    "    # split\n",
    "    r = split_8_1_1_per_source(real_keep)\n",
    "    f = split_8_1_1_per_source(fake_keep)\n",
    "    if strict:\n",
    "        test_fake = hold_ids                      # åªæ”¾ holdout\n",
    "    else:\n",
    "        test_fake = f[\"test\"] + hold_ids          # æ··åˆ test\n",
    "    ood = { \"train\": r[\"train\"] + f[\"train\"],\n",
    "            \"val\":   r[\"val\"]   + f[\"val\"],\n",
    "            \"test\":  r[\"test\"]  + test_fake }\n",
    "    for k in ood: random.shuffle(ood[k])\n",
    "    rep = {\n",
    "        \"train\": report_counts(ood[\"train\"]),\n",
    "        \"val\":   report_counts(ood[\"val\"]),\n",
    "        \"test\":  report_counts(ood[\"test\"]),\n",
    "        \"holdout_source\": hold_src,\n",
    "        \"holdout_size\": len(hold_ids)\n",
    "    }\n",
    "    return ood, rep\n",
    "\n",
    "ood_gen = {}\n",
    "ood_gen_strict = {}\n",
    "ood_reports = {}\n",
    "for src in [\"sd3\",\"midjourney\",\"flux\",\"dalle3\"]:\n",
    "    ood_gen[src], ood_reports[src] = build_ood_for(src, strict=False)\n",
    "    ood_gen_strict[src], _         = build_ood_for(src, strict=True)\n",
    "\n",
    "# ---------- 10% smokeï¼ˆå¾ SMOKE_BASE_KEY æŠ½ï¼‰ ----------\n",
    "def get_by_key(tree, key: str):\n",
    "    node = tree\n",
    "    for k in key.split(\".\"):\n",
    "        node = node[k]\n",
    "    return node\n",
    "\n",
    "def subsample_by_class_source(ids, frac=0.1, min_each=1, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    buckets = defaultdict(list)\n",
    "    for i in ids:\n",
    "        buckets[( \"real\" if is_real_id(i) else \"fake\", dataset_of(i) )].append(i)\n",
    "    keep = []\n",
    "    for (_c,_s), arr in buckets.items():\n",
    "        random.shuffle(arr)\n",
    "        k = max(min_each, int(round(len(arr)*frac)))\n",
    "        keep.extend(arr[:k])\n",
    "    random.shuffle(keep)\n",
    "    return keep\n",
    "\n",
    "tmp_all = {\n",
    "    \"iid\": iid,\n",
    "    \"iid_balanced\": iid_balanced,\n",
    "    \"ood_gen\": ood_gen,\n",
    "    \"ood_gen_strict\": ood_gen_strict\n",
    "}\n",
    "base = get_by_key(tmp_all, SMOKE_BASE_KEY)\n",
    "smoke_10p = {\n",
    "    \"train\": subsample_by_class_source(base[\"train\"], 0.10, seed=SEED+1),\n",
    "    \"val\":   subsample_by_class_source(base[\"val\"],   0.10, seed=SEED+2),\n",
    "    \"test\":  subsample_by_class_source(base[\"test\"],  0.10, seed=SEED+3),\n",
    "}\n",
    "\n",
    "# ---------- ç°¡å ± ----------\n",
    "print(\"\\n== IID summary ==\"); summarize(\"IID\", iid, iid_real)\n",
    "print(\"\\n== IID_balanced summary ==\"); summarize(\"IID_BAL\", iid_balanced, iid_real)\n",
    "print(\"\\n== OOD summaries ==\")\n",
    "for src in ood_gen:\n",
    "    summarize(f\"OOD-{src}\", ood_gen[src], iid_real)\n",
    "    print(f\"  holdout {src} â†’\", ood_reports[src][\"holdout_size\"])\n",
    "print(\"\\n== OOD_strict summaries ==\")\n",
    "for src in ood_gen_strict:\n",
    "    summarize(f\"OOD_STRICT-{src}\", ood_gen_strict[src], iid_real)\n",
    "\n",
    "# ---------- å­˜æª” ----------\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "payload = {\n",
    "    \"meta\": {\n",
    "        \"seed\": SEED,\n",
    "        \"intersect_features\": INTERSECT_FEATURES,\n",
    "        \"fake_quotas\": FAKE_QUOTAS,\n",
    "        \"real_quotas\": REAL_QUOTAS,\n",
    "        \"avail\": {\n",
    "            \"real_by_src\": {k:len(v) for k,v in real_bkt.items()},\n",
    "            \"fake_by_src\": {k:len(v) for k,v in fake_bkt.items()},\n",
    "            \"real_intersection_total\": len(common[\"real\"]),\n",
    "            \"fake_intersection_total\": len(common[\"fake\"]),\n",
    "        },\n",
    "        \"notes\": \"iid: å›ºå®šé…é¡ï¼›iid_balanced: fake ä¸‹æ¡æ¨£åˆ° real ç¸½é‡ï¼›ood_gen: æ··åˆ testï¼›ood_gen_strict: åš´æ ¼ OOD\",\n",
    "    },\n",
    "    \"iid\": iid,\n",
    "    \"iid_balanced\": iid_balanced,\n",
    "    \"ood_gen\": ood_gen,\n",
    "    \"ood_gen_strict\": ood_gen_strict,\n",
    "    \"smoke_10p\": smoke_10p,\n",
    "}\n",
    "OUT_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "print(\"\\nâœ… saved:\", OUT_JSON)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
