{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed93105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Loaded CLIP SVM: clip_linear_svm_feature_20250819_011904.joblib\n",
      "Loaded PRNU CNN: prnu_cnn_i8_best_20250819_020555.pt\n",
      "Loaded ELA CNN: ela_fromnpy_cnn_best_20250819_085103.pt\n",
      "[val] 使用樣本數（三路齊全）：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 14000/14000 [00:00<00:00, 23670.83it/s]\n",
      "PRNU logits (TTA=0): 100%|██████████| 55/55 [00:56<00:00,  1.02s/it]\n",
      "ELA logits (TTA=0): 100%|██████████| 55/55 [01:51<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_iid] 使用樣本數（三路齊全）：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 14000/14000 [00:08<00:00, 1565.58it/s]\n",
      "PRNU logits (TTA=0): 100%|██████████| 55/55 [00:58<00:00,  1.06s/it]\n",
      "ELA logits (TTA=0): 100%|██████████| 55/55 [01:57<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_ood] 使用樣本數（三路齊全）：47570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 47570/47570 [00:13<00:00, 3520.00it/s]\n",
      "PRNU logits (TTA=8): 100%|██████████| 186/186 [12:38<00:00,  4.08s/it]\n",
      "ELA logits (TTA=8): 100%|██████████| 186/186 [35:36<00:00, 11.49s/it]\n",
      "CLIP load: 100%|██████████| 47570/47570 [00:14<00:00, 3295.08it/s]\n",
      "PRNU logits (TTA=8): 100%|██████████| 186/186 [12:55<00:00,  4.17s/it]\n",
      "ELA logits (TTA=8): 100%|██████████| 186/186 [35:44<00:00, 11.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved fusion model: /home/yaya/ai-detect-proj/Script/saved_models/fusion3_lr_20250819_130718.joblib\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 三路融合：CLIP SVM + PRNU CNN + ELA CNN  (Val學權重 → Test評估)\n",
    "# ======================================================\n",
    "import os, json, time, glob, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import joblib\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "SCRIPT_ROOT = \"/home/yaya/ai-detect-proj/Script\"\n",
    "SPLITS_JSON = f\"{SCRIPT_ROOT}/saved_models/splits_clip_feature_iid_ood.json\"\n",
    "SAVED_DIR   = f\"{SCRIPT_ROOT}/saved_models\"\n",
    "\n",
    "# 可調參：TTA 次數（0=關閉）\n",
    "TTA_PRNU = 0   # 建議先 0 或 8\n",
    "TTA_ELA  = 0   # 建議先 0 或 8\n",
    "INPUT_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", DEVICE)\n",
    "\n",
    "# ---------------- 讀 splits + 目錄資訊 ----------------\n",
    "with open(SPLITS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    J = json.load(f)\n",
    "SPL = J[\"splits\"]\n",
    "META_DIRS = (J.get(\"meta\", {}) or {}).get(\"dirs\", {})\n",
    "\n",
    "# 回傳各 split 的 clip 路徑與標籤（以 clip 為主索引）\n",
    "def clip_paths_labels(split_name):\n",
    "    real = SPL[split_name][\"clip\"][\"real\"]; fake = SPL[split_name][\"clip\"][\"fake\"]\n",
    "    P = real + fake\n",
    "    y = np.array([0]*len(real) + [1]*len(fake), dtype=int)\n",
    "    stems = [Path(p).stem.lower() for p in P]\n",
    "    return P, y, stems\n",
    "\n",
    "# 建立索引：從 stem 找到對應的 PRNU / ELA 檔案\n",
    "def index_dir(d, exts=(\".npy\",\".npz\")):\n",
    "    d = Path(d); idx = {}\n",
    "    if not d.exists(): return idx\n",
    "    for ext in exts:\n",
    "        for q in d.glob(f\"*{ext}\"):\n",
    "            idx[q.stem.lower()] = str(q)\n",
    "            idx[q.name.lower()] = str(q)\n",
    "    return idx\n",
    "\n",
    "# 取得 PRNU/ELA 目錄（從 meta 取，若無則給預設）\n",
    "PRNU_REAL_DIR = META_DIRS.get(\"prnu\", {}).get(\"real\", f\"{SCRIPT_ROOT}/features_i8/prnu_real_i8_npy\")\n",
    "PRNU_FAKE_DIR = META_DIRS.get(\"prnu\", {}).get(\"fake\", f\"{SCRIPT_ROOT}/features_i8/prnu_fake_i8_npy\")\n",
    "ELA_REAL_DIR  = META_DIRS.get(\"ela\",  {}).get(\"real\", f\"{SCRIPT_ROOT}/features_npy/ela_real_npy\")\n",
    "ELA_FAKE_DIR  = META_DIRS.get(\"ela\",  {}).get(\"fake\", f\"{SCRIPT_ROOT}/features_npy/ela_fake_npy\")\n",
    "\n",
    "IDX_PRNU_REAL = index_dir(PRNU_REAL_DIR, (\".npy\",))\n",
    "IDX_PRNU_FAKE = index_dir(PRNU_FAKE_DIR, (\".npy\",))\n",
    "IDX_ELA_REAL  = index_dir(ELA_REAL_DIR,  (\".npy\",\".npz\"))\n",
    "IDX_ELA_FAKE  = index_dir(ELA_FAKE_DIR,  (\".npy\",\".npz\"))\n",
    "\n",
    "def map_stems_to_paths(stems, is_real, kind):\n",
    "    if kind==\"prnu\":\n",
    "        idx = IDX_PRNU_REAL if is_real==0 else IDX_PRNU_FAKE\n",
    "    else:\n",
    "        idx = IDX_ELA_REAL if is_real==0 else IDX_ELA_FAKE\n",
    "    out = []\n",
    "    miss = 0\n",
    "    for s in stems:\n",
    "        q = idx.get(s) or idx.get(s + \".npy\") or idx.get(s + \".npz\")\n",
    "        if q is None:\n",
    "            miss += 1\n",
    "            out.append(None)\n",
    "        else:\n",
    "            out.append(q)\n",
    "    return out, miss\n",
    "\n",
    "# ---------------- 載入三路模型 ----------------\n",
    "# 1) CLIP SVM\n",
    "svm_paths = sorted(Path(SAVED_DIR).glob(\"clip_linear_svm_feature_*.joblib\"))\n",
    "assert svm_paths, \"找不到 clip_linear_svm_feature_*.joblib，請先訓練 CLIP SVM。\"\n",
    "clip_svm = joblib.load(svm_paths[-1])\n",
    "print(\"Loaded CLIP SVM:\", svm_paths[-1].name)\n",
    "\n",
    "# 2) PRNU CNN（網路結構需與訓練一致）\n",
    "class SmallForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__()\n",
    "        def blk(ci, co, groups=8):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.net = nn.Sequential(\n",
    "            blk(in_ch,32), blk(32,32), nn.AvgPool2d(2),\n",
    "            blk(32,64),   blk(64,64),  nn.AvgPool2d(2),\n",
    "            blk(64,128),  blk(128,128), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "prnu_ckpts = sorted(Path(SAVED_DIR).glob(\"prnu_cnn_i8_best_*.pt\"))\n",
    "assert prnu_ckpts, \"找不到 prnu_cnn_i8_best_*.pt，請先訓練 PRNU CNN。\"\n",
    "prnu_model = SmallForensicCNN(1).to(DEVICE).eval()\n",
    "prnu_model.load_state_dict(torch.load(prnu_ckpts[-1], map_location=DEVICE))\n",
    "print(\"Loaded PRNU CNN:\", prnu_ckpts[-1].name)\n",
    "\n",
    "# 3) ELA CNN\n",
    "class ELAForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        def bnblk(ci, co):\n",
    "            return nn.Sequential(nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                                 nn.BatchNorm2d(co), nn.ReLU(inplace=True))\n",
    "        def gnblk(ci, co, groups=8):\n",
    "            return nn.Sequential(nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                                 nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(\n",
    "            bnblk(in_ch,32), bnblk(32,32), nn.AvgPool2d(2),\n",
    "            bnblk(32,64),    bnblk(64,64), nn.AvgPool2d(2),\n",
    "            gnblk(64,128),   gnblk(128,128), nn.AvgPool2d(2),\n",
    "            gnblk(128,256),  gnblk(256,256), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "ela_ckpts = sorted(Path(SAVED_DIR).glob(\"ela_fromnpy_cnn_best_*.pt\"))\n",
    "assert ela_ckpts, \"找不到 ela_fromnpy_cnn_best_*.pt，請先訓練 ELA CNN。\"\n",
    "ela_model = ELAForensicCNN(3).to(DEVICE).eval()\n",
    "ela_model.load_state_dict(torch.load(ela_ckpts[-1], map_location=DEVICE))\n",
    "print(\"Loaded ELA CNN:\", ela_ckpts[-1].name)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- 三路分數計算（含前處理/TTA） ----------------\n",
    "def l2norm(v):\n",
    "    v = v.astype(np.float32).reshape(-1)\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return v / n\n",
    "\n",
    "def clip_scores(paths, batch=2048):\n",
    "    X = np.vstack([l2norm(np.load(p, allow_pickle=True)) for p in tqdm(paths, desc=\"CLIP load\")])\n",
    "    sc = clip_svm.decision_function(X).astype(np.float32)  # >0 => fake\n",
    "    return sc\n",
    "\n",
    "# PRNU helpers\n",
    "def avg_pool_2x(x):\n",
    "    H,W = x.shape\n",
    "    H2,W2 = (H//2)*2, (W//2)*2\n",
    "    x = x[:H2,:W2]  \n",
    "    return x.reshape(H2//2,2,W2//2,2).mean(axis=(1,3))\n",
    "\n",
    "def crop_center_or_rand(x, size=256, center=True, rng=None):\n",
    "    h,w = x.shape[:2]\n",
    "    if h<size or w<size:\n",
    "        ph,pw = max(0,size-h), max(0,size-w)\n",
    "        if x.ndim==2:\n",
    "            x = np.pad(x, ((ph//2, ph-ph//2),(pw//2, pw-pw//2)), mode='edge')\n",
    "        else:\n",
    "            x = np.pad(x, ((ph//2, ph-ph//2),(pw//2, pw-pw//2),(0,0)), mode='edge')\n",
    "        h,w = x.shape[:2]\n",
    "    if h==size and w==size: \n",
    "        return x.copy()\n",
    "    if center:\n",
    "        y0,x0 = (h-size)//2, (w-size)//2\n",
    "    else:\n",
    "        if rng is None: rng = np.random.default_rng()\n",
    "        y0 = int(rng.integers(0, h-size+1)); x0 = int(rng.integers(0, w-size+1))\n",
    "    return x[y0:y0+size, x0:x0+size].copy() if x.ndim==2 else x[y0:y0+size, x0:x0+size, :].copy()\n",
    "\n",
    "def prnu_per_image_norm(a_i8: np.ndarray):\n",
    "    x = a_i8.astype(np.float32, copy=False)\n",
    "    m,s = x.mean(), x.std()\n",
    "    if (not np.isfinite(s)) or s<1e-6:\n",
    "        m,s = 0.0, 20.0\n",
    "    return (x - m) / (s if s>0 else 1.0)\n",
    "\n",
    "def load_prnu_i8(path):\n",
    "    a = np.load(path, mmap_mode='r')\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 3 and (a.shape[0]==1 or a.shape[-1]==1):\n",
    "        a = a.squeeze()\n",
    "    assert a.ndim==2 and a.dtype==np.int8, f\"PRNU expect 2D int8, got {a.shape} {a.dtype}\"\n",
    "    return a\n",
    "\n",
    "@torch.no_grad()\n",
    "def prnu_logits(paths, tta=TTA_PRNU, batch=256):\n",
    "    prnu_model.eval()\n",
    "    rng = np.random.default_rng(1337)\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    for i in tqdm(range(0, len(paths), batch), desc=f\"PRNU logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+batch]\n",
    "        acc = np.zeros(len(chunk), np.float32)\n",
    "        reps = max(1, int(tta))  # 0->1\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                a = load_prnu_i8(p)\n",
    "                if a.shape[0] >= 512 and a.shape[1] >= 512:\n",
    "                    a = avg_pool_2x(a)\n",
    "                a = crop_center_or_rand(a, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                a = prnu_per_image_norm(a)\n",
    "                xs.append(torch.from_numpy(a).unsqueeze(0))  # [1,H,W]\n",
    "            xb = torch.stack(xs,0).to(DEVICE)               # [B,1,H,W]\n",
    "            logit = prnu_model(xb).float().cpu().numpy()\n",
    "            acc += logit.astype(np.float32)\n",
    "        out[i:i+batch] = acc/float(reps)\n",
    "    return out  # logits\n",
    "\n",
    "# ELA helpers\n",
    "def _npz_pick(z):\n",
    "    for k in ('ela','arr','arr_0','data'):\n",
    "        if isinstance(z, np.lib.npyio.NpzFile) and (k in z.files):\n",
    "            return z[k]\n",
    "    return z[z.files[0]] if isinstance(z, np.lib.npyio.NpzFile) else z\n",
    "\n",
    "def load_ela_array(path):\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    a = _npz_pick(z); a = np.asarray(a)\n",
    "    if a.ndim==2: a = np.repeat(a[...,None],3,axis=2)\n",
    "    elif a.ndim==3 and a.shape[0] in (1,3) and a.shape[-1] not in (1,3):\n",
    "        a = np.transpose(a,(1,2,0))\n",
    "    elif a.ndim==3 and a.shape[-1]==1:\n",
    "        a = np.repeat(a,3,axis=2)\n",
    "    assert a.ndim==3 and a.shape[-1]==3, f\"ELA expect HxWx3, got {a.shape}\"\n",
    "    a = a.astype(np.float32, copy=False)\n",
    "    if np.nanmax(a) > 1.5: a *= (1.0/255.0)\n",
    "    return a\n",
    "\n",
    "def ela_zscore(x):\n",
    "    m = x.mean(axis=(0,1), keepdims=True)\n",
    "    s = x.std(axis=(0,1), keepdims=True); s[s<1e-6] = 1.0\n",
    "    return (x - m) / s\n",
    "\n",
    "@torch.no_grad()\n",
    "def ela_logits(paths, tta=TTA_ELA, batch=256):\n",
    "    ela_model.eval()\n",
    "    rng = np.random.default_rng(1337)\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    for i in tqdm(range(0, len(paths), batch), desc=f\"ELA logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+batch]\n",
    "        acc = np.zeros(len(chunk), np.float32)\n",
    "        reps = max(1, int(tta))\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                x = load_ela_array(p)\n",
    "                x = crop_center_or_rand(x, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                x = ela_zscore(x)\n",
    "                xs.append(torch.from_numpy(np.transpose(x,(2,0,1))))  # [3,H,W]\n",
    "            xb = torch.stack(xs,0).to(DEVICE)\n",
    "            logit = ela_model(xb).float().cpu().numpy()\n",
    "            acc += logit.astype(np.float32)\n",
    "        out[i:i+batch] = acc/float(reps)\n",
    "    return out  # logits\n",
    "\n",
    "# === TENT: norm-only 自適應（BN / SyncBN / GroupNorm）===\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tent_enable_norm(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm, nn.GroupNorm)):\n",
    "            m.train()\n",
    "            for p in m.parameters(): p.requires_grad_(True)   # 只開 norm 層\n",
    "        else:\n",
    "            for p in getattr(m, 'parameters', lambda: [])():\n",
    "                p.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "def tent_adapt(model, unlabeled_loader, steps=1, lr=1e-4, device=DEVICE):\n",
    "    model = tent_enable_norm(model)\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "    model.train()\n",
    "    for _ in range(steps):\n",
    "        for xb,_ in unlabeled_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).clamp(1e-6, 1-1e-6)\n",
    "            ent = -(prob*torch.log(prob) + (1-prob)*torch.log(1-prob)).mean()  # 最小化熵\n",
    "            opt.zero_grad(); ent.backward(); opt.step()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------- 建立三路對齊資料（Val / Test-IID / Test-OOD） ----------------\n",
    "def aligned_triplet(split_name):\n",
    "    clipP, y, stems = clip_paths_labels(split_name)\n",
    "    # map stem -> prnu/ela path（依 label 選 real/fake 目錄）\n",
    "    prnu_paths = []\n",
    "    ela_paths  = []\n",
    "    miss_prnu = miss_ela = 0\n",
    "    for s, yi in zip(stems, y):\n",
    "        p_prnu = (IDX_PRNU_REAL.get(s) if yi==0 else IDX_PRNU_FAKE.get(s)) \\\n",
    "                 or (IDX_PRNU_REAL.get(s+\".npy\") if yi==0 else IDX_PRNU_FAKE.get(s+\".npy\"))\n",
    "        p_ela  = (IDX_ELA_REAL.get(s)  if yi==0 else IDX_ELA_FAKE.get(s)) \\\n",
    "                 or (IDX_ELA_REAL.get(s+\".npy\") if yi==0 else IDX_ELA_FAKE.get(s+\".npy\")) \\\n",
    "                 or (IDX_ELA_REAL.get(s+\".npz\") if yi==0 else IDX_ELA_FAKE.get(s+\".npz\"))\n",
    "        prnu_paths.append(p_prnu)\n",
    "        ela_paths.append(p_ela)\n",
    "        miss_prnu += int(p_prnu is None); miss_ela += int(p_ela is None)\n",
    "    if miss_prnu or miss_ela:\n",
    "        print(f\"[{split_name}] 對齊缺檔 → PRNU:{miss_prnu}  ELA:{miss_ela}（將跳過）\")\n",
    "\n",
    "    # 保留三路皆存在的索引\n",
    "    keep = [i for i,(p1,p2) in enumerate(zip(prnu_paths, ela_paths)) if (p1 is not None and p2 is not None)]\n",
    "    clipP = [clipP[i] for i in keep]\n",
    "    prnu_paths = [prnu_paths[i] for i in keep]\n",
    "    ela_paths  = [ela_paths[i] for i in keep]\n",
    "    y = y[keep]\n",
    "    print(f\"[{split_name}] 使用樣本數（三路齊全）：{len(keep)}\")\n",
    "    return clipP, prnu_paths, ela_paths, y\n",
    "\n",
    "def compute_triplet_scores(clipP, prnuP, elaP):\n",
    "    s_clip = clip_scores(clipP)\n",
    "    z_prnu = prnu_logits(prnuP, tta=TTA_PRNU)\n",
    "    z_ela  = ela_logits(elaP,  tta=TTA_ELA)\n",
    "    # 轉成特徵向量： [clip_decision, prnu_logit, ela_logit]\n",
    "    X = np.stack([s_clip, z_prnu, z_ela], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ---------------- 取各 split 的三路分數 ----------------\n",
    "X_va = None\n",
    "clip_va, prnu_va, ela_va, y_va = aligned_triplet(\"val\")\n",
    "if len(y_va): X_va = compute_triplet_scores(clip_va, prnu_va, ela_va)\n",
    "\n",
    "clip_ti, prnu_ti, ela_ti, y_ti = aligned_triplet(\"test_iid\")\n",
    "X_ti = compute_triplet_scores(clip_ti, prnu_ti, ela_ti) if len(y_ti) else None\n",
    "\n",
    "clip_to, prnu_to, ela_to, y_to = aligned_triplet(\"test_ood\")\n",
    "\n",
    "# === OOD 的 unlabeled loader（只用影像，不用標籤）===\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PRNUUnlab(Dataset):\n",
    "    def __init__(self, paths): self.paths = paths\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        a = load_prnu_i8(self.paths[i]).astype(np.float32)\n",
    "        if a.shape[0] >= 512 and a.shape[1] >= 512:\n",
    "            a = avg_pool_2x(a)\n",
    "        a = crop_center_or_rand(a, INPUT_SIZE, center=False)\n",
    "        a = prnu_per_image_norm(a)\n",
    "        return torch.from_numpy(a).unsqueeze(0), 0.0  # [1,H,W], dummy label\n",
    "\n",
    "class ELAUnlab(Dataset):\n",
    "    def __init__(self, paths): self.paths = paths\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        x = load_ela_array(self.paths[i])\n",
    "        x = crop_center_or_rand(x, INPUT_SIZE, center=False)\n",
    "        x = ela_zscore(x)\n",
    "        return torch.from_numpy(np.transpose(x,(2,0,1))), 0.0  # [3,H,W], dummy label\n",
    "\n",
    "bs = 128\n",
    "prnu_unlab = DataLoader(PRNUUnlab(prnu_to), batch_size=bs, shuffle=True, num_workers=0, drop_last=True)\n",
    "ela_unlab  = DataLoader(ELAUnlab(ela_to),  batch_size=bs, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "# === 在 OOD 上做一次 TENT 自適應（小步長、一次即可）===\n",
    "prnu_model = tent_adapt(prnu_model, prnu_unlab, steps=1, lr=1e-4)\n",
    "ela_model  = tent_adapt(ela_model,  ela_unlab,  steps=1, lr=1e-4)\n",
    "\n",
    "# === 開 TTA，重算 OOD 的 logits，再組合 X_to ===\n",
    "TTA_PRNU = 8\n",
    "TTA_ELA  = 8\n",
    "s_clip_to = clip_scores(clip_to)                 # 原本就有\n",
    "z_prnu_to = prnu_logits(prnu_to, tta=TTA_PRNU)   # 重新計算（已自適應）\n",
    "z_ela_to  = ela_logits(ela_to,   tta=TTA_ELA)\n",
    "X_to = np.stack([s_clip_to, z_prnu_to, z_ela_to], axis=1).astype(np.float32)\n",
    "\n",
    "\n",
    "X_to = compute_triplet_scores(clip_to, prnu_to, ela_to) if len(y_to) else None\n",
    "\n",
    "assert X_va is not None and len(X_va), \"Val split 在三路對齊後為空，請檢查資料。\"\n",
    "\n",
    "# ---------------- 訓練融合器（StandardScaler → LogisticRegression） ----------------\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_va)\n",
    "Xv = scaler.transform(X_va)\n",
    "\n",
    "base = HistGradientBoostingClassifier(\n",
    "    max_depth=3, learning_rate=0.1, max_iter=300,\n",
    "    l2_regularization=1e-3, min_samples_leaf=20, random_state=1337\n",
    ")\n",
    "# 先訓練、再用 Val 做保留折校準（讓輸出可當機率）\n",
    "fuser = CalibratedClassifierCV(base, method=\"isotonic\", cv=3)\n",
    "fuser.fit(Xv, y_va)\n",
    "\n",
    "def proba_from(X): return fuser.predict_proba(scaler.transform(X))[:,1]\n",
    "p_va = proba_from(X_va)\n",
    "p_ti = proba_from(X_ti) if X_ti is not None else None\n",
    "p_to = proba_from(X_to) if X_to is not None else None\n",
    "\n",
    "\n",
    "# ---------------- 保存融合模型 ----------------\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = f\"{SAVED_DIR}/fusion3_lr_{stamp}.joblib\"\n",
    "joblib.dump({\n",
    "    \"scaler\": scaler,\n",
    "    \"fuser\": fuser,\n",
    "    \"config\": {\n",
    "        \"tta_prnu\": TTA_PRNU, \"tta_ela\": TTA_ELA,\n",
    "        \"input_size\": INPUT_SIZE, \"device\": DEVICE\n",
    "    },\n",
    "    \"components\": {\n",
    "        \"clip_svm\": Path(svm_paths[-1]).name,\n",
    "        \"prnu_cnn\": Path(prnu_ckpts[-1]).name,\n",
    "        \"ela_cnn\":  Path(ela_ckpts[-1]).name,\n",
    "    },\n",
    "    \"threshold_val_youden\": th_star\n",
    "}, save_path)\n",
    "print(\"\\n✅ Saved fusion model:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956a1e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fusion: fusion3_lr_20250819_130718.joblib\n",
      "Val-Youden threshold = 0.6154414870978013\n",
      "device = cuda\n",
      "Loaded: prnu_cnn_i8_best_20250819_020555.pt | ela_fromnpy_cnn_best_20250819_130848.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n",
      "PRNU logits (TTA=0): 100%|██████████| 28/28 [05:18<00:00, 11.36s/it]\n",
      "ELA logits (TTA=0): 100%|██████████| 28/28 [04:13<00:00,  9.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAL] acc@thr=0.9644 | auc=0.9885 | n=14000\n",
      "[[6959   41]\n",
      " [ 458 6542]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9382    0.9941    0.9654      7000\n",
      "     fake(1)     0.9938    0.9346    0.9633      7000\n",
      "\n",
      "    accuracy                         0.9644     14000\n",
      "   macro avg     0.9660    0.9644    0.9643     14000\n",
      "weighted avg     0.9660    0.9644    0.9643     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      "PRNU logits (TTA=0): 100%|██████████| 28/28 [05:22<00:00, 11.53s/it]\n",
      "ELA logits (TTA=0): 100%|██████████| 28/28 [03:48<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST_IID] acc@thr=0.9642 | auc=0.9841 | n=14000\n",
      "[[6956   44]\n",
      " [ 457 6543]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9384    0.9937    0.9652      7000\n",
      "     fake(1)     0.9933    0.9347    0.9631      7000\n",
      "\n",
      "    accuracy                         0.9642     14000\n",
      "   macro avg     0.9658    0.9642    0.9642     14000\n",
      "weighted avg     0.9658    0.9642    0.9642     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 12/12 [00:18<00:00,  1.52s/it]\n",
      "PRNU logits (TTA=0):   5%|▌         | 5/93 [01:04<18:57, 12.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SPL: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    251\u001b[39m     clipP, prnuP, elaP, y = _get_split(split)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     X = \u001b[43mscores_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprnuP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melaP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     _ = eval_block(split.upper(), X, y, thr=thr_star)\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# ---------- Single-sample inference ----------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 232\u001b[39m, in \u001b[36mscores_for\u001b[39m\u001b[34m(clipP, prnuP, elaP)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscores_for\u001b[39m(clipP, prnuP, elaP):\n\u001b[32m    231\u001b[39m     s_clip = clip_scores(clipP)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     z_prnu = \u001b[43mprnu_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprnuP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTTA_PRNU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     z_ela  = ela_logits(elaP,  tta=TTA_ELA)\n\u001b[32m    234\u001b[39m     X = np.stack([s_clip, z_prnu, z_ela], \u001b[32m1\u001b[39m).astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/imgfeat/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mprnu_logits\u001b[39m\u001b[34m(paths, tta)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.float16, enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    175\u001b[39m             logit = prnu_model(xb)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         acc += \u001b[43mlogit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy().astype(np.float32)\n\u001b[32m    177\u001b[39m     out[i:i+BATCH_PRNU] = acc/\u001b[38;5;28mfloat\u001b[39m(reps)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Test fusion model on splits (val / test_iid / test_ood) + single sample\n",
    "# ======================================================\n",
    "import os, json, glob, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "\n",
    "# ---------- Paths ----------\n",
    "SCRIPT_ROOT = \"/home/yaya/ai-detect-proj/Script\"\n",
    "SAVED_DIR   = f\"{SCRIPT_ROOT}/saved_models\"\n",
    "SPLITS_JSON = f\"{SAVED_DIR}/splits_clip_feature_iid_ood.json\"\n",
    "\n",
    "# 自動抓最後一個融合模型\n",
    "FUSION_PATH = sorted(Path(SAVED_DIR).glob(\"fusion3_lr_*.joblib\"))[-1]\n",
    "print(\"Using fusion:\", FUSION_PATH.name)\n",
    "\n",
    "# ---------- Load fusion (scaler + fuser + thr) ----------\n",
    "F = joblib.load(FUSION_PATH)\n",
    "scaler = F[\"scaler\"]\n",
    "fuser  = F[\"fuser\"]\n",
    "thr_star = float(F.get(\"threshold_val_youden\", 0.5))\n",
    "print(\"Val-Youden threshold =\", thr_star)\n",
    "\n",
    "# ---------- Backbones (同你訓練時結構) ----------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", DEVICE)\n",
    "\n",
    "# PRNU CNN（GroupNorm 小網）\n",
    "class SmallForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__()\n",
    "        def blk(ci, co, groups=8):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.net = nn.Sequential(\n",
    "            blk(in_ch,32), blk(32,32), nn.AvgPool2d(2),\n",
    "            blk(32,64),   blk(64,64),  nn.AvgPool2d(2),\n",
    "            blk(64,128),  blk(128,128), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "# ELA CNN（BN → GN）\n",
    "class ELAForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        def bnblk(ci, co):\n",
    "            return nn.Sequential(nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                                 nn.BatchNorm2d(co), nn.ReLU(inplace=True))\n",
    "        def gnblk(ci, co, groups=8):\n",
    "            return nn.Sequential(nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                                 nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                                 nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(\n",
    "            bnblk(in_ch,32), bnblk(32,32), nn.AvgPool2d(2),\n",
    "            bnblk(32,64),    bnblk(64,64), nn.AvgPool2d(2),\n",
    "            gnblk(64,128),   gnblk(128,128), nn.AvgPool2d(2),\n",
    "            gnblk(128,256),  gnblk(256,256), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "# 載入你最近的 PRNU/ELA 權重 + CLIP SVM\n",
    "prnu_ckpt = sorted(Path(SAVED_DIR).glob(\"prnu_cnn_i8_best_*.pt\"))[-1]\n",
    "ela_ckpt  = sorted(Path(SAVED_DIR).glob(\"ela_fromnpy_cnn_best_*.pt\"))[-1]\n",
    "clip_svm  = joblib.load(sorted(Path(SAVED_DIR).glob(\"clip_linear_svm_feature_*.joblib\"))[-1])\n",
    "\n",
    "prnu_model = SmallForensicCNN(1).to(DEVICE).eval()\n",
    "ela_model  = ELAForensicCNN(3).to(DEVICE).eval()\n",
    "prnu_model.load_state_dict(torch.load(prnu_ckpt, map_location=DEVICE))\n",
    "ela_model.load_state_dict(torch.load(ela_ckpt,  map_location=DEVICE))\n",
    "print(\"Loaded:\", prnu_ckpt.name, \"|\", ela_ckpt.name)\n",
    "\n",
    "# ---------- IO helpers ----------\n",
    "def l2norm(v): \n",
    "    v = v.astype(np.float32).reshape(-1)\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return v / n\n",
    "\n",
    "def load_prnu_i8(path):\n",
    "    a = np.load(path, mmap_mode='r')\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==3 and (a.shape[0]==1 or a.shape[-1]==1): a = a.squeeze()\n",
    "    assert a.ndim==2 and a.dtype==np.int8, f\"PRNU expect 2D int8, got {a.shape} {a.dtype}\"\n",
    "    return a\n",
    "\n",
    "def avg_pool_2x(x):\n",
    "    H,W = x.shape; H2,W2=(H//2)*2,(W//2)*2\n",
    "    x = x[:H2,:W2]; return x.reshape(H2//2,2,W2//2,2).mean(axis=(1,3))\n",
    "\n",
    "def prnu_per_image_norm(a_i8):\n",
    "    x = a_i8.astype(np.float32, copy=False)\n",
    "    m,s = x.mean(), x.std()\n",
    "    if (not np.isfinite(s)) or s<1e-6: m,s = 0.0,20.0\n",
    "    return (x-m)/(s if s>0 else 1.0)\n",
    "\n",
    "def load_ela_array(path):\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    if isinstance(z, np.lib.npyio.NpzFile):\n",
    "        for k in ('ela','arr','arr_0','data'):\n",
    "            if k in z.files: a = z[k]; break\n",
    "        else: a = z[z.files[0]]\n",
    "    else:\n",
    "        a = z\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==2: a = np.repeat(a[...,None],3,axis=2)\n",
    "    elif a.ndim==3 and a.shape[0] in (1,3) and a.shape[-1] not in (1,3):\n",
    "        a = np.transpose(a,(1,2,0))\n",
    "    elif a.ndim==3 and a.shape[-1]==1:\n",
    "        a = np.repeat(a,3,axis=2)\n",
    "    a = a.astype(np.float32, copy=False)\n",
    "    if np.nanmax(a)>1.5: a *= (1.0/255.0)\n",
    "    return a\n",
    "\n",
    "def ela_zscore(x):\n",
    "    m = x.mean(axis=(0,1), keepdims=True)\n",
    "    s = x.std(axis=(0,1), keepdims=True); s[s<1e-6]=1.0\n",
    "    return (x-m)/s\n",
    "\n",
    "def crop_center_or_rand(x, size=256, center=True, rng=None):\n",
    "    h,w = x.shape[:2]\n",
    "    if h<size or w<size:\n",
    "        ph,pw=max(0,size-h),max(0,size-w)\n",
    "        if x.ndim==2: x=np.pad(x,((ph//2, ph-ph//2),(pw//2, pw-pw//2)),mode='edge')\n",
    "        else: x=np.pad(x,((ph//2, ph-ph//2),(pw//2, pw-pw//2),(0,0)),mode='edge')\n",
    "        h,w=x.shape[:2]\n",
    "    if h==size and w==size: return x.copy()\n",
    "    if center:\n",
    "        y0,x0=(h-size)//2,(w-size)//2\n",
    "    else:\n",
    "        if rng is None: rng=np.random.default_rng()\n",
    "        y0=int(rng.integers(0, h-size+1)); x0=int(rng.integers(0, w-size+1))\n",
    "    return x[y0:y0+size, x0:x0+size].copy() if x.ndim==2 else x[y0:y0+size, x0:x0+size,:].copy()\n",
    "\n",
    "# ---------- Score extractors (可調 batch / TTA / AMP) ----------\n",
    "INPUT_SIZE=256\n",
    "BATCH_CLIP=4096\n",
    "BATCH_PRNU=512\n",
    "BATCH_ELA =512\n",
    "TTA_PRNU=0\n",
    "TTA_ELA =0\n",
    "\n",
    "def clip_scores(paths):\n",
    "    out=[]\n",
    "    for i in tqdm(range(0,len(paths),BATCH_CLIP), desc=\"CLIP load\"):\n",
    "        X = np.vstack([l2norm(np.load(p, allow_pickle=True)) for p in paths[i:i+BATCH_CLIP]]).astype(np.float32)\n",
    "        out.append(clip_svm.decision_function(X).astype(np.float32))\n",
    "    return np.concatenate(out,0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def prnu_logits(paths, tta=TTA_PRNU):\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    rng = np.random.default_rng(1337)\n",
    "    for i in tqdm(range(0,len(paths),BATCH_PRNU), desc=f\"PRNU logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+BATCH_PRNU]\n",
    "        acc = np.zeros(len(chunk), np.float32); reps=max(1,int(tta))\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                a = load_prnu_i8(p)\n",
    "                if a.shape[0]>=512 and a.shape[1]>=512: a = avg_pool_2x(a)\n",
    "                a = crop_center_or_rand(a, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                a = prnu_per_image_norm(a)\n",
    "                xs.append(torch.from_numpy(a).unsqueeze(0))\n",
    "            xb = torch.stack(xs,0).to(DEVICE).contiguous(memory_format=torch.channels_last)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                logit = prnu_model(xb)\n",
    "            acc += logit.float().cpu().numpy().astype(np.float32)\n",
    "        out[i:i+BATCH_PRNU] = acc/float(reps)\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def ela_logits(paths, tta=TTA_ELA):\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    rng = np.random.default_rng(1337)\n",
    "    for i in tqdm(range(0,len(paths),BATCH_ELA), desc=f\"ELA logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+BATCH_ELA]\n",
    "        acc = np.zeros(len(chunk), np.float32); reps=max(1,int(tta))\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                x = load_ela_array(p)\n",
    "                x = crop_center_or_rand(x, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                x = ela_zscore(x)\n",
    "                xs.append(torch.from_numpy(np.transpose(x,(2,0,1))))\n",
    "            xb = torch.stack(xs,0).to(DEVICE).contiguous(memory_format=torch.channels_last)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                logit = ela_model(xb)\n",
    "            acc += logit.float().cpu().numpy().astype(np.float32)\n",
    "        out[i:i+BATCH_ELA] = acc/float(reps)\n",
    "    return out\n",
    "\n",
    "# ---------- Read splits & build aligned triplets ----------\n",
    "with open(SPLITS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    J = json.load(f)\n",
    "SPL = J[\"splits\"]\n",
    "\n",
    "def _get_split(name):\n",
    "    real = SPL[name][\"clip\"][\"real\"]; fake = SPL[name][\"clip\"][\"fake\"]\n",
    "    clipP = real + fake\n",
    "    y = np.array([0]*len(real) + [1]*len(fake), dtype=int)\n",
    "    # 以目錄快速推測 PRNU/ELA 路徑（與你訓練一致）\n",
    "    prnu_real = (J[\"meta\"][\"dirs\"][\"prnu\"][\"real\"])\n",
    "    prnu_fake = (J[\"meta\"][\"dirs\"][\"prnu\"][\"fake\"])\n",
    "    ela_real  = (J[\"meta\"][\"dirs\"][\"ela\"][\"real\"])\n",
    "    ela_fake  = (J[\"meta\"][\"dirs\"][\"ela\"][\"fake\"])\n",
    "    prnuP=[]; elaP=[]\n",
    "    miss_prnu=miss_ela=0\n",
    "    for p,yi in zip(clipP, y):\n",
    "        stem = Path(p).stem\n",
    "        pr = Path(prnu_real if yi==0 else prnu_fake)/f\"{stem}.npy\"\n",
    "        eln= Path(ela_real  if yi==0 else ela_fake )/f\"{stem}.npy\"\n",
    "        elz= Path(ela_real  if yi==0 else ela_fake )/f\"{stem}.npz\"\n",
    "        prnuP.append(str(pr) if pr.is_file() else None);  miss_prnu += int(not pr.is_file())\n",
    "        elaP.append( str(eln) if eln.is_file() else (str(elz) if elz.is_file() else None) )\n",
    "        miss_ela  += int(not (eln.is_file() or elz.is_file()))\n",
    "    keep=[i for i,(a,b) in enumerate(zip(prnuP, elaP)) if (a and b)]\n",
    "    if miss_prnu or miss_ela: print(f\"[{name}] 缺檔 PRNU:{miss_prnu}  ELA:{miss_ela} → 使用齊全 {len(keep)}\")\n",
    "    clipP=[clipP[i] for i in keep]; prnuP=[prnuP[i] for i in keep]; elaP=[elaP[i] for i in keep]; y=y[keep]\n",
    "    return clipP, prnuP, elaP, y\n",
    "\n",
    "def scores_for(clipP, prnuP, elaP):\n",
    "    s_clip = clip_scores(clipP)\n",
    "    z_prnu = prnu_logits(prnuP, tta=TTA_PRNU)\n",
    "    z_ela  = ela_logits(elaP,  tta=TTA_ELA)\n",
    "    X = np.stack([s_clip, z_prnu, z_ela], 1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "def eval_block(name, X, y, thr=thr_star):\n",
    "    Xs = scaler.transform(X)\n",
    "    p  = fuser.predict_proba(Xs)[:,1]\n",
    "    pred = (p>=thr).astype(int)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    auc = roc_auc_score(y, p)\n",
    "    print(f\"\\n[{name}] acc@thr={acc:.4f} | auc={auc:.4f} | n={len(y)}\")\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred, target_names=[\"real(0)\",\"fake(1)\"], digits=4))\n",
    "    return p\n",
    "\n",
    "# ---------- Run on splits ----------\n",
    "for split in [\"val\",\"test_iid\",\"test_ood\"]:\n",
    "    if split not in SPL: continue\n",
    "    clipP, prnuP, elaP, y = _get_split(split)\n",
    "    X = scores_for(clipP, prnuP, elaP)\n",
    "    _ = eval_block(split.upper(), X, y, thr=thr_star)\n",
    "\n",
    "# ---------- Single-sample inference ----------\n",
    "def predict_one_by_stem(stem, is_real_unknown=True):\n",
    "    # 從三個目錄拼出三路檔案\n",
    "    prnu_real = (J[\"meta\"][\"dirs\"][\"prnu\"][\"real\"]); prnu_fake = (J[\"meta\"][\"dirs\"][\"prnu\"][\"fake\"])\n",
    "    ela_real  = (J[\"meta\"][\"dirs\"][\"ela\"][\"real\"]);  ela_fake  = (J[\"meta\"][\"dirs\"][\"ela\"][\"fake\"])\n",
    "    # 先嘗試 real 目錄，找不到再用 fake（若你已知真假可直接指定）\n",
    "    pr = Path(prnu_real)/f\"{stem}.npy\"; \n",
    "    if not pr.is_file(): pr = Path(prnu_fake)/f\"{stem}.npy\"\n",
    "    el = Path(ela_real)/f\"{stem}.npy\"; \n",
    "    if not el.is_file(): el = Path(ela_real)/f\"{stem}.npz\"\n",
    "    if not el.exists():   el = Path(ela_fake)/f\"{stem}.npy\"\n",
    "    if not el.exists():   el = Path(ela_fake)/f\"{stem}.npz\"\n",
    "    cp = None\n",
    "    # 在 clip 兩個資料夾找向量\n",
    "    for cdir in [f\"{SCRIPT_ROOT}/features_npy/clip_real_npy\", f\"{SCRIPT_ROOT}/features_npy/clip_fake_npy\"]:\n",
    "        cand = Path(cdir)/f\"{stem}.npy\"\n",
    "        if cand.is_file(): cp = cand; break\n",
    "    assert cp and pr.is_file() and el.exists(), f\"找不到三路檔案：{stem}\"\n",
    "    # 算三路分數 → fusion\n",
    "    s_clip = clip_scores([str(cp)])\n",
    "    z_prnu = prnu_logits([str(pr)], tta=0)\n",
    "    z_ela  = ela_logits ([str(el)], tta=0)\n",
    "    X = np.stack([s_clip, z_prnu, z_ela], 1).astype(np.float32)\n",
    "    p = fuser.predict_proba(scaler.transform(X))[:,1][0]\n",
    "    yhat = int(p>=thr_star)\n",
    "    return float(p), yhat  # prob_fake, 1=fake\n",
    "\n",
    "# 範例：prob, pred = predict_one_by_stem(\"some_image_stem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4495a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "\n",
      "===== Evaluate: CLIP =====\n",
      "\n",
      "[CLIP] val 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 4/4 [00:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP|Val] thresholds → youden=0.081 | fpr@5%=-0.225\n",
      "[CLIP|val] acc@thr=0.9611 | auc=0.9842 | thr=0.081\n",
      "[[6880  120]\n",
      " [ 424 6576]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9419    0.9829    0.9620      7000\n",
      "     fake(1)     0.9821    0.9394    0.9603      7000\n",
      "\n",
      "    accuracy                         0.9611     14000\n",
      "   macro avg     0.9620    0.9611    0.9611     14000\n",
      "weighted avg     0.9620    0.9611    0.9611     14000\n",
      "\n",
      "\n",
      "[CLIP] test_iid 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 4/4 [00:00<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP|test_iid] acc@thr=0.9624 | auc=0.9834 | thr=0.081\n",
      "[[6913   87]\n",
      " [ 440 6560]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9402    0.9876    0.9633      7000\n",
      "     fake(1)     0.9869    0.9371    0.9614      7000\n",
      "\n",
      "    accuracy                         0.9624     14000\n",
      "   macro avg     0.9635    0.9624    0.9623     14000\n",
      "weighted avg     0.9635    0.9624    0.9623     14000\n",
      "\n",
      "\n",
      "[CLIP] test_ood 使用樣本數：47570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 12/12 [00:01<00:00,  6.35it/s]\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP|test_ood] acc@thr=0.6222 | auc=0.7130 | thr=0.081\n",
      "[[ 8210 15575]\n",
      " [ 2398 21387]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.7739    0.3452    0.4774     23785\n",
      "     fake(1)     0.5786    0.8992    0.7041     23785\n",
      "\n",
      "    accuracy                         0.6222     47570\n",
      "   macro avg     0.6763    0.6222    0.5908     47570\n",
      "weighted avg     0.6763    0.6222    0.5908     47570\n",
      "\n",
      "\n",
      "== OOD per-dataset (CLIP) | thr=Val-Youden ==\n",
      "- fake:midjourney n=17273 | acc=0.8824 | r_real=0.0000 r_fake=0.8824 | auc=nan\n",
      "- real:places365 n=15000 | acc=0.1260 | r_real=0.1260 r_fake=0.0000 | auc=nan\n",
      "- real:coco2017 n= 8785 | acc=0.7194 | r_real=0.7194 r_fake=0.0000 | auc=nan\n",
      "- fake:dalle3  n= 6512 | acc=0.9438 | r_real=0.0000 r_fake=0.9438 | auc=nan\n",
      "\n",
      ">>> 最差 Top-8（依 acc）\n",
      "  real:places365 n=15000 | acc=0.1260 | r_real=0.1260 r_fake=0.0000 | auc=nan\n",
      "  real:coco2017 n= 8785 | acc=0.7194 | r_real=0.7194 r_fake=0.0000 | auc=nan\n",
      "  fake:midjourney n=17273 | acc=0.8824 | r_real=0.0000 r_fake=0.8824 | auc=nan\n",
      "  fake:dalle3  n= 6512 | acc=0.9438 | r_real=0.0000 r_fake=0.9438 | auc=nan\n",
      "\n",
      "===== Evaluate: PRNU =====\n",
      "\n",
      "[PRNU] val 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRNU logits (TTA=0): 100%|██████████| 28/28 [05:43<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRNU|Val] thresholds → youden=-0.189 | fpr@5%=1.226\n",
      "[PRNU|val] acc@thr=0.8759 | auc=0.9321 | thr=-0.189\n",
      "[[6156  844]\n",
      " [ 894 6106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.8732    0.8794    0.8763      7000\n",
      "     fake(1)     0.8786    0.8723    0.8754      7000\n",
      "\n",
      "    accuracy                         0.8759     14000\n",
      "   macro avg     0.8759    0.8759    0.8759     14000\n",
      "weighted avg     0.8759    0.8759    0.8759     14000\n",
      "\n",
      "\n",
      "[PRNU] test_iid 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRNU logits (TTA=0): 100%|██████████| 28/28 [05:05<00:00, 10.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRNU|test_iid] acc@thr=0.8685 | auc=0.9286 | thr=-0.189\n",
      "[[6086  914]\n",
      " [ 927 6073]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.8678    0.8694    0.8686      7000\n",
      "     fake(1)     0.8692    0.8676    0.8684      7000\n",
      "\n",
      "    accuracy                         0.8685     14000\n",
      "   macro avg     0.8685    0.8685    0.8685     14000\n",
      "weighted avg     0.8685    0.8685    0.8685     14000\n",
      "\n",
      "\n",
      "[PRNU] test_ood 使用樣本數：47570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRNU logits (TTA=0): 100%|██████████| 93/93 [15:26<00:00,  9.96s/it]\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRNU|test_ood] acc@thr=0.6100 | auc=0.6544 | thr=-0.189\n",
      "[[ 9508 14277]\n",
      " [ 4277 19508]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.6897    0.3997    0.5061     23785\n",
      "     fake(1)     0.5774    0.8202    0.6777     23785\n",
      "\n",
      "    accuracy                         0.6100     47570\n",
      "   macro avg     0.6336    0.6100    0.5919     47570\n",
      "weighted avg     0.6336    0.6100    0.5919     47570\n",
      "\n",
      "\n",
      "== OOD per-dataset (PRNU) | thr=Val-Youden ==\n",
      "- fake:midjourney n=17273 | acc=0.8487 | r_real=0.0000 r_fake=0.8487 | auc=nan\n",
      "- real:places365 n=15000 | acc=0.1281 | r_real=0.1281 r_fake=0.0000 | auc=nan\n",
      "- real:coco2017 n= 8785 | acc=0.8635 | r_real=0.8635 r_fake=0.0000 | auc=nan\n",
      "- fake:dalle3  n= 6512 | acc=0.7446 | r_real=0.0000 r_fake=0.7446 | auc=nan\n",
      "\n",
      ">>> 最差 Top-8（依 acc）\n",
      "  real:places365 n=15000 | acc=0.1281 | r_real=0.1281 r_fake=0.0000 | auc=nan\n",
      "  fake:dalle3  n= 6512 | acc=0.7446 | r_real=0.0000 r_fake=0.7446 | auc=nan\n",
      "  fake:midjourney n=17273 | acc=0.8487 | r_real=0.0000 r_fake=0.8487 | auc=nan\n",
      "  real:coco2017 n= 8785 | acc=0.8635 | r_real=0.8635 r_fake=0.0000 | auc=nan\n",
      "\n",
      "===== Evaluate: ELA =====\n",
      "\n",
      "[ELA] val 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELA logits (TTA=0): 100%|██████████| 28/28 [03:47<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ELA|Val] thresholds → youden=0.163 | fpr@5%=-0.137\n",
      "[ELA|val] acc@thr=0.9309 | auc=0.9679 | thr=0.163\n",
      "[[6705  295]\n",
      " [ 672 6328]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9089    0.9579    0.9327      7000\n",
      "     fake(1)     0.9555    0.9040    0.9290      7000\n",
      "\n",
      "    accuracy                         0.9309     14000\n",
      "   macro avg     0.9322    0.9309    0.9309     14000\n",
      "weighted avg     0.9322    0.9309    0.9309     14000\n",
      "\n",
      "\n",
      "[ELA] test_iid 使用樣本數：14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELA logits (TTA=0): 100%|██████████| 28/28 [03:44<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ELA|test_iid] acc@thr=0.9341 | auc=0.9686 | thr=0.163\n",
      "[[6727  273]\n",
      " [ 650 6350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.9119    0.9610    0.9358      7000\n",
      "     fake(1)     0.9588    0.9071    0.9322      7000\n",
      "\n",
      "    accuracy                         0.9341     14000\n",
      "   macro avg     0.9353    0.9341    0.9340     14000\n",
      "weighted avg     0.9353    0.9341    0.9340     14000\n",
      "\n",
      "\n",
      "[ELA] test_ood 使用樣本數：47570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELA logits (TTA=0): 100%|██████████| 93/93 [14:14<00:00,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ELA|test_ood] acc@thr=0.5665 | auc=0.6163 | thr=0.163\n",
      "[[ 7665 16120]\n",
      " [ 4503 19282]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     real(0)     0.6299    0.3223    0.4264     23785\n",
      "     fake(1)     0.5447    0.8107    0.6516     23785\n",
      "\n",
      "    accuracy                         0.5665     47570\n",
      "   macro avg     0.5873    0.5665    0.5390     47570\n",
      "weighted avg     0.5873    0.5665    0.5390     47570\n",
      "\n",
      "\n",
      "== OOD per-dataset (ELA) | thr=Val-Youden ==\n",
      "- fake:midjourney n=17273 | acc=0.9907 | r_real=0.0000 r_fake=0.9907 | auc=nan\n",
      "- real:places365 n=15000 | acc=0.0268 | r_real=0.0268 r_fake=0.0000 | auc=nan\n",
      "- real:coco2017 n= 8785 | acc=0.8268 | r_real=0.8268 r_fake=0.0000 | auc=nan\n",
      "- fake:dalle3  n= 6512 | acc=0.3332 | r_real=0.0000 r_fake=0.3332 | auc=nan\n",
      "\n",
      ">>> 最差 Top-8（依 acc）\n",
      "  real:places365 n=15000 | acc=0.0268 | r_real=0.0268 r_fake=0.0000 | auc=nan\n",
      "  fake:dalle3  n= 6512 | acc=0.3332 | r_real=0.0000 r_fake=0.3332 | auc=nan\n",
      "  real:coco2017 n= 8785 | acc=0.8268 | r_real=0.8268 r_fake=0.0000 | auc=nan\n",
      "  fake:midjourney n=17273 | acc=0.9907 | r_real=0.0000 r_fake=0.9907 | auc=nan\n",
      "\n",
      "完成。你可以從各路的 '最差 Top-8' 直接看到 OOD 是哪個資料集在拖分。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 通用特徵測試：CLIP / PRNU / ELA 逐路評估 + OOD 逐資料集拆解\n",
    "# ============================================================\n",
    "import os, json, glob, math, time, re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, classification_report,\n",
    "                             confusion_matrix, roc_curve, precision_recall_fscore_support)\n",
    "\n",
    "# ---------------- 基本設定 ----------------\n",
    "SCRIPT_ROOT = \"/home/yaya/ai-detect-proj/Script\"\n",
    "SAVED_DIR   = f\"{SCRIPT_ROOT}/saved_models\"\n",
    "SPLITS_JSON = f\"{SAVED_DIR}/splits_clip_feature_iid_ood.json\"\n",
    "\n",
    "# 執行設定：可依硬體與需求調整\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP         = True                 # 推論時使用 AMP\n",
    "BATCH_CLIP  = 4096\n",
    "BATCH_PRNU  = 512\n",
    "BATCH_ELA   = 512\n",
    "INPUT_SIZE  = 256\n",
    "TTA_PRNU    = 0                    # 先設 0（測速）；要更穩可拉到 4~8\n",
    "TTA_ELA     = 0\n",
    "\n",
    "print(\"device =\", DEVICE)\n",
    "\n",
    "# ---------------- 來源前綴解析（與你 split 腳本一致） ----------------\n",
    "SEPS = (\"__\", \"---\", \"--\", \"_\", \"-\", \" \")\n",
    "ALIASES = {\n",
    "    # real\n",
    "    \"imagenet1k\":\"imagenet\", \"imgnet\":\"imagenet\", \"imagenet\":\"imagenet\",\n",
    "    \"unslpash\":\"unsplash\", \"unsplash\":\"unsplash\",\n",
    "    \"flicker30k\":\"flickr30k\", \"flicker30K\":\"flickr30k\", \"flickr30k\":\"flickr30k\",\n",
    "    \"places365\":\"places365\", \"coco2017\":\"coco2017\", \"div2k\":\"div2k\",\n",
    "    # fake\n",
    "    \"sd3\":\"sd3\", \"sdxl\":\"sd3\",\n",
    "    \"flux\":\"flux\", \"black-forest-labs\":\"flux\",\n",
    "    \"dalle3\":\"dalle3\", \"dalle-3\":\"dalle3\",\n",
    "    \"midjourney-v6-llava\":\"midjourney\", \"midjourney\":\"midjourney\"\n",
    "}\n",
    "def canonical(tag:str)->str:\n",
    "    return ALIASES.get(tag.lower().strip(), tag.lower().strip())\n",
    "\n",
    "def infer_tag_from_stem(stem:str, is_real:bool)->str:\n",
    "    for k in sorted(ALIASES.keys(), key=lambda s: -len(s)):\n",
    "        if stem.lower().startswith(k):\n",
    "            return canonical(k)\n",
    "    cut = None\n",
    "    for s in SEPS:\n",
    "        i = stem.find(s)\n",
    "        if i != -1:\n",
    "            cut = i if cut is None else min(cut, i)\n",
    "    tag = stem[:cut] if cut is not None else stem\n",
    "    if (not tag) or tag.isdigit():\n",
    "        tag = \"imagenet\" if is_real else \"unknown\"\n",
    "    return canonical(tag)\n",
    "\n",
    "# ---------------- 讀 splits ----------------\n",
    "assert os.path.isfile(SPLITS_JSON), f\"找不到 splits json：{SPLITS_JSON}\"\n",
    "with open(SPLITS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    J = json.load(f)\n",
    "\n",
    "META = J.get(\"meta\", {})\n",
    "DIRS = META.get(\"dirs\", {})\n",
    "CLIP_REAL_DIR = DIRS.get(\"clip\", {}).get(\"real\", f\"{SCRIPT_ROOT}/features_npy/clip_real_npy\")\n",
    "CLIP_FAKE_DIR = DIRS.get(\"clip\", {}).get(\"fake\", f\"{SCRIPT_ROOT}/features_npy/clip_fake_npy\")\n",
    "PRNU_REAL_DIR = DIRS.get(\"prnu\", {}).get(\"real\", f\"{SCRIPT_ROOT}/features_i8/prnu_real_i8_npy\")\n",
    "PRNU_FAKE_DIR = DIRS.get(\"prnu\", {}).get(\"fake\", f\"{SCRIPT_ROOT}/features_i8/prnu_fake_i8_npy\")\n",
    "ELA_REAL_DIR  = DIRS.get(\"ela\",  {}).get(\"real\", f\"{SCRIPT_ROOT}/features_npy/ela_real_npy\")\n",
    "ELA_FAKE_DIR  = DIRS.get(\"ela\",  {}).get(\"fake\", f\"{SCRIPT_ROOT}/features_npy/ela_fake_npy\")\n",
    "\n",
    "SPL = J[\"splits\"]\n",
    "\n",
    "def is_new_split_format(s):\n",
    "    return isinstance(s, dict) and \"clip\" in s\n",
    "\n",
    "def get_clip_lists(name):\n",
    "    # 回傳 clip 路徑（real+fake）與標籤、stem\n",
    "    S = SPL[name]\n",
    "    if is_new_split_format(S):\n",
    "        real = S[\"clip\"][\"real\"]; fake = S[\"clip\"][\"fake\"]\n",
    "    else:\n",
    "        # 老格式：直接是一串檔案路徑\n",
    "        allp = S\n",
    "        real = [p for p in allp if \"/clip_real_npy/\" in Path(p).as_posix()]\n",
    "        fake = [p for p in allp if \"/clip_fake_npy/\" in Path(p).as_posix()]\n",
    "    clip_paths = real + fake\n",
    "    y = np.array([0]*len(real) + [1]*len(fake), dtype=int)\n",
    "    stems = [Path(p).stem for p in clip_paths]\n",
    "    return clip_paths, y, stems\n",
    "\n",
    "def map_by_stem(stems, y, real_dir, fake_dir, suff=(\".npy\",)):\n",
    "    # 依 stem 在 real/fake 目錄組路徑；找不到給 None\n",
    "    out=[]; miss=0\n",
    "    for st, yi in zip(stems, y):\n",
    "        base = Path(real_dir if yi==0 else fake_dir)\n",
    "        found=None\n",
    "        for sx in suff:\n",
    "            cand = base/f\"{st}{sx}\"\n",
    "            if cand.is_file():\n",
    "                found=str(cand); break\n",
    "        if found is None:\n",
    "            miss+=1\n",
    "        out.append(found)\n",
    "    return out, miss\n",
    "\n",
    "# ---------------- 載入/建置各路模型與讀檔 ----------------\n",
    "def l2norm(v):\n",
    "    v = np.asarray(v, dtype=np.float32).reshape(-1)\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return v / n\n",
    "\n",
    "# CLIP：用你已訓練的 LinearSVC\n",
    "CLIP_SVM = joblib.load(sorted(Path(SAVED_DIR).glob(\"clip_linear_svm_feature_*.joblib\"))[-1])\n",
    "def clip_scores(paths):\n",
    "    # 每個 .npy 是一個向量；決策函數越大越像「fake」\n",
    "    out=[]\n",
    "    for i in tqdm(range(0,len(paths),BATCH_CLIP), desc=\"CLIP load\"):\n",
    "        chunk = paths[i:i+BATCH_CLIP]\n",
    "        X = np.vstack([l2norm(np.load(p, allow_pickle=True)) for p in chunk]).astype(np.float32)\n",
    "        out.append(CLIP_SVM.decision_function(X).astype(np.float32))\n",
    "    return np.concatenate(out, 0)\n",
    "\n",
    "# PRNU CNN（你的小網）\n",
    "class SmallForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__()\n",
    "        def blk(ci, co, groups=8):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.net = nn.Sequential(\n",
    "            blk(in_ch,32), blk(32,32), nn.AvgPool2d(2),\n",
    "            blk(32,64),   blk(64,64),  nn.AvgPool2d(2),\n",
    "            blk(64,128),  blk(128,128), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "PRNU_CKPT = sorted(Path(SAVED_DIR).glob(\"prnu_cnn_i8_best_*.pt\"))[-1]\n",
    "prnu_model = SmallForensicCNN(1).to(DEVICE).eval()\n",
    "prnu_model.load_state_dict(torch.load(PRNU_CKPT, map_location=DEVICE))\n",
    "def load_prnu_i8(path):\n",
    "    a = np.load(path, mmap_mode='r')\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==3 and (a.shape[0]==1 or a.shape[-1]==1): a = a.squeeze()\n",
    "    assert a.ndim==2 and a.dtype==np.int8, f\"PRNU expect 2D int8, got {a.shape} {a.dtype} from {path}\"\n",
    "    return a\n",
    "def avg_pool_2x(x):\n",
    "    H,W = x.shape; H2,W2=(H//2)*2,(W//2)*2\n",
    "    x = x[:H2,:W2]; return x.reshape(H2//2,2,W2//2,2).mean(axis=(1,3))\n",
    "def prnu_per_image_norm(a_i8):\n",
    "    x = a_i8.astype(np.float32, copy=False)\n",
    "    m,s = x.mean(), x.std()\n",
    "    if (not np.isfinite(s)) or s<1e-6: m,s=0.0,20.0\n",
    "    return (x-m)/(s if s>0 else 1.0)\n",
    "@torch.no_grad()\n",
    "def prnu_logits(paths, tta=TTA_PRNU):\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    rng = np.random.default_rng(1337)\n",
    "    for i in tqdm(range(0,len(paths),BATCH_PRNU), desc=f\"PRNU logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+BATCH_PRNU]\n",
    "        acc = np.zeros(len(chunk), np.float32); reps=max(1,int(tta))\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                a = load_prnu_i8(p)\n",
    "                if a.shape[0]>=512 and a.shape[1]>=512: a = avg_pool_2x(a)\n",
    "                a = crop_center_or_rand(a, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                a = prnu_per_image_norm(a)\n",
    "                xs.append(torch.from_numpy(a).unsqueeze(0))\n",
    "            xb = torch.stack(xs,0).to(DEVICE).contiguous(memory_format=torch.channels_last)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=AMP):\n",
    "                logit = prnu_model(xb)\n",
    "            acc += logit.float().cpu().numpy().astype(np.float32)\n",
    "        out[i:i+BATCH_PRNU] = acc/float(reps)\n",
    "    return out\n",
    "\n",
    "# ELA CNN\n",
    "class ELAForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        def bnblk(ci, co):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(co),\n",
    "                nn.ReLU(inplace=True))\n",
    "        def gnblk(ci, co, groups=8):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(\n",
    "            bnblk(in_ch,32), bnblk(32,32), nn.AvgPool2d(2),\n",
    "            bnblk(32,64),    bnblk(64,64), nn.AvgPool2d(2),\n",
    "            gnblk(64,128),   gnblk(128,128), nn.AvgPool2d(2),\n",
    "            gnblk(128,256),  gnblk(256,256), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "ELA_CKPT = sorted(Path(SAVED_DIR).glob(\"ela_fromnpy_cnn_best_*.pt\"))[-1]\n",
    "ela_model = ELAForensicCNN(3).to(DEVICE).eval()\n",
    "ela_model.load_state_dict(torch.load(ELA_CKPT, map_location=DEVICE))\n",
    "def load_ela_array(path):\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    if isinstance(z, np.lib.npyio.NpzFile):\n",
    "        for k in ('ela','arr','arr_0','data'):\n",
    "            if k in z.files: a = z[k]; break\n",
    "        else: a = z[z.files[0]]\n",
    "    else:\n",
    "        a = z\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==2: a = np.repeat(a[...,None],3,axis=2)\n",
    "    elif a.ndim==3 and a.shape[0] in (1,3) and a.shape[-1] not in (1,3):\n",
    "        a = np.transpose(a,(1,2,0))\n",
    "    elif a.ndim==3 and a.shape[-1]==1:\n",
    "        a = np.repeat(a,3,axis=2)\n",
    "    a = a.astype(np.float32, copy=False)\n",
    "    if np.nanmax(a)>1.5: a *= (1.0/255.0)\n",
    "    return a\n",
    "def ela_zscore(x):\n",
    "    m = x.mean(axis=(0,1), keepdims=True)\n",
    "    s = x.std(axis=(0,1), keepdims=True); s[s<1e-6]=1.0\n",
    "    return (x-m)/s\n",
    "@torch.no_grad()\n",
    "def ela_logits(paths, tta=TTA_ELA):\n",
    "    out = np.zeros(len(paths), np.float32)\n",
    "    rng = np.random.default_rng(1337)\n",
    "    for i in tqdm(range(0,len(paths),BATCH_ELA), desc=f\"ELA logits (TTA={tta})\"):\n",
    "        chunk = paths[i:i+BATCH_ELA]\n",
    "        acc = np.zeros(len(chunk), np.float32); reps=max(1,int(tta))\n",
    "        for _ in range(reps):\n",
    "            xs=[]\n",
    "            for p in chunk:\n",
    "                x = load_ela_array(p)\n",
    "                x = crop_center_or_rand(x, INPUT_SIZE, center=(tta==0), rng=rng)\n",
    "                x = ela_zscore(x)\n",
    "                xs.append(torch.from_numpy(np.transpose(x,(2,0,1))))\n",
    "            xb = torch.stack(xs,0).to(DEVICE).contiguous(memory_format=torch.channels_last)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=AMP):\n",
    "                logit = ela_model(xb)\n",
    "            acc += logit.float().cpu().numpy().astype(np.float32)\n",
    "        out[i:i+BATCH_ELA] = acc/float(reps)\n",
    "    return out\n",
    "\n",
    "# 通用裁切（和你前面一致）\n",
    "def crop_center_or_rand(x, size=256, center=True, rng=None):\n",
    "    h,w = x.shape[:2]\n",
    "    if h<size or w<size:\n",
    "        ph,pw=max(0,size-h),max(0,size-w)\n",
    "        if x.ndim==2: x=np.pad(x,((ph//2, ph-ph//2),(pw//2, pw-pw//2)),mode='edge')\n",
    "        else: x=np.pad(x,((ph//2, ph-ph//2),(pw//2, pw-pw//2),(0,0)),mode='edge')\n",
    "        h,w=x.shape[:2]\n",
    "    if h==size and w==size: return x.copy()\n",
    "    if center:\n",
    "        y0,x0=(h-size)//2,(w-size)//2\n",
    "    else:\n",
    "        if rng is None: rng=np.random.default_rng()\n",
    "        y0=int(rng.integers(0, h-size+1)); x0=int(rng.integers(0, w-size+1))\n",
    "    return x[y0:y0+size, x0:x0+size].copy() if x.ndim==2 else x[y0:y0+size, x0:x0+size,:].copy()\n",
    "\n",
    "# ---------------- 驗證門檻（Youden / FPR@5%） ----------------\n",
    "def thr_youden(y, s):\n",
    "    fpr, tpr, thr = roc_curve(y, s)\n",
    "    j = tpr - fpr\n",
    "    return float(thr[int(np.argmax(j))])\n",
    "\n",
    "def thr_fpr_at(y, s, target_fpr=0.05):\n",
    "    fpr, tpr, thr = roc_curve(y, s)\n",
    "    # 找到 <= target_fpr 的最大門檻\n",
    "    ok = np.where(fpr <= target_fpr)[0]\n",
    "    if len(ok)==0: return float(thr[-1])\n",
    "    return float(thr[int(ok[-1])])\n",
    "\n",
    "# ---------------- 逐路評估主程式 ----------------\n",
    "def evaluate_feature(feature:str):\n",
    "    assert feature in (\"clip\",\"prnu\",\"ela\")\n",
    "\n",
    "    results = {}\n",
    "    tags_cache = {}\n",
    "\n",
    "    for split in [\"val\",\"test_iid\",\"test_ood\"]:\n",
    "        if split not in SPL: continue\n",
    "\n",
    "        clipP, y, stems = get_clip_lists(split)\n",
    "\n",
    "        if feature==\"clip\":\n",
    "            paths = clipP\n",
    "        elif feature==\"prnu\":\n",
    "            paths, miss = map_by_stem(stems, y, PRNU_REAL_DIR, PRNU_FAKE_DIR, (\".npy\",))\n",
    "            if miss: print(f\"[{split}|PRNU] 缺檔 {miss}，將忽略。\")\n",
    "        else: # ela\n",
    "            paths, miss = map_by_stem(stems, y, ELA_REAL_DIR, ELA_FAKE_DIR, (\".npy\",\".npz\"))\n",
    "            if miss: print(f\"[{split}|ELA] 缺檔 {miss}，將忽略。\")\n",
    "\n",
    "        # 過濾缺檔\n",
    "        keep = [i for i,p in enumerate(paths) if p is not None]\n",
    "        paths = [paths[i] for i in keep]; yy = y[keep]\n",
    "        ss   = [stems[i] for i in keep]\n",
    "\n",
    "        print(f\"\\n[{feature.upper()}] {split} 使用樣本數：{len(paths)}\")\n",
    "\n",
    "        # 計分\n",
    "        if feature==\"clip\":\n",
    "            scores = clip_scores(paths)\n",
    "        elif feature==\"prnu\":\n",
    "            scores = prnu_logits(paths, tta=TTA_PRNU)\n",
    "        else:\n",
    "            scores = ela_logits(paths, tta=TTA_ELA)\n",
    "\n",
    "        # 在 Val 上選門檻\n",
    "        if split==\"val\":\n",
    "            thrJ  = thr_youden(yy, scores)\n",
    "            thr5  = thr_fpr_at(yy, scores, target_fpr=0.05)\n",
    "            print(f\"[{feature.upper()}|Val] thresholds → youden={thrJ:.3f} | fpr@5%={thr5:.3f}\")\n",
    "            results[\"thr_youden\"] = thrJ\n",
    "            results[\"thr_fpr5\"]   = thr5\n",
    "\n",
    "        # 總體評估（用 Val-Youden 門檻）\n",
    "        thr_used = results.get(\"thr_youden\", 0.5)\n",
    "        pred = (scores >= thr_used).astype(int)\n",
    "        acc  = accuracy_score(yy, pred)\n",
    "        try:\n",
    "            auc  = roc_auc_score(yy, scores)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "        print(f\"[{feature.upper()}|{split}] acc@thr={acc:.4f} | auc={auc:.4f} | thr={thr_used:.3f}\")\n",
    "        print(confusion_matrix(yy, pred))\n",
    "        print(classification_report(yy, pred, target_names=[\"real(0)\",\"fake(1)\"], digits=4))\n",
    "\n",
    "        # 存放以便 OOD 分析\n",
    "        results[split] = {\"y\": yy, \"scores\": scores, \"pred\": pred, \"stems\": ss, \"paths\": paths}\n",
    "\n",
    "    # -------- OOD 逐資料集拆解（若有 OOD） --------\n",
    "    if \"test_ood\" in results:\n",
    "        y  = results[\"test_ood\"][\"y\"]\n",
    "        ss = results[\"test_ood\"][\"stems\"]\n",
    "        sc = results[\"test_ood\"][\"scores\"]\n",
    "        pr = results[\"test_ood\"][\"pred\"]\n",
    "\n",
    "        # 以 label 決定 real/fake，再由 stem 推 tag\n",
    "        keys = []\n",
    "        for yi, st in zip(y, ss):\n",
    "            tag = infer_tag_from_stem(st, is_real=(yi==0))\n",
    "            keys.append((\"real\" if yi==0 else \"fake\") + \":\" + tag)\n",
    "\n",
    "        # 聚合指標\n",
    "        groups = defaultdict(list)\n",
    "        for i,k in enumerate(keys):\n",
    "            groups[k].append(i)\n",
    "\n",
    "        print(f\"\\n== OOD per-dataset ({feature.upper()}) | thr=Val-Youden ==\")\n",
    "        rows=[]\n",
    "        for k, idxs in sorted(groups.items(), key=lambda kv: -len(kv[1])):\n",
    "            yy = y[idxs]; pp = pr[idxs]; ss_ = sc[idxs]\n",
    "            n  = len(idxs)\n",
    "            acc = accuracy_score(yy, pp)\n",
    "            # class-wise recall\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(yy, pp, labels=[0,1], zero_division=0)\n",
    "            r_real, r_fake = rec[0], rec[1]\n",
    "            try:\n",
    "                auc = roc_auc_score(yy, ss_)\n",
    "            except ValueError:\n",
    "                auc = float(\"nan\")\n",
    "            print(f\"- {k:12s} n={n:5d} | acc={acc:.4f} | r_real={r_real:.4f} r_fake={r_fake:.4f} | auc={auc if not np.isnan(auc) else 'nan'}\")\n",
    "            rows.append((k, n, acc, r_real, r_fake, auc))\n",
    "\n",
    "        # 額外列出「表現最差 Top-8」（依 acc 升序）\n",
    "        worst = sorted(rows, key=lambda x: (x[2], x[1]))[:8]\n",
    "        print(\"\\n>>> 最差 Top-8（依 acc）\")\n",
    "        for k,n,acc,r0,r1,auc in worst:\n",
    "            print(f\"  {k:12s} n={n:5d} | acc={acc:.4f} | r_real={r0:.4f} r_fake={r1:.4f} | auc={auc if not np.isnan(auc) else 'nan'}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ================== 執行：分別測試三條路 ==================\n",
    "print(\"\\n===== Evaluate: CLIP =====\")\n",
    "RES_CLIP = evaluate_feature(\"clip\")\n",
    "\n",
    "print(\"\\n===== Evaluate: PRNU =====\")\n",
    "RES_PRNU = evaluate_feature(\"prnu\")\n",
    "\n",
    "print(\"\\n===== Evaluate: ELA =====\")\n",
    "RES_ELA  = evaluate_feature(\"ela\")\n",
    "\n",
    "print(\"\\n完成。你可以從各路的 '最差 Top-8' 直接看到 OOD 是哪個資料集在拖分。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
