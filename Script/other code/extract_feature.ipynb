{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d3655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using splits: /home/yaya/ai-detect-proj/Script/saved_models/splits_clip_feature_iid_ood.json\n",
      "device = cuda\n",
      "[val] CLIP=18000 | ELA=18000(miss 0) | PRNU=18000(miss 0)\n",
      "[test_iid] CLIP=18000 | ELA=18000(miss 0) | PRNU=18000(miss 0)\n",
      "[test_ood] CLIP=43989 | ELA=43989(miss 0) | PRNU=43989(miss 0)\n",
      "CLIP SVM: /home/yaya/ai-detect-proj/Script/saved_models/clip_linear_svm_feature_20250815_170428.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP load: 100%|██████████| 18000/18000 [00:09<00:00, 1812.67it/s]\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELA model: /home/yaya/ai-detect-proj/Script/saved_models/ela_fromnpy_cnn_best_20250815_172022.pt\n",
      "PRNU model: /home/yaya/ai-detect-proj/Script/saved_models/prnu_cnn_i8_best_20250815_183635.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELA infer: 100%|██████████| 141/141 [01:52<00:00,  1.25it/s]\n",
      "PRNU infer: 100%|██████████| 141/141 [00:41<00:00,  3.40it/s]\n",
      "CLIP load: 100%|██████████| 18000/18000 [00:05<00:00, 3052.24it/s]\n",
      "ELA infer: 100%|██████████| 141/141 [02:46<00:00,  1.18s/it]\n",
      "PRNU infer: 100%|██████████| 141/141 [00:42<00:00,  3.35it/s]\n",
      "CLIP load: 100%|██████████| 43989/43989 [00:16<00:00, 2676.14it/s]\n",
      "ELA infer: 100%|██████████| 344/344 [07:10<00:00,  1.25s/it]\n",
      "PRNU infer: 100%|██████████| 344/344 [01:35<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion weights: [[7.38651767 2.42389983 0.25492934]] bias: [-4.71015858]\n",
      "[Val Fused] AUC=0.9892 | thr_youden=0.733 | thr_fpr@5%=0.101\n",
      "\n",
      "[Val (Youden)] acc@thr=0.9779 | auc=0.9892 | thr=0.733\n",
      "[[ 5958    42]\n",
      " [  356 11644]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9436    0.9930    0.9677      6000\n",
      "           1     0.9964    0.9703    0.9832     12000\n",
      "\n",
      "    accuracy                         0.9779     18000\n",
      "   macro avg     0.9700    0.9817    0.9754     18000\n",
      "weighted avg     0.9788    0.9779    0.9780     18000\n",
      "\n",
      "\n",
      "[Test-IID (Youden)] acc@thr=0.9793 | auc=0.9904 | thr=0.733\n",
      "[[ 5961    39]\n",
      " [  334 11666]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9469    0.9935    0.9697      6000\n",
      "           1     0.9967    0.9722    0.9843     12000\n",
      "\n",
      "    accuracy                         0.9793     18000\n",
      "   macro avg     0.9718    0.9828    0.9770     18000\n",
      "weighted avg     0.9801    0.9793    0.9794     18000\n",
      "\n",
      "\n",
      "[Test-OOD (Youden)] acc@thr=0.4818 | auc=0.8806 | thr=0.733\n",
      "[[ 2218 22771]\n",
      " [   23 18977]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9897    0.0888    0.1629     24989\n",
      "           1     0.4546    0.9988    0.6248     19000\n",
      "\n",
      "    accuracy                         0.4818     43989\n",
      "   macro avg     0.7221    0.5438    0.3938     43989\n",
      "weighted avg     0.7586    0.4818    0.3624     43989\n",
      "\n",
      "\n",
      "[Val (FPR@5%)] acc@thr=0.9688 | auc=0.9892 | thr=0.101\n",
      "[[ 5701   299]\n",
      " [  263 11737]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9559    0.9502    0.9530      6000\n",
      "           1     0.9752    0.9781    0.9766     12000\n",
      "\n",
      "    accuracy                         0.9688     18000\n",
      "   macro avg     0.9655    0.9641    0.9648     18000\n",
      "weighted avg     0.9687    0.9688    0.9688     18000\n",
      "\n",
      "\n",
      "[Test-IID (FPR@5%)] acc@thr=0.9709 | auc=0.9904 | thr=0.101\n",
      "[[ 5707   293]\n",
      " [  230 11770]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9613    0.9512    0.9562      6000\n",
      "           1     0.9757    0.9808    0.9783     12000\n",
      "\n",
      "    accuracy                         0.9709     18000\n",
      "   macro avg     0.9685    0.9660    0.9672     18000\n",
      "weighted avg     0.9709    0.9709    0.9709     18000\n",
      "\n",
      "\n",
      "[Test-OOD (FPR@5%)] acc@thr=0.4512 | auc=0.8806 | thr=0.101\n",
      "[[  849 24140]\n",
      " [    0 19000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.0340    0.0657     24989\n",
      "           1     0.4404    1.0000    0.6115     19000\n",
      "\n",
      "    accuracy                         0.4512     43989\n",
      "   macro avg     0.7202    0.5170    0.3386     43989\n",
      "weighted avg     0.7583    0.4512    0.3015     43989\n",
      "\n",
      "\n",
      "[OOD breakdown @ FPR@5%]\n",
      "\n",
      "== OOD per-dataset ==\n",
      "- unsplash   n=24989 | acc=0.0340 | auc=nan\n",
      "[[  849 24140]\n",
      " [    0     0]]\n",
      "\n",
      "- dalle3     n=19000 | acc=1.0000 | auc=nan\n",
      "[[19000]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 三特徵融合（CLIP + ELA + PRNU）→ OOD 評估\n",
    "# - 自動抓最新模型\n",
    "# - 用同一份 splits_*.json 對齊樣本\n",
    "# - Val 上訓練融合器（LogReg），測 Val/Test-IID/Test-OOD\n",
    "# ==========================================\n",
    "import os, json, glob, math, re, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "import joblib\n",
    "\n",
    "# ---------- 路徑設定 ----------\n",
    "SCRIPT_ROOT = \"/home/yaya/ai-detect-proj/Script\"\n",
    "OUTPUT_DIR  = os.path.join(SCRIPT_ROOT, \"saved_models\")\n",
    "SPLITS_CAND = [\n",
    "    os.path.join(OUTPUT_DIR, \"splits_fewshot_iid_ood.json\"),\n",
    "    os.path.join(OUTPUT_DIR, \"splits_clip_feature_iid_ood.json\"),\n",
    "]\n",
    "SPLITS_JSON = next((p for p in SPLITS_CAND if os.path.isfile(p)), None)\n",
    "assert SPLITS_JSON, \"找不到 splits JSON，請先產生（fewshot 或 clip_feature）\"\n",
    "print(\"Using splits:\", SPLITS_JSON)\n",
    "\n",
    "# 資料目錄\n",
    "CLIP_REAL_DIR = os.path.join(SCRIPT_ROOT, \"features_npy\", \"clip_real_npy\")\n",
    "CLIP_FAKE_DIR = os.path.join(SCRIPT_ROOT, \"features_npy\", \"clip_fake_npy\")\n",
    "ELA_REAL_DIR  = os.path.join(SCRIPT_ROOT, \"features_npy\", \"ela_real_npy\")\n",
    "ELA_FAKE_DIR  = os.path.join(SCRIPT_ROOT, \"features_npy\", \"ela_fake_npy\")\n",
    "PRNU_REAL_I8  = os.path.join(SCRIPT_ROOT, \"features_quant\", \"prnu_real_i8\")\n",
    "PRNU_FAKE_I8  = os.path.join(SCRIPT_ROOT, \"features_quant\", \"prnu_fake_i8\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "# ---------- 讀 splits ----------\n",
    "with open(SPLITS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    SPLITS = json.load(f)[\"splits\"]\n",
    "\n",
    "def is_real_clip(p: str) -> bool:\n",
    "    s = Path(p).as_posix()\n",
    "    return \"/clip_real_npy/\" in s\n",
    "\n",
    "# 建 ELA/PRNU 檔名索引（用 basename 或 stem 對應）\n",
    "def index_dir(d):\n",
    "    idx={}\n",
    "    for q in Path(d).glob(\"*.npy\"):\n",
    "        idx.setdefault(q.name.lower(), str(q))\n",
    "        idx.setdefault(q.stem.lower(), str(q))\n",
    "    for q in Path(d).glob(\"*.npz\"):\n",
    "        idx.setdefault(q.name.lower(), str(q))\n",
    "        idx.setdefault(q.stem.lower(), str(q))\n",
    "    return idx\n",
    "\n",
    "IDX_ELA_REAL = index_dir(ELA_REAL_DIR)\n",
    "IDX_ELA_FAKE = index_dir(ELA_FAKE_DIR)\n",
    "IDX_PR_REAL  = index_dir(PRNU_REAL_I8)\n",
    "IDX_PR_FAKE  = index_dir(PRNU_FAKE_I8)\n",
    "\n",
    "def map_to(dir_idx_real, dir_idx_fake, p: str):\n",
    "    \"\"\"從 CLIP split 的路徑 map 到某種特徵（ELA/PRNU）的目錄\"\"\"\n",
    "    y = 0 if is_real_clip(p) else 1\n",
    "    key_bn = Path(p).name.lower(); key_st = Path(p).stem.lower()\n",
    "    idx = dir_idx_real if y==0 else dir_idx_fake\n",
    "    q = idx.get(key_bn) or idx.get(key_st)\n",
    "    return q, y\n",
    "\n",
    "def get_split_paths(name: str):\n",
    "    clp = SPLITS.get(name, [])\n",
    "    clip_paths = []; clip_y = []\n",
    "    ela_paths  = []; ela_y  = []; miss_ela=0\n",
    "    pr_paths   = []; pr_y   = []; miss_pr =0\n",
    "    for p in clp:\n",
    "        clip_paths.append(p); clip_y.append(0 if is_real_clip(p) else 1)\n",
    "        q1,y1 = map_to(IDX_ELA_REAL, IDX_ELA_FAKE, p)\n",
    "        if q1 is None: miss_ela += 1\n",
    "        else: ela_paths.append(q1); ela_y.append(y1)\n",
    "        q2,y2 = map_to(IDX_PR_REAL, IDX_PR_FAKE, p)\n",
    "        if q2 is None: miss_pr += 1\n",
    "        else: pr_paths.append(q2); pr_y.append(y2)\n",
    "    print(f\"[{name}] CLIP={len(clip_paths)} | ELA={len(ela_paths)}(miss {miss_ela}) | PRNU={len(pr_paths)}(miss {miss_pr})\")\n",
    "    return (clip_paths, np.array(clip_y,int),\n",
    "            ela_paths, np.array(ela_y,int),\n",
    "            pr_paths,  np.array(pr_y,int))\n",
    "\n",
    "(val_clip, yv_clip, val_ela, yv_ela, val_pr, yv_pr) = get_split_paths(\"val\")\n",
    "(ti_clip, yti_clip, ti_ela, yti_ela, ti_pr, yti_pr) = get_split_paths(\"test_iid\")\n",
    "(to_clip, yto_clip, to_ela, yto_ela, to_pr, yto_pr) = get_split_paths(\"test_ood\")\n",
    "\n",
    "assert len(val_clip)>0 and len(val_ela)>0 and len(val_pr)>0, \"Val split 對不到（請檢查命名/目錄）\"\n",
    "assert (len(yv_clip)==len(yv_ela)==len(yv_pr)), \"Val 標籤長度不一致，請檢查索引\"\n",
    "\n",
    "# ---------- CLIP：載入最新 LinearSVC，Val 上做 sigmoid 校準 ----------\n",
    "def latest_file(patterns):\n",
    "    cand=[]\n",
    "    for pat in (patterns if isinstance(patterns,(list,tuple)) else [patterns]):\n",
    "        cand += glob.glob(os.path.join(OUTPUT_DIR, pat))\n",
    "    assert cand, f\"找不到模型：{patterns}\"\n",
    "    cand = sorted(cand, key=os.path.getmtime)\n",
    "    return cand[-1]\n",
    "\n",
    "CLIP_SVM_PATH = latest_file(\"clip_linear_svm_feature_*.joblib\")\n",
    "print(\"CLIP SVM:\", CLIP_SVM_PATH)\n",
    "svc = joblib.load(CLIP_SVM_PATH)\n",
    "\n",
    "def load_clip_vec(p):\n",
    "    v = np.asarray(np.load(p, allow_pickle=True)).astype(np.float32).reshape(-1)\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return v / n\n",
    "\n",
    "def load_clip_matrix(paths):\n",
    "    X = np.stack([load_clip_vec(p) for p in tqdm(paths, desc=\"CLIP load\")], axis=0)\n",
    "    return X\n",
    "\n",
    "# 校準（用 Val）\n",
    "Xv_clip = load_clip_matrix(val_clip)\n",
    "cal_clip = CalibratedClassifierCV(svc, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal_clip.fit(Xv_clip, yv_clip)  # 只學校準參數\n",
    "\n",
    "# ---------- ELA：模型與讀檔 ----------\n",
    "def load_ela_array(path):\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    if isinstance(z, np.lib.npyio.NpzFile):\n",
    "        a = z.get('ela', z.get('arr', z.get('arr_0')))\n",
    "    else:\n",
    "        a = z\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 2:\n",
    "        a = np.repeat(a[...,None], 3, axis=2)\n",
    "    elif a.ndim == 3 and a.shape[0] in (1,3) and a.shape[-1] not in (1,3):\n",
    "        a = np.transpose(a, (1,2,0))\n",
    "    elif a.ndim == 3 and a.shape[-1] == 1:\n",
    "        a = np.repeat(a, 3, axis=2)\n",
    "    a = a.astype(np.float32)\n",
    "    if a.max() > 1.5: a *= (1/255.0)\n",
    "    return a\n",
    "\n",
    "def zscore3(x):\n",
    "    m = x.mean(axis=(0,1), keepdims=True)\n",
    "    s = x.std(axis=(0,1), keepdims=True); s[s<1e-6]=1.0\n",
    "    return (x - m) / s\n",
    "\n",
    "def crop_hw3(img, size=256, center=True):\n",
    "    h,w,_ = img.shape\n",
    "    if h < size or w < size:\n",
    "        ph, pw = max(0,size-h), max(0,size-w)\n",
    "        img = np.pad(img, ((ph//2,ph-ph//2),(pw//2,pw-pw//2),(0,0)), mode='edge')\n",
    "        h,w,_ = img.shape\n",
    "    if center:\n",
    "        top=(h-size)//2; left=(w-size)//2\n",
    "    else:\n",
    "        top=0; left=0\n",
    "    return img[top:top+size, left:left+size, :].copy()\n",
    "\n",
    "class ELAForensicCNN(nn.Module):\n",
    "    def __init__(self,in_ch=3):\n",
    "        super().__init__()\n",
    "        def bnblk(ci,co): \n",
    "            return nn.Sequential(nn.Conv2d(ci,co,3,padding=1,bias=False), nn.BatchNorm2d(co), nn.ReLU(True))\n",
    "        def gnblk(ci,co,g=8):\n",
    "            return nn.Sequential(nn.Conv2d(ci,co,3,padding=1,bias=False), nn.GroupNorm(num_groups=min(g,co), num_channels=co), nn.ReLU(True))\n",
    "        self.net = nn.Sequential(\n",
    "            bnblk(in_ch,32), bnblk(32,32), nn.AvgPool2d(2),\n",
    "            bnblk(32,64),    bnblk(64,64), nn.AvgPool2d(2),\n",
    "            gnblk(64,128),   gnblk(128,128), nn.AvgPool2d(2),\n",
    "            gnblk(128,256),  gnblk(256,256), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(256,1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "ELA_MODEL_PATH = latest_file(\"ela_fromnpy_cnn_best_*.pt\")\n",
    "print(\"ELA model:\", ELA_MODEL_PATH)\n",
    "ela_model = ELAForensicCNN().to(device).eval()\n",
    "ela_model.load_state_dict(torch.load(ELA_MODEL_PATH, map_location=device), strict=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ela_probs(paths, batch=128, size=256):\n",
    "    ps=[]\n",
    "    for i in tqdm(range(0, len(paths), batch), desc=\"ELA infer\"):\n",
    "        xs=[]\n",
    "        for p in paths[i:i+batch]:\n",
    "            a = load_ela_array(p)\n",
    "            a = crop_hw3(a, size=size, center=True)\n",
    "            a = zscore3(a)\n",
    "            xs.append(np.transpose(a,(2,0,1)))\n",
    "        t = torch.from_numpy(np.stack(xs,0)).to(device)\n",
    "        logit = ela_model(t.contiguous(memory_format=torch.channels_last))\n",
    "        ps.extend(torch.sigmoid(logit).float().cpu().numpy().tolist())\n",
    "    return np.array(ps)\n",
    "\n",
    "# ---------- PRNU：模型與讀檔（int8） ----------\n",
    "def load_i8_2d(path):\n",
    "    a = np.load(path, mmap_mode='r')\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 3 and (a.shape[0]==1 or a.shape[-1]==1):\n",
    "        a = a.squeeze()\n",
    "    assert a.ndim==2, f\"{path} got {a.shape}\"\n",
    "    return a.astype(np.float32)\n",
    "\n",
    "def per_image_norm(x):\n",
    "    m, s = x.mean(), x.std()\n",
    "    if not np.isfinite(s) or s<1e-6: s=20.0; m=0.0\n",
    "    return (x - m) / s\n",
    "\n",
    "def avg_pool_2x(x):\n",
    "    H,W = x.shape; H2,W2 = H//2*2, W//2*2\n",
    "    x = x[:H2,:W2].reshape(H2//2,2,W2//2,2).mean(axis=(1,3))\n",
    "    return x\n",
    "\n",
    "def crop_2d(img, size=256, center=True):\n",
    "    h,w = img.shape\n",
    "    if h < size or w < size:\n",
    "        ph,pw = max(0,size-h), max(0,size-w)\n",
    "        img = np.pad(img, ((ph//2,ph-ph//2),(pw//2,pw-pw//2)), mode='edge')\n",
    "        h,w = img.shape\n",
    "    if center:\n",
    "        y0=(h-size)//2; x0=(w-size)//2\n",
    "    else:\n",
    "        y0=0; x0=0\n",
    "    return img[y0:y0+size, x0:x0+size].copy()\n",
    "\n",
    "class SmallForensicCNN(nn.Module):\n",
    "    def __init__(self,in_ch=1):\n",
    "        super().__init__()\n",
    "        def blk(ci,co,g=8):\n",
    "            return nn.Sequential(nn.Conv2d(ci,co,3,padding=1,bias=False),\n",
    "                                 nn.GroupNorm(num_groups=min(g,co), num_channels=co),\n",
    "                                 nn.ReLU(True))\n",
    "        self.net = nn.Sequential(\n",
    "            blk(in_ch,32), blk(32,32), nn.AvgPool2d(2),\n",
    "            blk(32,64),    blk(64,64), nn.AvgPool2d(2),\n",
    "            blk(64,128),   blk(128,128), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128,1)\n",
    "    def forward(self,x): return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "PRNU_MODEL_PATH = latest_file([\"prnu_cnn_i8_best_*.pt\",\"prnu_fromnpy_cnn_best_*.pt\"])\n",
    "print(\"PRNU model:\", PRNU_MODEL_PATH)\n",
    "pr_model = SmallForensicCNN(1).to(device).eval()\n",
    "pr_model.load_state_dict(torch.load(PRNU_MODEL_PATH, map_location=device), strict=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def prnu_probs(paths, batch=128, size=256, downsample=True):\n",
    "    ps=[]\n",
    "    for i in tqdm(range(0, len(paths), batch), desc=\"PRNU infer\"):\n",
    "        xs=[]\n",
    "        for p in paths[i:i+batch]:\n",
    "            w = load_i8_2d(p)                 # int8→float32\n",
    "            if downsample and (w.shape[0]>=512 and w.shape[1]>=512):\n",
    "                w = avg_pool_2x(w)\n",
    "            w = crop_2d(w, size=size, center=True)\n",
    "            w = per_image_norm(w)\n",
    "            xs.append(w[None,...])\n",
    "        t = torch.from_numpy(np.stack(xs,0)).to(device)\n",
    "        logit = pr_model(t.contiguous(memory_format=torch.channels_last))\n",
    "        ps.extend(torch.sigmoid(logit).float().cpu().numpy().tolist())\n",
    "    return np.array(ps)\n",
    "\n",
    "# ---------- 取得三路機率 ----------\n",
    "# Val\n",
    "pv_clip = cal_clip.predict_proba(Xv_clip)[:,1]\n",
    "pv_ela  = ela_probs(val_ela)\n",
    "pv_pr   = prnu_probs(val_pr)\n",
    "assert len(pv_clip)==len(pv_ela)==len(pv_pr)==len(yv_clip)\n",
    "\n",
    "# Test-IID\n",
    "Xi_clip = load_clip_matrix(ti_clip) if len(ti_clip) else np.zeros((0, Xv_clip.shape[1]), np.float32)\n",
    "pi_clip = cal_clip.predict_proba(Xi_clip)[:,1] if len(ti_clip) else np.array([])\n",
    "pi_ela  = ela_probs(ti_ela) if len(ti_ela) else np.array([])\n",
    "pi_pr   = prnu_probs(ti_pr) if len(ti_pr) else np.array([])\n",
    "\n",
    "# Test-OOD\n",
    "Xo_clip = load_clip_matrix(to_clip) if len(to_clip) else np.zeros((0, Xv_clip.shape[1]), np.float32)\n",
    "po_clip = cal_clip.predict_proba(Xo_clip)[:,1] if len(to_clip) else np.array([])\n",
    "po_ela  = ela_probs(to_ela) if len(to_ela) else np.array([])\n",
    "po_pr   = prnu_probs(to_pr) if len(to_pr) else np.array([])\n",
    "\n",
    "# ---------- 融合器（在 Val 上學） ----------\n",
    "def stack_feats(p1,p2,p3): \n",
    "    return np.stack([p1,p2,p3], axis=1).astype(np.float32)\n",
    "Xv = stack_feats(pv_clip, pv_ela, pv_pr); yv = yv_clip\n",
    "Xi = stack_feats(pi_clip, pi_ela, pi_pr) if len(pi_clip) else None\n",
    "Xo = stack_feats(po_clip, po_ela, po_pr) if len(po_clip) else None\n",
    "\n",
    "fuser = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "fuser.fit(Xv, yv)\n",
    "print(\"Fusion weights:\", fuser.coef_, \"bias:\", fuser.intercept_)\n",
    "\n",
    "def eval_block(name, X, y):\n",
    "    if X is None or len(X)==0:\n",
    "        print(f\"[{name}] (empty)\"); return None\n",
    "    proba = fuser.predict_proba(X)[:,1]\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    # Youden 門檻（用 Val 的）\n",
    "    return proba, auc\n",
    "\n",
    "# 以 Val 找兩個門檻\n",
    "p_val_fused, auc_val = eval_block(\"Val\", Xv, yv)\n",
    "fpr, tpr, thr = roc_curve(yv, p_val_fused)\n",
    "thr_youden = float(thr[(tpr - fpr).argmax()])\n",
    "idx = np.where(fpr <= 0.05)[0]\n",
    "thr_fpr05 = float(thr[idx[-1]]) if len(idx) else float(thr[0])\n",
    "print(f\"[Val Fused] AUC={auc_val:.4f} | thr_youden={thr_youden:.3f} | thr_fpr@5%={thr_fpr05:.3f}\")\n",
    "\n",
    "def report(name, X, y, thr):\n",
    "    if X is None or len(X)==0:\n",
    "        print(f\"[{name}] (empty)\"); return\n",
    "    p = fuser.predict_proba(X)[:,1]\n",
    "    pred = (p >= thr).astype(int)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    try: auc = roc_auc_score(y, p)\n",
    "    except: auc = float(\"nan\")\n",
    "    print(f\"\\n[{name}] acc@thr={acc:.4f} | auc={auc:.4f} | thr={thr:.3f}\")\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred, digits=4))\n",
    "\n",
    "# 報告（Youden 與 FPR@5%）\n",
    "report(\"Val (Youden)\", Xv, yv, thr_youden)\n",
    "if Xi is not None: report(\"Test-IID (Youden)\", Xi, yti_clip, thr_youden)\n",
    "if Xo is not None: report(\"Test-OOD (Youden)\", Xo, yto_clip, thr_youden)\n",
    "\n",
    "report(\"Val (FPR@5%)\", Xv, yv, thr_fpr05)\n",
    "if Xi is not None: report(\"Test-IID (FPR@5%)\", Xi, yti_clip, thr_fpr05)\n",
    "if Xo is not None: report(\"Test-OOD (FPR@5%)\", Xo, yto_clip, thr_fpr05)\n",
    "\n",
    "# ---------- OOD 分來源（用 FPR@5% 門檻） ----------\n",
    "def tag_from_path(p: str, y: int):\n",
    "    s = Path(p).as_posix().lower()\n",
    "    if y==0:\n",
    "        return \"unsplash\" if \"unsplash\" in s else (\"imagenet\" if \"imagenet\" in s else \"real\")\n",
    "    for k in [\"dalle3\",\"midjourney\",\"sd3\",\"flux\",\"stable\",\"sdxl\",\"playground\",\"kolors\"]:\n",
    "        if k in s: return k\n",
    "    # 從檔名首段猜\n",
    "    stem = Path(p).stem.lower().split(\"_\")[0].split(\"-\")[0]\n",
    "    return stem or \"fake\"\n",
    "\n",
    "def ood_breakdown(thr):\n",
    "    if Xo is None: \n",
    "        print(\"\\n[OOD breakdown] (empty)\"); return\n",
    "    p = fuser.predict_proba(Xo)[:,1]; y = yto_clip\n",
    "    # 用 CLIP split 的原始路徑來抓標籤（tag）\n",
    "    tags = [tag_from_path(pp, yy) for pp,yy in zip(to_clip,y)]\n",
    "    buckets = {}\n",
    "    for yy,pp,tag in zip(y,p,tags):\n",
    "        buckets.setdefault(tag, []).append((yy,pp))\n",
    "    print(\"\\n== OOD per-dataset ==\")\n",
    "    for tag, lst in sorted(buckets.items(), key=lambda kv: -len(kv[1])):\n",
    "        arr_y = np.array([a for a,_ in lst]); arr_p = np.array([b for _,b in lst])\n",
    "        try: auc = roc_auc_score(arr_y, arr_p)\n",
    "        except: auc = float(\"nan\")\n",
    "        pred = (arr_p >= thr).astype(int)\n",
    "        acc = (pred==arr_y).mean()\n",
    "        cm  = confusion_matrix(arr_y, pred)\n",
    "        print(f\"- {tag:10s} n={len(arr_y):5d} | acc={acc:.4f} | auc={auc:.4f}\\n{cm}\\n\")\n",
    "\n",
    "print(\"\\n[OOD breakdown @ FPR@5%]\")\n",
    "ood_breakdown(thr_fpr05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27265c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Val (Youden, ELA+PRNU)] acc=0.9353 | auc=0.9729 | thr=0.467\n",
      "[[ 5708   292]\n",
      " [  873 11127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8673    0.9513    0.9074      6000\n",
      "           1     0.9744    0.9273    0.9503     12000\n",
      "\n",
      "    accuracy                         0.9353     18000\n",
      "   macro avg     0.9209    0.9393    0.9288     18000\n",
      "weighted avg     0.9387    0.9353    0.9360     18000\n",
      "\n",
      "\n",
      "[Test-IID (Youden, ELA+PRNU)] acc=0.9382 | auc=0.9753 | thr=0.467\n",
      "[[ 5714   286]\n",
      " [  827 11173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8736    0.9523    0.9113      6000\n",
      "           1     0.9750    0.9311    0.9526     12000\n",
      "\n",
      "    accuracy                         0.9382     18000\n",
      "   macro avg     0.9243    0.9417    0.9319     18000\n",
      "weighted avg     0.9412    0.9382    0.9388     18000\n",
      "\n",
      "\n",
      "[Test-OOD (Youden, ELA+PRNU)] acc=0.7845 | auc=0.8802 | thr=0.467\n",
      "[[19791  5198]\n",
      " [ 4283 14717]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8221    0.7920    0.8068     24989\n",
      "           1     0.7390    0.7746    0.7564     19000\n",
      "\n",
      "    accuracy                         0.7845     43989\n",
      "   macro avg     0.7805    0.7833    0.7816     43989\n",
      "weighted avg     0.7862    0.7845    0.7850     43989\n",
      "\n",
      "\n",
      "[Val (FPR@5%, ELA+PRNU)] acc=0.9355 | auc=0.9729 | thr=0.457\n",
      "[[ 5703   297]\n",
      " [  864 11136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8684    0.9505    0.9076      6000\n",
      "           1     0.9740    0.9280    0.9505     12000\n",
      "\n",
      "    accuracy                         0.9355     18000\n",
      "   macro avg     0.9212    0.9393    0.9290     18000\n",
      "weighted avg     0.9388    0.9355    0.9362     18000\n",
      "\n",
      "\n",
      "[Test-IID (FPR@5%, ELA+PRNU)] acc=0.9384 | auc=0.9753 | thr=0.457\n",
      "[[ 5710   290]\n",
      " [  818 11182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8747    0.9517    0.9116      6000\n",
      "           1     0.9747    0.9318    0.9528     12000\n",
      "\n",
      "    accuracy                         0.9384     18000\n",
      "   macro avg     0.9247    0.9417    0.9322     18000\n",
      "weighted avg     0.9414    0.9384    0.9390     18000\n",
      "\n",
      "\n",
      "[Test-OOD (FPR@5%, ELA+PRNU)] acc=0.7841 | auc=0.8802 | thr=0.457\n",
      "[[19730  5259]\n",
      " [ 4237 14763]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8232    0.7895    0.8060     24989\n",
      "           1     0.7373    0.7770    0.7567     19000\n",
      "\n",
      "    accuracy                         0.7841     43989\n",
      "   macro avg     0.7803    0.7833    0.7813     43989\n",
      "weighted avg     0.7861    0.7841    0.7847     43989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 只用 ELA+PRNU 在 Val 訓練融合器，然後評估\n",
    "Xv_2 = np.stack([pv_ela, pv_pr], axis=1).astype(np.float32); yv_2 = yv\n",
    "Xi_2 = np.stack([pi_ela, pi_pr], axis=1).astype(np.float32) if len(pi_ela) else None\n",
    "Xo_2 = np.stack([po_ela, po_pr], axis=1).astype(np.float32) if len(po_ela) else None\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "fuser_2 = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=0.5)\n",
    "fuser_2.fit(Xv_2, yv_2)\n",
    "\n",
    "p_val_2 = fuser_2.predict_proba(Xv_2)[:,1]\n",
    "fpr,tpr,thr = roc_curve(yv_2, p_val_2)\n",
    "thr_y_2 = float(thr[(tpr-fpr).argmax()])\n",
    "idx = np.where(fpr<=0.05)[0]; thr_f05_2 = float(thr[idx[-1]]) if len(idx) else float(thr[0])\n",
    "\n",
    "def report(name, X, y, thr):\n",
    "    if X is None: print(f\"[{name}] empty\"); return\n",
    "    p = fuser_2.predict_proba(X)[:,1]\n",
    "    pred = (p>=thr).astype(int)\n",
    "    print(f\"\\n[{name}] acc={ (pred==y).mean():.4f} | auc={roc_auc_score(y,p):.4f} | thr={thr:.3f}\")\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred, digits=4))\n",
    "\n",
    "report(\"Val (Youden, ELA+PRNU)\", Xv_2, yv_2, thr_y_2)\n",
    "if Xi_2 is not None: report(\"Test-IID (Youden, ELA+PRNU)\", Xi_2, yti_clip, thr_y_2)\n",
    "if Xo_2 is not None: report(\"Test-OOD (Youden, ELA+PRNU)\", Xo_2, yto_clip, thr_y_2)\n",
    "\n",
    "report(\"Val (FPR@5%, ELA+PRNU)\", Xv_2, yv_2, thr_f05_2)\n",
    "if Xi_2 is not None: report(\"Test-IID (FPR@5%, ELA+PRNU)\", Xi_2, yti_clip, thr_f05_2)\n",
    "if Xo_2 is not None: report(\"Test-OOD (FPR@5%, ELA+PRNU)\", Xo_2, yto_clip, thr_f05_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1e0641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test-OOD (Youden, cost-sensitive)] acc=0.4814 | auc=0.8844 | thr=0.639\n",
      "[[ 2198 22791]\n",
      " [   21 18979]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9905    0.0880    0.1616     24989\n",
      "           1     0.4544    0.9989    0.6246     19000\n",
      "\n",
      "    accuracy                         0.4814     43989\n",
      "   macro avg     0.7225    0.5434    0.3931     43989\n",
      "weighted avg     0.7590    0.4814    0.3616     43989\n",
      "\n",
      "\n",
      "[Test-OOD (FPR@5%, cost-sensitive)] acc=0.4517 | auc=0.8844 | thr=0.072\n",
      "[[  871 24118]\n",
      " [    0 19000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.0349    0.0674     24989\n",
      "           1     0.4407    1.0000    0.6117     19000\n",
      "\n",
      "    accuracy                         0.4517     43989\n",
      "   macro avg     0.7203    0.5174    0.3396     43989\n",
      "weighted avg     0.7584    0.4517    0.3025     43989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以 Val 的真實類加權（權重可從 2~5 試）\n",
    "w = np.where(yv==0, 3.0, 1.0).astype(np.float32)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "fuser_cs = LogisticRegression(max_iter=1000, C=0.5)  # 可再小一點更保守\n",
    "fuser_cs.fit(np.stack([pv_clip,pv_ela,pv_pr],1), yv, sample_weight=w)\n",
    "\n",
    "# 門檻沿用 Val ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "p_val_cs = fuser_cs.predict_proba(np.stack([pv_clip,pv_ela,pv_pr],1))[:,1]\n",
    "fpr,tpr,thr = roc_curve(yv, p_val_cs)\n",
    "thr_y_cs = float(thr[(tpr-fpr).argmax()])\n",
    "idx = np.where(fpr<=0.05)[0]; thr_f05_cs = float(thr[idx[-1]]) if len(idx) else float(thr[0])\n",
    "\n",
    "def report_cs(name, X, y, thr):\n",
    "    if X is None: print(f\"[{name}] empty\"); return\n",
    "    p = fuser_cs.predict_proba(X)[:,1]; pred=(p>=thr).astype(int)\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "    print(f\"\\n[{name}] acc={ (pred==y).mean():.4f} | auc={roc_auc_score(y,p):.4f} | thr={thr:.3f}\")\n",
    "    print(confusion_matrix(y, pred)); print(classification_report(y, pred, digits=4))\n",
    "\n",
    "Xi = np.stack([pi_clip,pi_ela,pi_pr],1) if len(pi_clip) else None\n",
    "Xo = np.stack([po_clip,po_ela,po_pr],1) if len(po_clip) else None\n",
    "report_cs(\"Test-OOD (Youden, cost-sensitive)\", Xo, yto_clip, thr_y_cs)\n",
    "report_cs(\"Test-OOD (FPR@5%, cost-sensitive)\", Xo, yto_clip, thr_f05_cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835a3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "將改名 2700 個檔案；唯一影像數 900；零填位數 4。\n",
      "📝 對照表已輸出： /home/yaya/ai-detect-proj/Script/features_npy/DIV2K_rename_map_4d.csv\n",
      "✅ 已完成改名。\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Rename DIV2K npy → \"DIV2K__000001.npy\" (consistent across features)\n",
    "# ======================================================\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# ---- 設定 ----\n",
    "OUT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_npy\")  # 你的特徵輸出根目錄\n",
    "NEW_PREFIX = \"DIV2K\"                      # 新檔名前綴\n",
    "MATCH_PREFIXES = (\"DIV2K\", \"DIV2K_HR\")    # 視為 DIV2K 的舊前綴\n",
    "DRY_RUN = False                           # True=只預覽不改名\n",
    "START_INDEX = 1                           # 編號起始（通常 1）\n",
    "PAD_WIDTH = None                          # None=自動；或手動給 6 → 000001\n",
    "\n",
    "# ---- 收集目標檔案（僅限 *_npy 資料夾）----\n",
    "assert OUT_ROOT.exists(), f\"找不到目錄：{OUT_ROOT}\"\n",
    "all_npy = [p for p in OUT_ROOT.rglob(\"*.npy\") if p.parent.name.endswith(\"_npy\")]\n",
    "\n",
    "def is_div2k_stem(stem: str) -> bool:\n",
    "    up = stem.upper()\n",
    "    return any(up.startswith(pref.upper()) for pref in MATCH_PREFIXES)\n",
    "\n",
    "target_files = [p for p in all_npy if is_div2k_stem(p.stem)]\n",
    "if not target_files:\n",
    "    raise SystemExit(\"在 features_npy 中找不到 DIV2K 相關的 .npy 檔；請確認 MATCH_PREFIXES 或路徑。\")\n",
    "\n",
    "# ---- 建立「舊stem → 新stem」的編號映射（跨特徵一致）----\n",
    "unique_stems = sorted({p.stem for p in target_files})\n",
    "total = len(unique_stems)\n",
    "pad = PAD_WIDTH or max(4, len(str(total)))\n",
    "mapping = {\n",
    "    stem: f\"{NEW_PREFIX}__{idx:0{pad}d}\"\n",
    "    for idx, stem in enumerate(unique_stems, start=START_INDEX)\n",
    "}\n",
    "\n",
    "# ---- 規劃改名 & 檢查衝突 ----\n",
    "plan = []\n",
    "conflicts = []\n",
    "for src in target_files:\n",
    "    new_stem = mapping[src.stem]\n",
    "    dst = src.with_name(new_stem + src.suffix)\n",
    "    plan.append((src, dst))\n",
    "    if dst.exists() and dst != src:\n",
    "        conflicts.append((src, dst))\n",
    "\n",
    "print(f\"將改名 {len(plan)} 個檔案；唯一影像數 {len(unique_stems)}；零填位數 {pad}。\")\n",
    "if conflicts:\n",
    "    print(\"⚠️ 偵測到會覆蓋既有檔案，為安全起見先中止。衝突範例：\")\n",
    "    for s, d in conflicts[:10]:\n",
    "        print(\" -\", d)\n",
    "    raise SystemExit(\"請先處理衝突或調整 MATCH_PREFIXES。\")\n",
    "\n",
    "# ---- 輸出對照表 CSV（可用來回復舊名）----\n",
    "map_csv = OUT_ROOT / f\"DIV2K_rename_map_{pad}d.csv\"\n",
    "with open(map_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"old_path\",\"new_path\",\"old_stem\",\"new_stem\"])\n",
    "    for s, d in plan:\n",
    "        w.writerow([str(s), str(d), s.stem, d.stem])\n",
    "print(\"📝 對照表已輸出：\", map_csv)\n",
    "\n",
    "# ---- 執行改名 ----\n",
    "if not DRY_RUN:\n",
    "    for s, d in plan:\n",
    "        s.rename(d)\n",
    "    print(\"✅ 已完成改名。\")\n",
    "else:\n",
    "    print(\"（DRY_RUN=True 僅預覽，未實際改名）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3929b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "將改名 26355 個檔案；唯一影像數 8785；零填位數 4。\n",
      "📝 對照表已輸出： /home/yaya/ai-detect-proj/Script/features_npy/COCO2017_rename_map_4d.csv\n",
      "✅ 已完成改名。\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Rename COCO2017 npy → \"COCO2017__000001.npy\"（跨特徵共享編號）\n",
    "# ======================================================\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# ---- 設定 ----\n",
    "OUT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_npy\")   # 你的特徵輸出根目錄\n",
    "NEW_PREFIX = \"COCO2017\"                    # 新檔名前綴\n",
    "# 你的產生器以前大多會用到這些前綴（可視需要增減）\n",
    "MATCH_PREFIXES = (\"coco2017_ge512\", \"coco_ge512\", \"coco2017\", \"COCO2017\")\n",
    "DRY_RUN = False                            # True=只預覽不改名\n",
    "START_INDEX = 1                            # 編號起始\n",
    "PAD_WIDTH = None                           # None=自動位數；或手動給 6 → 000001\n",
    "\n",
    "# ---- 收集目標檔案（僅限 *_npy 資料夾）----\n",
    "assert OUT_ROOT.exists(), f\"找不到目錄：{OUT_ROOT}\"\n",
    "all_npy = [p for p in OUT_ROOT.rglob(\"*.npy\") if p.parent.name.endswith(\"_npy\")]\n",
    "\n",
    "def is_coco_stem(stem: str) -> bool:\n",
    "    up = stem.upper()\n",
    "    return any(up.startswith(pref.upper()) for pref in MATCH_PREFIXES)\n",
    "\n",
    "target_files = [p for p in all_npy if is_coco_stem(p.stem)]\n",
    "if not target_files:\n",
    "    raise SystemExit(\"在 features_npy 中找不到 COCO2017 相關的 .npy；請確認 MATCH_PREFIXES 或路徑。\")\n",
    "\n",
    "# ---- 建立「舊 stem → 新 stem」的映射（跨特徵一致）----\n",
    "unique_stems = sorted({p.stem for p in target_files})\n",
    "total = len(unique_stems)\n",
    "pad = PAD_WIDTH or max(4, len(str(total)))\n",
    "mapping = {\n",
    "    stem: f\"{NEW_PREFIX}__{idx:0{pad}d}\"\n",
    "    for idx, stem in enumerate(unique_stems, start=START_INDEX)\n",
    "}\n",
    "\n",
    "# ---- 規劃改名 & 檢查衝突 ----\n",
    "plan, conflicts = [], []\n",
    "for src in target_files:\n",
    "    new_stem = mapping[src.stem]\n",
    "    dst = src.with_name(new_stem + src.suffix)\n",
    "    plan.append((src, dst))\n",
    "    if dst.exists() and dst != src:\n",
    "        conflicts.append((src, dst))\n",
    "\n",
    "print(f\"將改名 {len(plan)} 個檔案；唯一影像數 {len(unique_stems)}；零填位數 {pad}。\")\n",
    "if conflicts:\n",
    "    print(\"⚠️ 偵測到將覆蓋既有檔案，為安全起見先中止。衝突範例：\")\n",
    "    for s, d in conflicts[:10]:\n",
    "        print(\" -\", d)\n",
    "    raise SystemExit(\"請先處理衝突或調整 MATCH_PREFIXES。\")\n",
    "\n",
    "# ---- 輸出對照表 CSV（可拿來回復舊名）----\n",
    "map_csv = OUT_ROOT / f\"COCO2017_rename_map_{pad}d.csv\"\n",
    "with open(map_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"old_path\",\"new_path\",\"old_stem\",\"new_stem\"])\n",
    "    for s, d in plan:\n",
    "        w.writerow([str(s), str(d), s.stem, d.stem])\n",
    "print(\"📝 對照表已輸出：\", map_csv)\n",
    "\n",
    "# ---- 執行改名 ----\n",
    "if not DRY_RUN:\n",
    "    for s, d in plan:\n",
    "        s.rename(d)\n",
    "    print(\"✅ 已完成改名。\")\n",
    "else:\n",
    "    print(\"（DRY_RUN=True 僅預覽，未實際改名）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18297f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "將改名 45000 個檔案；唯一影像數 15000；零填位數 5。\n",
      "📝 對照表已輸出： /home/yaya/ai-detect-proj/Script/features_npy/PLACES365_rename_map_5d.csv\n",
      "✅ 已完成改名。\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Rename Places365 npy → \"PLACES365__000001.npy\"（跨特徵共享編號）\n",
    "# ======================================================\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# ---- 設定 ----\n",
    "OUT_ROOT = Path(\"/home/yaya/ai-detect-proj/Script/features_npy\")   # 你的特徵輸出根目錄\n",
    "NEW_PREFIX = \"PLACES365\"                   # 新檔名前綴\n",
    "# 依你前面兩種 Places 產生器的前綴做匹配（可自行增減）\n",
    "MATCH_PREFIXES = (\n",
    "    \"Places365_valHR_15k_gt512\",\n",
    "    \"Places365_val256_15k\",\n",
    "    \"Places365\",\n",
    "    \"PLACES365\",\n",
    ")\n",
    "DRY_RUN = False                            # True=只預覽不改名\n",
    "START_INDEX = 1                            # 編號起始\n",
    "PAD_WIDTH = None                           # None=自動位數；或手動給 6 → 000001\n",
    "\n",
    "# ---- 收集目標檔案（僅限 *_npy 資料夾）----\n",
    "assert OUT_ROOT.exists(), f\"找不到目錄：{OUT_ROOT}\"\n",
    "all_npy = [p for p in OUT_ROOT.rglob(\"*.npy\") if p.parent.name.endswith(\"_npy\")]\n",
    "\n",
    "def is_places_stem(stem: str) -> bool:\n",
    "    up = stem.upper()\n",
    "    return any(up.startswith(pref.upper()) for pref in MATCH_PREFIXES)\n",
    "\n",
    "target_files = [p for p in all_npy if is_places_stem(p.stem)]\n",
    "if not target_files:\n",
    "    raise SystemExit(\"在 features_npy 中找不到 Places365 相關的 .npy；請確認 MATCH_PREFIXES 或路徑。\")\n",
    "\n",
    "# ---- 建立「舊 stem → 新 stem」的映射（跨特徵一致）----\n",
    "unique_stems = sorted({p.stem for p in target_files})\n",
    "total = len(unique_stems)\n",
    "pad = PAD_WIDTH or max(4, len(str(total)))\n",
    "mapping = {\n",
    "    stem: f\"{NEW_PREFIX}__{idx:0{pad}d}\"\n",
    "    for idx, stem in enumerate(unique_stems, start=START_INDEX)\n",
    "}\n",
    "\n",
    "# ---- 規劃改名 & 檢查衝突 ----\n",
    "plan, conflicts = [], []\n",
    "for src in target_files:\n",
    "    new_stem = mapping[src.stem]\n",
    "    dst = src.with_name(new_stem + src.suffix)\n",
    "    plan.append((src, dst))\n",
    "    if dst.exists() and dst != src:\n",
    "        conflicts.append((src, dst))\n",
    "\n",
    "print(f\"將改名 {len(plan)} 個檔案；唯一影像數 {len(unique_stems)}；零填位數 {pad}。\")\n",
    "if conflicts:\n",
    "    print(\"⚠️ 偵測到將覆蓋既有檔案，為安全起見先中止。衝突範例：\")\n",
    "    for s, d in conflicts[:10]:\n",
    "        print(\" -\", d)\n",
    "    raise SystemExit(\"請先處理衝突或調整 MATCH_PREFIXES。\")\n",
    "\n",
    "# ---- 輸出對照表 CSV（可拿來回復舊名）----\n",
    "map_csv = OUT_ROOT / f\"PLACES365_rename_map_{pad}d.csv\"\n",
    "with open(map_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"old_path\",\"new_path\",\"old_stem\",\"new_stem\"])\n",
    "    for s, d in plan:\n",
    "        w.writerow([str(s), str(d), s.stem, d.stem])\n",
    "print(\"📝 對照表已輸出：\", map_csv)\n",
    "\n",
    "# ---- 執行改名 ----\n",
    "if not DRY_RUN:\n",
    "    for s, d in plan:\n",
    "        s.rename(d)\n",
    "    print(\"✅ 已完成改名。\")\n",
    "else:\n",
    "    print(\"（DRY_RUN=True 僅預覽，未實際改名）\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
