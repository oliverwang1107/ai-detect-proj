{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d4e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CLIP: cuML LogisticRegression（logit）\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Slide-256 Full-Image Inference (PRNU-CNN / ELA-CNN / CLIP-LogReg + LR Fusion)\n",
    "-----------------------------------------------------------------------------\n",
    "目標：用 256×256 覆蓋整張圖（可重疊），對每一塊做推論，輸出：\n",
    "  1) 每塊的 logit 與機率、座標、預測\n",
    "  2) 疊加可視化（格子上色 + 半透明熱度覆蓋）\n",
    "  3) 整張圖的彙總結果（多種匯總策略）\n",
    "\n",
    "特點：\n",
    "- 非 .jpg 來源會先做臨時 .jpg（EXIF 方向修正、RGB、4:4:4, Q=95），以利 ELA/CLIP；\n",
    "  PRNU 直接用 numpy tile 計算，不受 jpg 轉檔影響。\n",
    "- CLIP 針對每個 tile 直接用 256×256 JPG（4:4:4, Q=95），與 Center-256 規格一致。\n",
    "- 融合順序固定 [PRNU, ELA, CLIP]，優先用 sklearn LR（fusion_lr.pkl），否則 fallback 權重。\n",
    "\n",
    "可直接放到 Jupyter；下方 __main__ 給出用法範例。\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# CUDA/encoding 建議\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "os.environ.setdefault(\"PYTHONUTF8\",\"1\")\n",
    "os.environ.setdefault(\"PYTHONIOENCODING\",\"utf-8\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageChops, ImageOps, ImageDraw, ImageFont\n",
    "from skimage import io as skio\n",
    "from skimage.util import img_as_float32\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "# 路徑（照你的專案）\n",
    "SCRIPT_ROOT     = Path(\"/home/yaya/ai-detect-proj/Script\")\n",
    "SAVED_MODELS    = SCRIPT_ROOT / \"saved_models\"\n",
    "\n",
    "PRNU_MODEL_PATH = SAVED_MODELS / \"prnu_fastcnn_best.pt\"\n",
    "ELA_MODEL_PATH  = SAVED_MODELS / \"ela_fastcnn_best.pt\"\n",
    "\n",
    "CLIP_LOGREG_PKL = SAVED_MODELS / \"clip_logreg_gpu.pkl\"    # cuML LogisticRegression（建議）\n",
    "CLIP_SVM_CUML   = SAVED_MODELS / \"clip_svm_gpu.pkl\"       # cuML SVM（回退）\n",
    "CLIP_SVM_TORCH  = SAVED_MODELS / \"clip_svm_gpu_torch.pt\"  # Torch 線性 SVM（回退）\n",
    "CLIP_PLATT_PKL  = SAVED_MODELS / \"clip_platt.pkl\"         #（可選）Platt 標定\n",
    "\n",
    "FUSER_PKL       = SAVED_MODELS / \"fusion_lr.pkl\"\n",
    "FUSER_META      = SAVED_MODELS / \"fusion_lr_meta.json\"\n",
    "\n",
    "# 參數\n",
    "SEED            = 42\n",
    "TILE            = 256      # tile 尺寸\n",
    "STRIDE          = 128      # 步長（128 = 50% 重疊）\n",
    "ELA_QUALITY     = 90\n",
    "ELA_SCALE       = 15\n",
    "ELA_FEASZ       = 128\n",
    "PRNU_MODE       = \"soft\"\n",
    "PRNU_WAVELET    = \"db8\"\n",
    "PRNU_Q_MODE     = \"per_file\"\n",
    "PRNU_Q_PERC     = 0.999\n",
    "PRNU_Q_SAMPLES  = 4096\n",
    "\n",
    "FORCE_JPG_NONJPG = True\n",
    "FORCE_JPG_FOR    = {\"ela\": True, \"clip\": True, \"prnu\": False}  # prnu 不依賴 jpg\n",
    "\n",
    "JPEG_FORCE_QUALITY     = 95\n",
    "JPEG_FORCE_SUBSAMPLING = 0     # 4:4:4\n",
    "\n",
    "CLIP_BACKBONE   = \"ViT-L-14\"\n",
    "CLIP_PRETRAINED = {\"ViT-L-14\":\"laion2b_s32b_b82k\",\"ViT-B-32\":\"laion400m_e32\",\"ViT-L-14-336\":\"laion2b_s32b_b82k\"}.get(CLIP_BACKBONE, \"laion2b_s32b_b82k\")\n",
    "\n",
    "# 裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========= 小工具 =========\n",
    "def _safe_logit(p: float, eps: float = 1e-6, clamp: float = 20.0) -> float:\n",
    "    p = float(np.clip(p, eps, 1.0 - eps))\n",
    "    z = math.log(p) - math.log(1.0 - p)\n",
    "    return float(np.clip(z, -clamp, clamp))\n",
    "\n",
    "def as_jpg_if_needed(p: Path,\n",
    "                     quality: int = JPEG_FORCE_QUALITY,\n",
    "                     subsampling: int | str = JPEG_FORCE_SUBSAMPLING) -> Tuple[Path, str | None]:\n",
    "    p = Path(p)\n",
    "    if p.suffix.lower() == \".jpg\":\n",
    "        return p, None\n",
    "    img = Image.open(p)\n",
    "    try:\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    tmp = p.with_suffix(f\".tmp_infer_{uuid.uuid4().hex[:8]}.jpg\")\n",
    "    buf = BytesIO(); img.save(buf, format=\"JPEG\", quality=int(quality), subsampling=subsampling)\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(buf.getvalue())\n",
    "    return tmp, str(tmp)\n",
    "\n",
    "# ========= PRNU/ELA CNN =========\n",
    "class DSBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(c_in, c_in, 3, stride=stride, padding=1, groups=c_in, bias=False)\n",
    "        self.bn1= nn.BatchNorm2d(c_in)\n",
    "        self.pw = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn2= nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.dw(x)))\n",
    "        x = self.act(self.bn2(self.pw(x)))\n",
    "        return x\n",
    "\n",
    "class FastCNN_1ch(nn.Module):\n",
    "    def __init__(self, base=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(nn.Conv2d(1, base, 3, padding=1, bias=False),\n",
    "                                  nn.BatchNorm2d(base), nn.ReLU(inplace=True))\n",
    "        self.stage= nn.Sequential(DSBlock(base,base*2,1), DSBlock(base*2,base*4,2),\n",
    "                                  DSBlock(base*4,base*4,1), DSBlock(base*4,base*8,2),\n",
    "                                  DSBlock(base*8,base*8,1))\n",
    "        self.head = nn.Sequential(nn.Conv2d(base*8, base*8, 1, bias=False),\n",
    "                                  nn.BatchNorm2d(base*8), nn.ReLU(inplace=True))\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1); self.fc = nn.Linear(base*8,2)\n",
    "    def forward(self, x):\n",
    "        x=self.stem(x); x=self.stage(x); x=self.head(x); x=self.pool(x).flatten(1); return self.fc(x)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _safe_load(path, map_location=\"cpu\"):\n",
    "    try:\n",
    "        return torch.load(path, map_location=map_location, weights_only=True)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=map_location)\n",
    "\n",
    "def _extract_state_dict(blob):\n",
    "    if isinstance(blob, dict):\n",
    "        for k in (\"state_dict\",\"model\",\"weights\"):\n",
    "            if k in blob and isinstance(blob[k], dict):\n",
    "                sd = blob[k]; break\n",
    "        else:\n",
    "            sd = {k:v for k,v in blob.items() if torch.is_tensor(v)}\n",
    "            if not sd: raise RuntimeError(\"未知 checkpoint 格式\")\n",
    "    elif isinstance(blob, torch.nn.Module):\n",
    "        sd = blob.state_dict()\n",
    "    else:\n",
    "        sd = blob\n",
    "    if len(sd)>0 and next(iter(sd)).startswith(\"module.\"):\n",
    "        sd = OrderedDict((k[len(\"module.\"):], v) for k,v in sd.items())\n",
    "    return sd\n",
    "\n",
    "prnu_model = FastCNN_1ch().to(device).eval()\n",
    "ela_model  = FastCNN_1ch().to(device).eval()\n",
    "prnu_model.load_state_dict(_extract_state_dict(_safe_load(PRNU_MODEL_PATH, device)))\n",
    "ela_model .load_state_dict(_extract_state_dict(_safe_load(ELA_MODEL_PATH,  device)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def prnu_logit_from_i8(arr_i8: np.ndarray) -> float:\n",
    "    a = arr_i8\n",
    "    if a.dtype == np.int8:\n",
    "        x = a.astype(np.float32) / 127.0\n",
    "    elif a.dtype == np.uint8:\n",
    "        x = a.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        x = a.astype(np.float32); x = x/255.0 if x.max()>1.5 else x\n",
    "    x = x - x.mean()\n",
    "    t = torch.from_numpy(x[None,None,...]).to(device)\n",
    "    p1 = torch.softmax(prnu_model(t), dim=1)[0,1].item()\n",
    "    return _safe_logit(p1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ela_logit_from_i8(arr_i8: np.ndarray) -> float:\n",
    "    a = arr_i8\n",
    "    if a.dtype == np.int8:\n",
    "        x = (a.astype(np.float32) + 128.0) / 255.0  # offset128\n",
    "    elif a.dtype == np.uint8:\n",
    "        x = a.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        x = a.astype(np.float32); x = x/255.0 if x.max()>1.5 else x\n",
    "    x = x - x.mean()\n",
    "    t = torch.from_numpy(x[None,None,...]).to(device)\n",
    "    p1 = torch.softmax(ela_model(t), dim=1)[0,1].item()\n",
    "    return _safe_logit(p1)\n",
    "\n",
    "# ========= open_clip + CLIP 分支 =========\n",
    "_openclip = {\"model\":None,\"pre\":None,\"dev\":\"cpu\"}\n",
    "\n",
    "def _cuml_ok():\n",
    "    try:\n",
    "        import cuml, cupy  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_openclip():\n",
    "    if _openclip[\"model\"] is None:\n",
    "        import open_clip\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model, _, pre = open_clip.create_model_and_transforms(CLIP_BACKBONE, pretrained=CLIP_PRETRAINED)\n",
    "        model = model.to(dev).eval()\n",
    "        _openclip.update(model=model, pre=pre, dev=dev)\n",
    "    return _openclip[\"model\"], _openclip[\"pre\"], _openclip[\"dev\"]\n",
    "\n",
    "class CLIPSingle:\n",
    "    def __init__(self):\n",
    "        self.mode=None; self.backend=None; self.model=None; self.D=None; self.platt=None\n",
    "        self._load()\n",
    "    def _load(self):\n",
    "        # cuML LR（優先）\n",
    "        if CLIP_LOGREG_PKL.exists():\n",
    "            import joblib\n",
    "            obj = joblib.load(CLIP_LOGREG_PKL)\n",
    "            self.model = obj[\"model\"] if isinstance(obj, dict) and \"model\" in obj else obj\n",
    "            self.mode, self.backend, self.D = \"logreg\", \"cuml\", 1024\n",
    "            print(\"✅ CLIP: cuML LogisticRegression（logit）\"); return\n",
    "        # cuML SVM（回退）\n",
    "        if CLIP_SVM_CUML.exists() and _cuml_ok():\n",
    "            import joblib\n",
    "            clf = joblib.load(CLIP_SVM_CUML)\n",
    "            self.model = clf[\"model\"] if isinstance(clf, dict) and \"model\" in clf else clf\n",
    "            self.mode, self.backend, self.D = \"svm\", \"cuml\", 1024\n",
    "            print(\"⚠️ CLIP: cuML SVM（margin）\")\n",
    "        # Torch 線性 SVM（回退）\n",
    "        elif CLIP_SVM_TORCH.exists():\n",
    "            sd = _safe_load(CLIP_SVM_TORCH, \"cpu\")\n",
    "            state = sd[\"state_dict\"] if isinstance(sd, dict) and \"state_dict\" in sd else sd\n",
    "            self.D = int(sd[\"D\"]) if isinstance(sd, dict) and \"D\" in sd else state[\"weight\"].shape[1]\n",
    "            lin = nn.Linear(self.D, 1, bias=True).eval(); lin.load_state_dict(state)\n",
    "            self.model = lin; self.mode, self.backend = \"svm\", \"torch\"\n",
    "            print(f\"⚠️ CLIP: Torch SVM（margin, D={self.D}）\")\n",
    "        else:\n",
    "            print(\"ℹ️ 未找到 CLIP 分類器（跳過 CLIP）\")\n",
    "            return\n",
    "        # 可選：Platt\n",
    "        if self.mode == \"svm\" and CLIP_PLATT_PKL.exists():\n",
    "            import joblib\n",
    "            try:\n",
    "                self.platt = joblib.load(CLIP_PLATT_PKL)\n",
    "                print(\"✅ 載入 Platt 標定器（SVM → prob）\")\n",
    "            except Exception as e:\n",
    "                print(\"⚠️ Platt 載入失敗：\", e)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_logit(self, pil_tile_256: Image.Image) -> float | None:\n",
    "        if self.model is None: return None\n",
    "        # 直接把 256×256 tile 另存為臨時 JPG，保持與 Center-256 一致\n",
    "        tmp = Path(f\"/tmp/clip_tile_{uuid.uuid4().hex[:8]}.jpg\")\n",
    "        buf = BytesIO(); pil_tile_256.save(buf, format=\"JPEG\", quality=95, subsampling=0)\n",
    "        with open(tmp, \"wb\") as f: f.write(buf.getvalue())\n",
    "        try:\n",
    "            model, pre, dev = load_openclip()\n",
    "            img = Image.open(tmp).convert(\"RGB\")\n",
    "            im  = pre(img).unsqueeze(0).to(dev)\n",
    "            visual = model.visual\n",
    "            tokens=None\n",
    "            def _apply_norm(x):\n",
    "                if x.ndim==2: x=x.unsqueeze(0)\n",
    "                if hasattr(visual, \"ln_post\") and visual.ln_post is not None:\n",
    "                    x = visual.ln_post(x)\n",
    "                elif hasattr(visual, \"trunk\") and hasattr(visual.trunk, \"norm\") and visual.trunk.norm is not None:\n",
    "                    x = visual.trunk.norm(x)\n",
    "                return x.squeeze(0)\n",
    "            # 嘗試抓 token map\n",
    "            ok=False\n",
    "            for try_path in (\"trunk.forward_features\",\"visual.forward_features\",\"hook\"):\n",
    "                try:\n",
    "                    if try_path==\"trunk.forward_features\" and hasattr(visual,\"trunk\") and hasattr(visual.trunk,\"forward_features\"):\n",
    "                        out = visual.trunk.forward_features(im)\n",
    "                        if isinstance(out,(tuple,list)): out=out[0]\n",
    "                        if isinstance(out,dict): out=out.get(\"x\", out.get(\"tokens\", out))\n",
    "                        if torch.is_tensor(out) and out.ndim==3:\n",
    "                            tokens=_apply_norm(out.detach()); ok=True; break\n",
    "                    if try_path==\"visual.forward_features\" and hasattr(visual,\"forward_features\"):\n",
    "                        out = visual.forward_features(im)\n",
    "                        if isinstance(out,(tuple,list)): out=out[0]\n",
    "                        if isinstance(out,dict): out=out.get(\"x\", out.get(\"tokens\", out))\n",
    "                        if torch.is_tensor(out) and out.ndim==3:\n",
    "                            tokens=_apply_norm(out.detach()); ok=True; break\n",
    "                    if try_path==\"hook\":\n",
    "                        feats={}; h=None\n",
    "                        try:\n",
    "                            if hasattr(visual,\"trunk\") and hasattr(visual.trunk,\"blocks\") and len(visual.trunk.blocks)>0:\n",
    "                                target=visual.trunk.blocks[-1]\n",
    "                                def _hook(_m,_i,o): feats[\"x\"]=o\n",
    "                                h=target.register_forward_hook(_hook)\n",
    "                                _ = getattr(visual.trunk, \"forward_features\", visual.trunk.forward)(im)\n",
    "                            elif hasattr(visual,\"transformer\") and hasattr(visual.transformer,\"resblocks\") and len(visual.transformer.resblocks)>0:\n",
    "                                target=visual.transformer.resblocks[-1]\n",
    "                                def _hook(_m,_i,o): feats[\"x\"]=o\n",
    "                                h=target.register_forward_hook(_hook); _=model.encode_image(im)\n",
    "                        finally:\n",
    "                            if h is not None:\n",
    "                                try: h.remove()\n",
    "                                except Exception: pass\n",
    "                        x=feats.get(\"x\",None)\n",
    "                        if torch.is_tensor(x) and x.ndim==3:\n",
    "                            tokens=_apply_norm(x.detach()); ok=True; break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not ok:\n",
    "                # 保底：取 ln_post CLS（這裡沒有分類器時不返回）\n",
    "                return None\n",
    "            tokens = tokens.float().cpu().numpy().astype(np.float32)\n",
    "            if tokens.shape[0] <= 1: return None\n",
    "            pooled = tokens[1:].mean(axis=0)\n",
    "            pooled /= (np.linalg.norm(pooled)+1e-12)\n",
    "            # 決策\n",
    "            if self.backend == \"cuml\":\n",
    "                import cupy as cp\n",
    "                s = self.model.decision_function(cp.asarray(pooled[None,:])).get().astype(np.float32)[0]\n",
    "                return float(np.clip(s, -20.0, 20.0))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    s = self.model(torch.from_numpy(pooled[None,:]).float()).squeeze(1).numpy().astype(np.float32)[0]\n",
    "                if self.mode == \"svm\" and self.platt is not None:\n",
    "                    from sklearn.linear_model import LogisticRegression  # noqa\n",
    "                    import numpy as _np\n",
    "                    p = float(self.platt.predict_proba(_np.array(s, _np.float32).reshape(1,1))[:,1][0])\n",
    "                    return _safe_logit(p)\n",
    "                return float(np.clip(s, -20.0, 20.0))\n",
    "        finally:\n",
    "            try: os.unlink(tmp)\n",
    "            except Exception: pass\n",
    "\n",
    "clip_single = CLIPSingle()\n",
    "\n",
    "# ========= 融合器 =========\n",
    "def read_json_utf8(p: Path):\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_fuser_and_meta():\n",
    "    meta=None; kind=\"dummy\"; fuser=None\n",
    "    try:\n",
    "        if FUSER_PKL.exists():\n",
    "            import joblib\n",
    "            fuser = joblib.load(FUSER_PKL); kind=\"lr\"\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ fusion_lr.pkl 載入失敗：\", e)\n",
    "    if FUSER_META.exists():\n",
    "        try: meta = read_json_utf8(FUSER_META)\n",
    "        except Exception as e: print(\"⚠️ fusion_lr_meta.json 讀取失敗：\", e)\n",
    "    return kind, fuser, meta\n",
    "\n",
    "fuser_kind, fuser, fmeta = load_fuser_and_meta()\n",
    "\n",
    "def fuse_logits(logits_dict: Dict[str, float | None]) -> float:\n",
    "    keys = [\"prnu\",\"ela\",\"clip\"]\n",
    "    z = [float(np.nan_to_num(logits_dict.get(k,0.0), nan=0.0, posinf=20.0, neginf=-20.0)) for k in keys]\n",
    "    X = np.array([z], np.float32)\n",
    "    if fuser_kind == \"lr\":\n",
    "        try:\n",
    "            proba = fuser.predict_proba(X)[:,1][0]\n",
    "            return float(np.clip(proba, 1e-6, 1-1e-6))\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ 融合器失敗，fallback：\", e)\n",
    "    if fmeta and \"weights_if_no_sklearn\" in fmeta:\n",
    "        w = np.array(fmeta[\"weights_if_no_sklearn\"], np.float32); w = w/(w.sum()+1e-12)\n",
    "    else:\n",
    "        w = np.array([1/3,1/3,1/3], np.float32)\n",
    "    zz = float(np.clip(np.dot(X[0], w), -20.0, 20.0))\n",
    "    return float(1.0 / (1.0 + np.exp(-zz)))\n",
    "\n",
    "# ========= 特徵抽取（tile 級） =========\n",
    "def _to_int8_offset128_from_01(x01: np.ndarray) -> np.ndarray:\n",
    "    u8 = np.rint(np.clip(x01, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    return (u8.astype(np.int16) - 128).astype(np.int8)\n",
    "\n",
    "def ela_i8_from_tile(pil_tile_256: Image.Image) -> np.ndarray:\n",
    "    buf = BytesIO(); pil_tile_256.save(buf, format=\"JPEG\", quality=int(ELA_QUALITY), subsampling=0, optimize=False)\n",
    "    buf.seek(0)\n",
    "    diff = ImageChops.difference(pil_tile_256, Image.open(buf)).point(lambda x: x * ELA_SCALE)\n",
    "    diff = diff.convert(\"L\").resize((ELA_FEASZ, ELA_FEASZ))\n",
    "    arr01 = np.asarray(diff, dtype=np.float32) / 255.0\n",
    "    return _to_int8_offset128_from_01(arr01)\n",
    "\n",
    "def prnu_i8_from_tile(np_tile_rgb: np.ndarray) -> np.ndarray:\n",
    "    if np_tile_rgb.ndim == 2:\n",
    "        np_tile_rgb = np.repeat(np_tile_rgb[...,None], 3, axis=-1)\n",
    "    gray = np_tile_rgb.mean(axis=2, dtype=np.float32)\n",
    "    try:\n",
    "        den = denoise_wavelet(gray, channel_axis=None, mode=PRNU_MODE, wavelet=PRNU_WAVELET, convert2ycbcr=False)\n",
    "    except TypeError:\n",
    "        den = denoise_wavelet(gray, multichannel=False, mode=PRNU_MODE, wavelet=PRNU_WAVELET, convert2ycbcr=False)\n",
    "    residual = gray - den\n",
    "    residual -= residual.mean()\n",
    "    if PRNU_Q_MODE == \"per_file\":\n",
    "        v = residual.reshape(-1).astype(np.float32, copy=False)\n",
    "        if v.size > PRNU_Q_SAMPLES:\n",
    "            rng = np.random.default_rng(SEED)\n",
    "            idx = rng.integers(0, v.size, size=PRNU_Q_SAMPLES, endpoint=False)\n",
    "            v = np.abs(v[idx])\n",
    "        else:\n",
    "            v = np.abs(v)\n",
    "        k = int(PRNU_Q_PERC * max(1, v.size-1))\n",
    "        S = float(max(1e-8, np.partition(v, k)[k]))\n",
    "    else:\n",
    "        S = max(1e-6, float(np.std(residual)) * 6.0)\n",
    "    x = np.clip(residual, -S, S) / S * 127.0\n",
    "    q = np.rint(x).astype(np.int16)\n",
    "    q = np.clip(q, -127, 127).astype(np.int8)\n",
    "    return q\n",
    "\n",
    "# ========= 覆蓋座標 =========\n",
    "def make_grid(w: int, h: int, tile: int = TILE, stride: int = STRIDE) -> List[Tuple[int,int,int,int]]:\n",
    "    xs = list(range(0, max(1, w - tile + 1), stride))\n",
    "    ys = list(range(0, max(1, h - tile + 1), stride))\n",
    "    if xs[-1] != w - tile: xs.append(max(0, w - tile))\n",
    "    if ys[-1] != h - tile: ys.append(max(0, h - tile))\n",
    "    coords = [(x, y, tile, tile) for y in ys for x in xs]\n",
    "    return coords\n",
    "\n",
    "# ========= 可視化 =========\n",
    "# --- 量文字大小：兼容 Pillow 舊新版本 ---\n",
    "def _text_size(draw, text, font):\n",
    "    # Pillow ≥10\n",
    "    if hasattr(draw, \"textbbox\"):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        return (r - l), (b - t)\n",
    "    # 後備：有些版本在 font 物件上\n",
    "    if hasattr(font, \"getbbox\"):\n",
    "        l, t, r, b = font.getbbox(text)\n",
    "        return (r - l), (b - t)\n",
    "    # 舊版\n",
    "    if hasattr(font, \"getsize\"):\n",
    "        return font.getsize(text)\n",
    "    # 最後的保底估計\n",
    "    return (8 * len(text), 12)\n",
    "\n",
    "def overlay_tiles(base_img: Image.Image,\n",
    "                  tiles: list[dict],\n",
    "                  alpha: float = 0.35,\n",
    "                  draw_frame: bool = True,\n",
    "                  show_score: bool = True) -> Image.Image:\n",
    "    W, H = base_img.size\n",
    "\n",
    "    # 以每像素平均機率做熱度圖\n",
    "    heat = np.zeros((H, W), dtype=np.float32)\n",
    "    cnt  = np.zeros((H, W), dtype=np.float32)\n",
    "    for t in tiles:\n",
    "        x, y, w, h = t['x'], t['y'], t['w'], t['h']\n",
    "        p = float(t['prob_fake'])\n",
    "        heat[y:y+h, x:x+w] += p\n",
    "        cnt [y:y+h, x:x+w] += 1.0\n",
    "    cnt[cnt == 0] = 1.0\n",
    "    heat = heat / cnt\n",
    "    heat_u8 = np.rint(np.clip(heat, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "\n",
    "    # 轉為紅色熱圖（R=heat, G=0, B=0）\n",
    "    heat_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "    heat_rgb[..., 0] = heat_u8\n",
    "    base_rgb = base_img.convert('RGB')\n",
    "    heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
    "\n",
    "    # 混合後轉 RGBA，方便畫半透明標籤\n",
    "    overlay = Image.blend(base_rgb, heat_img, alpha).convert('RGBA')\n",
    "    draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "\n",
    "    # 字型\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 16)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    if draw_frame or show_score:\n",
    "        for t in tiles:\n",
    "            x, y, w, h = t['x'], t['y'], t['w'], t['h']\n",
    "            p  = float(t['prob_fake'])\n",
    "            lab = int(t['pred_label'])\n",
    "            color = (255, 0, 0, 255) if lab == 1 else (0, 255, 0, 255)\n",
    "\n",
    "            if draw_frame:\n",
    "                draw.rectangle([x, y, x + w, y + h], outline=color, width=2)\n",
    "\n",
    "            if show_score:\n",
    "                txt = f\"{p:.2f}\"\n",
    "                tw, th = _text_size(draw, txt, font)\n",
    "                # 黑底半透明文字框\n",
    "                draw.rectangle([x, y, x + tw + 6, y + th + 4], fill=(0, 0, 0, 127))\n",
    "                draw.text((x + 3, y + 2), txt, fill=(255, 255, 255, 255), font=font)\n",
    "\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# ========= 主流程：整張圖（tile）推論 =========\n",
    "def infer_image_by_tiles(img_path: str | Path,\n",
    "                         tile: int = TILE,\n",
    "                         stride: int = STRIDE,\n",
    "                         aggregate: str = 'mean_prob',  # 'mean_prob'|'max_prob'|'topk_mean:0.2'|'mean_logit'\n",
    "                         use_clip: bool = True,\n",
    "                         save_overlay: bool = True,\n",
    "                         overlay_alpha: float = 0.35) -> Dict[str, Any]:\n",
    "    p = Path(img_path)\n",
    "    tmp_files = []\n",
    "    # 原圖（僅供 ELA/CLIP 的 jpg 流程需要；PRNU 走 numpy tile）\n",
    "    if FORCE_JPG_NONJPG:\n",
    "        p_jpg, t1 = as_jpg_if_needed(p); \n",
    "        if t1: tmp_files.append(t1)\n",
    "    else:\n",
    "        p_jpg = p\n",
    "\n",
    "    # 讀圖（給 tile 裁切）\n",
    "    img_pil = Image.open(p_jpg).convert('RGB')\n",
    "    W,H = img_pil.size\n",
    "    coords = make_grid(W, H, tile=tile, stride=stride)\n",
    "\n",
    "    tiles_out: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 也準備一份 numpy（PRNU 用），以原始檔路徑讀，避免重壓縮影響\n",
    "    np_img = np.asarray(Image.open(p).convert('RGB'), dtype=np.uint8)\n",
    "\n",
    "    for (x,y,w,h) in coords:\n",
    "        pil_tile = img_pil.crop((x,y,x+w,y+h))\n",
    "        np_tile  = np.asarray(np_img[y:y+h, x:x+w, :], dtype=np.uint8)\n",
    "\n",
    "        # 特徵\n",
    "        prnu_i8 = prnu_i8_from_tile(np_tile)\n",
    "        ela_i8  = ela_i8_from_tile(pil_tile)\n",
    "\n",
    "        # 三路 logit\n",
    "        z_prnu = prnu_logit_from_i8(prnu_i8)\n",
    "        z_ela  = ela_logit_from_i8(ela_i8)\n",
    "        z_clip = clip_single.predict_logit(pil_tile) if (use_clip and clip_single.model is not None) else None\n",
    "\n",
    "        prob_fake = fuse_logits({\"prnu\": z_prnu, \"ela\": z_ela, \"clip\": z_clip})\n",
    "        pred = int(prob_fake >= 0.5)\n",
    "\n",
    "        tiles_out.append({\n",
    "            \"x\":x, \"y\":y, \"w\":w, \"h\":h,\n",
    "            \"prnu_logit\": float(z_prnu),\n",
    "            \"ela_logit\":  float(z_ela),\n",
    "            \"clip_logit\": (None if z_clip is None else float(z_clip)),\n",
    "            \"prob_fake\": float(prob_fake),\n",
    "            \"pred_label\": pred,\n",
    "        })\n",
    "\n",
    "    # 匯總策略\n",
    "    probs = np.array([t['prob_fake'] for t in tiles_out], np.float32)\n",
    "    if aggregate == 'mean_prob':\n",
    "        whole_prob = float(np.clip(probs.mean() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "    elif aggregate == 'max_prob':\n",
    "        whole_prob = float(np.clip(probs.max() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "    elif aggregate.startswith('topk_mean'):\n",
    "        try:\n",
    "            frac = float(aggregate.split(':',1)[1]) if ':' in aggregate else 0.2\n",
    "        except Exception:\n",
    "            frac = 0.2\n",
    "        k = max(1, int(len(probs) * frac))\n",
    "        idx = np.argsort(-probs)[:k]\n",
    "        whole_prob = float(np.clip(probs[idx].mean(), 1e-6, 1-1e-6))\n",
    "    elif aggregate == 'mean_logit':\n",
    "        lgs = np.array([_safe_logit(float(p)) for p in probs], np.float32)\n",
    "        lg  = float(np.clip(lgs.mean() if len(lgs)>0 else 0.0, -20.0, 20.0))\n",
    "        whole_prob = float(1.0/(1.0+np.exp(-lg)))\n",
    "    else:\n",
    "        whole_prob = float(np.clip(probs.mean() if len(probs)>0 else 0.0, 1e-6, 1-1e-6))\n",
    "\n",
    "    whole_pred = int(whole_prob >= 0.5)\n",
    "\n",
    "    overlay_path = None\n",
    "    if save_overlay:\n",
    "        overlay_img = overlay_tiles(Image.open(p).convert('RGB'), tiles_out, alpha=overlay_alpha,\n",
    "                                    draw_frame=True, show_score=True)\n",
    "        overlay_path = str(p.with_suffix(\".tiles_overlay.png\"))\n",
    "        overlay_img.save(overlay_path)\n",
    "\n",
    "    out = {\n",
    "        \"path\": str(p),\n",
    "        \"image_size\": [H, W],\n",
    "        \"tile\": tile,\n",
    "        \"stride\": stride,\n",
    "        \"aggregate\": aggregate,\n",
    "        \"overall\": {\"prob_fake\": float(whole_prob), \"pred_label\": int(whole_pred)},\n",
    "        \"tiles\": tiles_out,\n",
    "        \"overlay_path\": overlay_path,\n",
    "    }\n",
    "\n",
    "    # 清理臨時檔\n",
    "    for t in tmp_files:\n",
    "        try: os.unlink(t)\n",
    "        except Exception: pass\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd67d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08198eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pillow-heif registered\n",
      "✅ pillow-avif registered\n",
      "HEIC/AVIF support is ready. 🎉\n"
     ]
    }
   ],
   "source": [
    "# ==== HEIC/HEIF/AVIF 支援（註冊外掛 + 強化開圖與轉JPG）====\n",
    "\n",
    "# 1) 註冊解碼器（可多次呼叫，無副作用）\n",
    "def _register_heif_avif():\n",
    "    ok = False\n",
    "    try:\n",
    "        import pillow_heif\n",
    "        pillow_heif.register_heif_opener()\n",
    "        ok = True\n",
    "        print(\"✅ pillow-heif registered\")\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ pillow-heif not available:\", e)\n",
    "    try:\n",
    "        # 匯入即完成 AVIF 註冊\n",
    "        import pillow_avif  # noqa: F401\n",
    "        from pillow_avif import AvifImagePlugin  # noqa: F401\n",
    "        ok = True or ok\n",
    "        print(\"✅ pillow-avif registered\")\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ pillow-avif not available:\", e)\n",
    "    return ok\n",
    "\n",
    "_register_heif_avif()\n",
    "\n",
    "# 2) 更穩的開圖（必要時直接用 pillow_heif 讀）\n",
    "from PIL import Image\n",
    "\n",
    "def _open_image_any(p: Path) -> Image.Image:\n",
    "    p = Path(p)\n",
    "    try:\n",
    "        img = Image.open(p)\n",
    "        img.load()  # 立刻讀入，避免延後才報錯\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        # 針對 heif/avif 再試一次（不靠 PIL opener）\n",
    "        suf = p.suffix.lower()\n",
    "        if suf in {\".heic\", \".heif\", \".heifs\", \".hif\", \".avif\"}:\n",
    "            try:\n",
    "                import pillow_heif\n",
    "                h = pillow_heif.read_heif(str(p))\n",
    "                img = Image.frombytes(h.mode, h.size, h.data, \"raw\")\n",
    "                return img\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(f\"HEIF/AVIF decode failed: {e2}\") from e2\n",
    "        raise\n",
    "\n",
    "def _to_rgb_no_alpha(img: Image.Image) -> Image.Image:\n",
    "    # 先套用 EXIF 方向\n",
    "    try:\n",
    "        from PIL import ImageOps\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 去除 alpha（白底貼上）\n",
    "    if img.mode in (\"RGBA\", \"LA\") or (img.mode == \"P\" and \"transparency\" in img.info):\n",
    "        bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        bg.paste(img.convert(\"RGBA\"), mask=img.convert(\"RGBA\").split()[-1])\n",
    "        return bg\n",
    "    # 其他色彩空間統一到 RGB\n",
    "    if img.mode != \"RGB\":\n",
    "        return img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "# 3) 取代你原本的 as_jpg_if_needed（擴充 .jpeg 與 heic/avif）\n",
    "def as_jpg_if_needed(p: Path,\n",
    "                     quality: int = 95,\n",
    "                     subsampling: int | str = 0) -> Tuple[Path, str | None]:\n",
    "    \"\"\"\n",
    "    - 若原檔已是 .jpg/.jpeg：直接回傳原路徑\n",
    "    - 否則開圖（含 HEIC/AVIF），做 EXIF 方向、去 alpha、轉 RGB，存臨時 JPG（4:4:4, Q=quality）\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    if p.suffix.lower() in {\".jpg\", \".jpeg\"}:\n",
    "        return p, None\n",
    "\n",
    "    # 用強化版開圖 + 規範成 RGB 無 alpha\n",
    "    img = _open_image_any(p)\n",
    "    img = _to_rgb_no_alpha(img)\n",
    "\n",
    "    # 輸出臨時 JPG（與你原邏輯一致：4:4:4, Q=95）\n",
    "    tmp = p.with_suffix(f\".tmp_infer_{uuid.uuid4().hex[:8]}.jpg\")\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=int(quality), subsampling=subsampling, optimize=False)\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(buf.getvalue())\n",
    "    return tmp, str(tmp)\n",
    "\n",
    "# === 也建議把 PRNU 端的讀取改用 _open_image_any，確保 HEIC 可讀 ===\n",
    "# 在 infer_image_by_tiles 裡這行：\n",
    "#   np_img = np.asarray(Image.open(p).convert('RGB'), dtype=np.uint8)\n",
    "# 換成：\n",
    "#   np_img = np.asarray(_to_rgb_no_alpha(_open_image_any(p)), dtype=np.uint8)\n",
    "\n",
    "print(\"HEIC/AVIF support is ready. 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d0e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Gradio UI with progress (drop-in) ===\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import tempfile, json, math\n",
    "\n",
    "def _aggregate_probs(probs: np.ndarray, method: str) -> float:\n",
    "    probs = np.asarray(probs, np.float32)\n",
    "    if probs.size == 0: \n",
    "        return 1e-6\n",
    "    if method == \"mean_prob\":\n",
    "        p = probs.mean()\n",
    "    elif method == \"max_prob\":\n",
    "        p = probs.max()\n",
    "    elif method.startswith(\"topk_mean\"):\n",
    "        try:\n",
    "            frac = float(method.split(\":\",1)[1]) if \":\" in method else 0.2\n",
    "        except Exception:\n",
    "            frac = 0.2\n",
    "        k = max(1, int(len(probs) * frac))\n",
    "        idx = np.argsort(-probs)[:k]\n",
    "        p = probs[idx].mean()\n",
    "    elif method == \"mean_logit\":\n",
    "        # same as你的程式：平均 logit 再 sigmoid\n",
    "        def _safe_logit(p, eps=1e-6, clamp=20.0):\n",
    "            p = float(np.clip(p, eps, 1.0 - eps))\n",
    "            z = math.log(p) - math.log(1.0 - p)\n",
    "            return float(np.clip(z, -clamp, clamp))\n",
    "        lgs = np.array([_safe_logit(float(x)) for x in probs], np.float32)\n",
    "        lg = float(np.clip(lgs.mean(), -20.0, 20.0))\n",
    "        p = float(1.0/(1.0+np.exp(-lg)))\n",
    "    else:\n",
    "        p = probs.mean()\n",
    "    return float(np.clip(p, 1e-6, 1-1e-6))\n",
    "\n",
    "def infer_stream(pil_img,\n",
    "                 tile=256, stride=128, aggregate=\"topk_mean:0.25\",\n",
    "                 use_clip=True, draw_boxes=True, show_score=True, alpha=0.35,\n",
    "                 progress=gr.Progress(track_tqdm=True)):\n",
    "    \"\"\"\n",
    "    逐步推論，每 N 個 tile 回傳一次中間結果，帶進度條。\n",
    "    輸出順序：caption(str), overlay(Image), raw(JSON)\n",
    "    \"\"\"\n",
    "    # 先把上傳圖暫存到檔案，沿用你現有的影像/轉 JPG 流程\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f:\n",
    "        pil_img.save(f.name)\n",
    "        p = Path(f.name)\n",
    "\n",
    "    try:\n",
    "        base_rgb = Image.open(p).convert(\"RGB\")\n",
    "        # ELA/CLIP 用 JPG（跟你的 code 一樣）\n",
    "        if FORCE_JPG_NONJPG:\n",
    "            p_jpg, _tmp = as_jpg_if_needed(p)\n",
    "        else:\n",
    "            p_jpg = p\n",
    "        img_pil = Image.open(p_jpg).convert(\"RGB\")\n",
    "        W, H = img_pil.size\n",
    "\n",
    "        coords = make_grid(W, H, tile=int(tile), stride=int(stride))\n",
    "        n = len(coords)\n",
    "        tiles_out = []\n",
    "\n",
    "        # PRNU 用原始圖\n",
    "        np_img = np.asarray(base_rgb, dtype=np.uint8)\n",
    "\n",
    "        # 每多少步更新一次 UI（大約 20 段）\n",
    "        every = max(1, n // 20)\n",
    "\n",
    "        progress(0.0, desc=f\"準備中… {W}×{H}, tiles={n}\")\n",
    "        for i, (x,y,w,h) in enumerate(coords):\n",
    "            # 取 tile\n",
    "            pil_tile = img_pil.crop((x,y,x+w,y+h))\n",
    "            np_tile  = np.asarray(np_img[y:y+h, x:x+w, :], dtype=np.uint8)\n",
    "\n",
    "            # 特徵 → 子模型 logit\n",
    "            prnu_i8 = prnu_i8_from_tile(np_tile)\n",
    "            ela_i8  = ela_i8_from_tile(pil_tile)\n",
    "\n",
    "            z_prnu = prnu_logit_from_i8(prnu_i8)\n",
    "            z_ela  = ela_logit_from_i8(ela_i8)\n",
    "            z_clip = clip_single.predict_logit(pil_tile) if (use_clip and clip_single.model is not None) else None\n",
    "\n",
    "            prob_fake = fuse_logits({\"prnu\": z_prnu, \"ela\": z_ela, \"clip\": z_clip})\n",
    "            pred = int(prob_fake >= 0.5)\n",
    "\n",
    "            tiles_out.append({\n",
    "                \"x\":x, \"y\":y, \"w\":w, \"h\":h,\n",
    "                \"prnu_logit\": float(z_prnu),\n",
    "                \"ela_logit\":  float(z_ela),\n",
    "                \"clip_logit\": (None if z_clip is None else float(z_clip)),\n",
    "                \"prob_fake\": float(prob_fake),\n",
    "                \"pred_label\": pred,\n",
    "            })\n",
    "\n",
    "            # 進度與中途回傳\n",
    "            if (i+1) % every == 0 or (i+1) == n:\n",
    "                progress((i+1)/n, desc=f\"推論中… ({i+1}/{n})\")\n",
    "                probs = np.array([t['prob_fake'] for t in tiles_out], np.float32)\n",
    "                whole_prob = _aggregate_probs(probs, aggregate)\n",
    "                whole_pred = int(whole_prob >= 0.5)\n",
    "\n",
    "                # 即時熱圖（用目前累積的 tiles）\n",
    "                overlay_img = overlay_tiles(base_rgb, tiles_out, alpha=float(alpha),\n",
    "                                            draw_frame=bool(draw_boxes), show_score=bool(show_score))\n",
    "\n",
    "                out = {\n",
    "                    \"path\": str(p),\n",
    "                    \"image_size\": [H, W],\n",
    "                    \"tile\": int(tile),\n",
    "                    \"stride\": int(stride),\n",
    "                    \"aggregate\": aggregate,\n",
    "                    \"overall\": {\"prob_fake\": float(whole_prob), \"pred_label\": int(whole_pred)},\n",
    "                    \"tiles_done\": len(tiles_out),\n",
    "                    \"tiles_total\": n,\n",
    "                    \"tiles\": tiles_out,   # 注意：這是累積中的 tiles（最後一次即是完整結果）\n",
    "                }\n",
    "                caption = f\"pred={whole_pred}  prob_fake={whole_prob:.3f}  ({i+1}/{n})\"\n",
    "                yield caption, overlay_img, out\n",
    "\n",
    "    finally:\n",
    "        try: p.unlink()\n",
    "        except Exception: pass\n",
    "\n",
    "\n",
    "# ===== Gradio 介面 =====\n",
    "with gr.Blocks(title=\"AI Image Detector (CUDA, with progress)\") as demo:\n",
    "    gr.Markdown(\"### Slide-256 Full-Image Inference — PRNU / ELA / CLIP + LR Fusion（含進度條）\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            inp = gr.Image(type=\"pil\", label=\"上傳影像\")\n",
    "            with gr.Row():\n",
    "                tile   = gr.Number(value=256, precision=0, label=\"Tile size\")\n",
    "                stride = gr.Number(value=128, precision=0, label=\"Stride\")\n",
    "            aggregate = gr.Dropdown(choices=[\"mean_prob\",\"max_prob\",\"topk_mean:0.2\",\"topk_mean:0.25\",\"mean_logit\"],\n",
    "                                    value=\"topk_mean:0.25\", label=\"Aggregator\")\n",
    "            use_clip   = gr.Checkbox(value=True,  label=\"Use CLIP head\")\n",
    "            draw_boxes = gr.Checkbox(value=True,  label=\"Draw tile boxes\")\n",
    "            show_score = gr.Checkbox(value=True,  label=\"Show tile score\")\n",
    "            alpha      = gr.Slider(0.0, 1.0, value=0.35, step=0.05, label=\"Overlay alpha\")\n",
    "            run = gr.Button(\"開始推論\", variant=\"primary\")\n",
    "        with gr.Column(scale=1):\n",
    "            caption = gr.Label(label=\"整體預測\")\n",
    "            overlay = gr.Image(label=\"即時疊加視覺化（會逐步更新）\")\n",
    "            raw     = gr.JSON(label=\"Raw 輸出（最後一次為完整結果）\")\n",
    "\n",
    "    run.click(\n",
    "        infer_stream,\n",
    "        inputs=[inp, tile, stride, aggregate, use_clip, draw_boxes, show_score, alpha],\n",
    "        outputs=[caption, overlay, raw]\n",
    "    )\n",
    "\n",
    "# 建議在 notebook 內用 queue() 取得平滑進度條\n",
    "demo.queue(max_size=8).launch(share=False, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11bc77a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 事後多標準彙總：不重跑模型 ===\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 同你之前小工具\n",
    "def _aggregate_probs(probs: np.ndarray, method: str) -> float:\n",
    "    probs = np.asarray(probs, np.float32)\n",
    "    if probs.size == 0: \n",
    "        return 1e-6\n",
    "    if method == \"mean_prob\":\n",
    "        p = probs.mean()\n",
    "    elif method == \"max_prob\":\n",
    "        p = probs.max()\n",
    "    elif method.startswith(\"topk_mean\"):\n",
    "        try:\n",
    "            frac = float(method.split(\":\",1)[1]) if \":\" in method else 0.5\n",
    "        except Exception:\n",
    "            frac = 0.5\n",
    "        k = max(1, int(len(probs) * frac))\n",
    "        idx = np.argsort(-probs)[:k]\n",
    "        p = probs[idx].mean()\n",
    "    elif method == \"mean_logit\":\n",
    "        def _safe_logit(p, eps=1e-6, clamp=20.0):\n",
    "            p = float(np.clip(p, eps, 1.0 - eps))\n",
    "            z = np.log(p) - np.log(1.0 - p)\n",
    "            return float(np.clip(z, -clamp, clamp))\n",
    "        lgs = np.array([_safe_logit(float(x)) for x in probs], np.float32)\n",
    "        lg = float(np.clip(lgs.mean(), -20.0, 20.0))\n",
    "        p = float(1.0/(1.0+np.exp(-lg)))\n",
    "    else:\n",
    "        p = probs.mean()\n",
    "    return float(np.clip(p, 1e-6, 1-1e-6))\n",
    "\n",
    "# 1) 跑一次完整推論（不需要 aggregate 入參）\n",
    "def run_once(img, tile, stride, use_clip, alpha):\n",
    "    res = infer_image_by_tiles(\n",
    "        img, tile=int(tile), stride=int(stride),\n",
    "        aggregate='mean_prob',           # 這裡只是占位，不影響後面多標準計算\n",
    "        use_clip=bool(use_clip),\n",
    "        save_overlay=True, overlay_alpha=float(alpha)\n",
    "    )\n",
    "    overlay = None\n",
    "    if res.get(\"overlay_path\"):\n",
    "        overlay = Image.open(res[\"overlay_path\"]).convert(\"RGB\")\n",
    "    # 提示：推論已完成，請在右側勾選彙總方式\n",
    "    caption = \"推論完成，請在右側勾選一個或多個彙總方式（不會重跑）\"\n",
    "    return caption, overlay, res\n",
    "\n",
    "# 2) 從 raw 結果（tiles）事後計算多種彙總\n",
    "def reaggregate(raw_result: dict, methods: list[str]):\n",
    "    if not raw_result or \"tiles\" not in raw_result:\n",
    "        return \"尚未有推論結果\", []\n",
    "    probs = np.array([t['prob_fake'] for t in raw_result['tiles']], np.float32)\n",
    "    table = []\n",
    "    best = (\"\", -1.0, 0)\n",
    "    for m in (methods or []):\n",
    "        p = _aggregate_probs(probs, m)\n",
    "        y = int(p >= 0.5)\n",
    "        table.append([m, round(float(p), 6), y])\n",
    "        if p > best[1]:\n",
    "            best = (m, p, y)\n",
    "    if not methods:\n",
    "        return \"請至少選擇一個彙總方式\", []\n",
    "    msg = f\"最佳：{best[0]}  prob={best[1]:.4f}  pred={best[2]}\"\n",
    "    return msg, table\n",
    "\n",
    "# === Gradio 介面（精簡、可直接用手機）===\n",
    "with gr.Blocks(title=\"AI Image Detector — 事後多標準彙總\", css=\"\"\"\n",
    "  .gradio-container {max-width: 980px !important;}\n",
    "\"\"\") as demo2:\n",
    "    gr.Markdown(\"### Slide-256 — 先推論，再多標準彙總（不重跑模型）\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            inp = gr.Image(type=\"filepath\", label=\"上傳或拍照\",\n",
    "                           sources=[\"upload\",\"webcam\",\"clipboard\"], height=320)\n",
    "            with gr.Row():\n",
    "                tile   = gr.Number(value=256, precision=0, label=\"Tile\")\n",
    "                stride = gr.Number(value=128, precision=0, label=\"Stride\")\n",
    "            use_clip = gr.Checkbox(value=True, label=\"Use CLIP head\")\n",
    "            alpha    = gr.Slider(0.0, 1.0, value=0.35, step=0.05, label=\"Overlay alpha\")\n",
    "            run_btn  = gr.Button(\"開始推論\", variant=\"primary\")\n",
    "        with gr.Column(scale=1):\n",
    "            caption = gr.Label(label=\"整體資訊 / 彙總訊息\")\n",
    "            overlay = gr.Image(label=\"Overlay（與彙總無關，僅依 tile 機率）\")\n",
    "            # 顯示原始結果（含 tiles），方便除錯 / 也提供 reaggregate 的輸入\n",
    "            raw     = gr.JSON(label=\"Raw 結果（tiles）\")\n",
    "            # 多選彙總方式（事後算）\n",
    "            methods = gr.CheckboxGroup(\n",
    "                choices=[\"mean_prob\",\"max_prob\",\"topk_mean:0.5\",\"topk_mean:0.25\",\"mean_logit\"],\n",
    "                label=\"選擇一個或多個彙總方式（不重跑）\"\n",
    "            )\n",
    "            recompute = gr.Button(\"重新計算彙總\")\n",
    "            table = gr.Dataframe(\n",
    "                headers=[\"method\",\"prob_fake\",\"pred_label\"],\n",
    "                datatype=[\"str\",\"number\",\"number\"],\n",
    "                interactive=False,\n",
    "                label=\"各彙總方式結果\"\n",
    "            )\n",
    "\n",
    "    # 跑一次推論，存 raw 結果；不需要 aggregate 入參\n",
    "    run_btn.click(\n",
    "        run_once,\n",
    "        inputs=[inp, tile, stride, use_clip, alpha],\n",
    "        outputs=[caption, overlay, raw]\n",
    "    )\n",
    "\n",
    "    # 事後多標準彙總：直接讀取 raw（tiles），不重跑模型\n",
    "    recompute.click(\n",
    "        reaggregate,\n",
    "        inputs=[raw, methods],\n",
    "        outputs=[caption, table]\n",
    "    )\n",
    "\n",
    "# 用 queue 可保持順暢；Gradio 4.x 不要 concurrency_count\n",
    "demo2.queue(max_size=8).launch(share=False, inline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6418551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n",
      "/tmp/ipykernel_7261/2033640146.py:459: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  heat_img = Image.fromarray(heat_rgb, mode='RGB')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # ========= 範例 =========\n",
    "# if __name__ == \"__main__\":\n",
    "#     IMG_PATH = \"/home/yaya/ai-detect-proj/test_img/real/Wtc-photo.jpg\"\n",
    "#     result = infer_image_by_tiles(IMG_PATH, tile=256, stride=128, aggregate='mean_logit', use_clip=True)\n",
    "#     print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "#     if result.get('overlay_path'):\n",
    "#         print(\"Overlay saved to:\", result['overlay_path'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
