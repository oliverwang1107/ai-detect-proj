{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3f81f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "[train] real=56000 fake=56000\n",
      "[val] real=7000 fake=7000\n",
      "[test_iid] real=7000 fake=7000\n",
      "[test_ood] real=8785 fake=8785\n",
      "Train(after cap): 112000 | real=56000 fake=56000\n",
      "Val  (after cap): 14000   | real=7000 fake=7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train ep1: 100%|██████████| 1750/1750 [06:38<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 01] train_loss=0.5100 | Val acc@0.5=0.7964 acc@thr*=0.8251 AUC=0.8903 thr*=0.258\n",
      "  ↳ saved best: /home/yaya/ai-detect-proj/Script/saved_models/ela_fromnpy_cnn_best_20250820_194931.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train ep2: 100%|██████████| 1750/1750 [06:49<00:00,  4.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 330\u001b[39m\n\u001b[32m    327\u001b[39m         scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m); sched.step()\n\u001b[32m    328\u001b[39m     running += loss.detach().float().item()\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m y_val, p_val = \u001b[43mcollect_val_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m acc = ((p_val>=\u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)==y_val).mean()\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m: auc = roc_auc_score(y_val, p_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/imgfeat/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mcollect_val_scores\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m    268\u001b[39m     x = x.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m).contiguous(memory_format=torch.channels_last \u001b[38;5;28;01mif\u001b[39;00m device==\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.contiguous_format)\n\u001b[32m    269\u001b[39m     logit = model(x)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     ps.extend(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy().tolist())\n\u001b[32m    271\u001b[39m     ys.extend(y.numpy().tolist())\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(ys), np.array(ps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# ELA(npy/npz) → CNN  — 直接讀取預先計算好的 ELA 陣列（修版：可直接跑通）\n",
    "# ======================================================\n",
    "import os, glob, time, math, random, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "SCRIPT_ROOT = \"/home/yaya/ai-detect-proj/Script\"\n",
    "REAL_DIR = os.path.join(SCRIPT_ROOT, \"features_npy\", \"ela_real_npy\")\n",
    "FAKE_DIR = os.path.join(SCRIPT_ROOT, \"features_npy\", \"ela_fake_npy\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_ROOT, \"saved_models\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "VAL_SIZE = 0.2\n",
    "RANDOM_SEED = 1337\n",
    "CAP_TRAIN_PER_CLASS = None   # None 不限制\n",
    "CAP_VAL_PER_CLASS   = None\n",
    "AUTO_BALANCE        = True\n",
    "\n",
    "INPUT_SIZE   = 256\n",
    "TRAIN_CENTER = False\n",
    "\n",
    "EPOCHS      = 15\n",
    "BATCH_SIZE  = 64\n",
    "ACCUM_STEPS = 1\n",
    "LR_MAX      = 3e-4\n",
    "WEIGHT_DECAY= 1e-2\n",
    "GRAD_CLIP   = 1.0\n",
    "NUM_WORKERS = max((os.cpu_count() or 1)-1, 0)\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------- IO Utils ----------------\n",
    "def list_np(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.npy\")) +\n",
    "                   glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "    if not files:\n",
    "        warnings.warn(f\"No .npy/.npz under {folder}\")\n",
    "    return files\n",
    "\n",
    "def _npz_pick(z):\n",
    "    # 優先常見鍵；全無時取第一個\n",
    "    for k in ('ela','arr','arr_0','data'):\n",
    "        if isinstance(z, np.lib.npyio.NpzFile) and (k in z.files):\n",
    "            return z[k]\n",
    "    return z[z.files[0]] if isinstance(z, np.lib.npyio.NpzFile) else z\n",
    "\n",
    "def load_ela_array(path):\n",
    "    \"\"\"讀入 .npy/.npz 的 ELA 陣列；輸出 HxWx3 float32\"\"\"\n",
    "    z = np.load(path, mmap_mode='r')\n",
    "    a = _npz_pick(z)\n",
    "    a = np.asarray(a)\n",
    "    # 形狀整理到 HxWx3\n",
    "    if a.ndim == 2:\n",
    "        a = np.repeat(a[..., None], 3, axis=2)\n",
    "    elif a.ndim == 3 and a.shape[0] in (1,3) and a.shape[-1] not in (1,3):\n",
    "        a = np.transpose(a, (1,2,0))  # CxHxW -> HxWxC\n",
    "    elif a.ndim == 3 and a.shape[-1] == 1:\n",
    "        a = np.repeat(a, 3, axis=2)\n",
    "    assert a.ndim == 3 and a.shape[-1] == 3, f\"Expect HxWx3, got {a.shape} from {path}\"\n",
    "\n",
    "    # dtype 正規化：uint8/int8/float* 全部可；把 0..255 類型先縮放到 0..1\n",
    "    a = a.astype(np.float32, copy=False)\n",
    "    if np.nanmax(a) > 1.5:\n",
    "        a *= (1.0 / 255.0)\n",
    "    return a\n",
    "\n",
    "def per_image_zscore_3ch(x: np.ndarray):\n",
    "    m = x.mean(axis=(0,1), keepdims=True)\n",
    "    s = x.std(axis=(0,1), keepdims=True); s[s<1e-6] = 1.0\n",
    "    return (x - m) / s\n",
    "\n",
    "def crop_2d_hw3(img, size=256, center=False, rng=None):\n",
    "    h, w, c = img.shape\n",
    "    if h < size or w < size:\n",
    "        pad_h = max(0, size - h); pad_w = max(0, size - w)\n",
    "        img = np.pad(img, ((pad_h//2, pad_h - pad_h//2),\n",
    "                           (pad_w//2, pad_w - pad_w//2),\n",
    "                           (0,0)), mode='edge')\n",
    "        h, w, _ = img.shape\n",
    "    if h == size and w == size:\n",
    "        return img.copy()\n",
    "    if center:\n",
    "        top = (h - size)//2; left = (w - size)//2\n",
    "    else:\n",
    "        if rng is None: rng = np.random.default_rng()\n",
    "        top  = int(rng.integers(0, h - size + 1))\n",
    "        left = int(rng.integers(0, w - size + 1))\n",
    "    return img[top:top+size, left:left+size, :].copy()\n",
    "\n",
    "def cap_per_class(paths, labels, cap, seed=1337):\n",
    "    if cap is None: return paths, labels\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idx = np.arange(len(paths)); labels = np.array(labels)\n",
    "    keep = []\n",
    "    for cls in [0,1]:\n",
    "        idc = idx[labels==cls]\n",
    "        if len(idc) > cap: idc = rng.choice(idc, cap, replace=False)\n",
    "        keep.append(idc)\n",
    "    keep = np.concatenate(keep); rng.shuffle(keep)\n",
    "    return [paths[i] for i in keep], [int(labels[i]) for i in keep]\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class ELAFromNPY(Dataset):\n",
    "    def __init__(self, paths, labels, train=True, size=256, train_center=False):\n",
    "        self.paths = paths; self.labels = labels\n",
    "        self.train = train; self.size = size; self.train_center = train_center\n",
    "        self.rng = np.random.default_rng()\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        x = load_ela_array(self.paths[i])                                  # HxWx3 float32\n",
    "        x = crop_2d_hw3(x, size=self.size, center=(not self.train or self.train_center), rng=self.rng)\n",
    "        x = per_image_zscore_3ch(x)                                        # z-score\n",
    "        x = np.transpose(x, (2,0,1))                                       # 3xHxW\n",
    "        t = torch.from_numpy(x)                                            # float32\n",
    "        y = torch.tensor(self.labels[i], dtype=torch.float32)\n",
    "        return t, y\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "class ELAForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        def bnblk(ci, co):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(co),\n",
    "                nn.ReLU(inplace=True))\n",
    "        def gnblk(ci, co, groups=8):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(groups, co), num_channels=co),\n",
    "                nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(\n",
    "            bnblk(in_ch,32), bnblk(32,32), nn.AvgPool2d(2),\n",
    "            bnblk(32,64),    bnblk(64,64), nn.AvgPool2d(2),\n",
    "            gnblk(64,128),   gnblk(128,128), nn.AvgPool2d(2),\n",
    "            gnblk(128,256),  gnblk(256,256), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    def forward(self,x):\n",
    "        return self.fc(self.net(x).flatten(1)).squeeze(1)\n",
    "\n",
    "# ---------------- Load splits from JSON (ELA-aware) ----------------\n",
    "SPLITS_JSON = os.path.join(OUTPUT_DIR, \"splits_clip_feature_iid_ood.json\")\n",
    "assert os.path.isfile(SPLITS_JSON), f\"找不到 splits json：{SPLITS_JSON}\"\n",
    "with open(SPLITS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    SPLITS_IN = json.load(f)[\"splits\"]\n",
    "\n",
    "def _index_dir(d):\n",
    "    idx = {}\n",
    "    for q in Path(d).glob(\"*.npy\"):\n",
    "        idx[q.name.lower()] = str(q)\n",
    "        idx[q.stem.lower()] = str(q)\n",
    "    for q in Path(d).glob(\"*.npz\"):\n",
    "        idx[q.name.lower()] = str(q)\n",
    "        idx[q.stem.lower()] = str(q)\n",
    "    return idx\n",
    "IDX_REAL = _index_dir(REAL_DIR)\n",
    "IDX_FAKE = _index_dir(FAKE_DIR)\n",
    "\n",
    "def _pick_lists(sp_name):\n",
    "    \"\"\"回傳 (real_list, fake_list) of ELA 路徑或檔名（未必存在）\"\"\"\n",
    "    S = SPLITS_IN.get(sp_name, {})\n",
    "    # 1) 若 JSON 已包含 ELA 的路徑\n",
    "    if isinstance(S, dict) and \"ela\" in S and isinstance(S[\"ela\"], dict):\n",
    "        real = [p for p in S[\"ela\"].get(\"real\", []) if p]\n",
    "        fake = [p for p in S[\"ela\"].get(\"fake\", []) if p]\n",
    "        return real, fake\n",
    "    # 2) 有 stems：直接用 stem 拼檔名\n",
    "    if isinstance(S, dict) and \"stems\" in S and isinstance(S[\"stems\"], dict):\n",
    "        real = [s + \".npy\" for s in S[\"stems\"].get(\"real\", [])]\n",
    "        fake = [s + \".npy\" for s in S[\"stems\"].get(\"fake\", [])]\n",
    "        return real, fake\n",
    "    # 3) 退而用 clip 清單，將目錄名改寫成 ELA\n",
    "    if isinstance(S, dict) and \"clip\" in S and isinstance(S[\"clip\"], dict):\n",
    "        r = []\n",
    "        for p in S[\"clip\"].get(\"real\", []):\n",
    "            s = Path(p).as_posix().replace(\"/clip_real_npy/\", \"/ela_real_npy/\")\n",
    "            r.append(s)\n",
    "        f = []\n",
    "        for p in S[\"clip\"].get(\"fake\", []):\n",
    "            s = Path(p).as_posix().replace(\"/clip_fake_npy/\", \"/ela_fake_npy/\")\n",
    "            f.append(s)\n",
    "        return r, f\n",
    "    # 舊版 JSON（直接是 list）的相容\n",
    "    if isinstance(S, list):\n",
    "        return S, []\n",
    "    return [], []\n",
    "\n",
    "def _map_to_existing(src_list, is_real):\n",
    "    idx = IDX_REAL if is_real else IDX_FAKE\n",
    "    mapped, miss = [], 0\n",
    "    for p in src_list:\n",
    "        q = Path(p)\n",
    "        # 先試絕對/相對路徑是否存在\n",
    "        if q.is_file():\n",
    "            mapped.append(str(q)); continue\n",
    "        # 不存在則用 stem/basename 在 ELA 目錄找\n",
    "        key = q.name.lower()\n",
    "        q2 = idx.get(key) or idx.get(q.stem.lower())\n",
    "        if q2 is None:\n",
    "            miss += 1; continue\n",
    "        mapped.append(q2)\n",
    "    return mapped, miss\n",
    "\n",
    "def build_split(sp_name):\n",
    "    r_list, f_list = _pick_lists(sp_name)\n",
    "    r_map, r_miss = _map_to_existing(r_list, True)\n",
    "    f_map, f_miss = _map_to_existing(f_list, False)\n",
    "    if (r_miss + f_miss) > 0:\n",
    "        print(f\"[{sp_name}] 對應 real {len(r_map)}/{len(r_list)}（缺 {r_miss}），fake {len(f_map)}/{len(f_list)}（缺 {f_miss}）\")\n",
    "    else:\n",
    "        print(f\"[{sp_name}] real={len(r_map)} fake={len(f_map)}\")\n",
    "    paths = r_map + f_map\n",
    "    labels = [0]*len(r_map) + [1]*len(f_map)\n",
    "    return paths, labels\n",
    "\n",
    "train_paths, y_tr = build_split(\"train\")\n",
    "val_paths,   y_va = build_split(\"val\")\n",
    "test_iid,    y_ti = build_split(\"test_iid\") if \"test_iid\" in SPLITS_IN else ([],[])\n",
    "test_ood,    y_to = build_split(\"test_ood\") if \"test_ood\" in SPLITS_IN else ([],[])\n",
    "\n",
    "# 自動對半 / cap\n",
    "if AUTO_BALANCE and CAP_TRAIN_PER_CLASS is None:\n",
    "    n0, n1 = sum(np.array(y_tr)==0), sum(np.array(y_tr)==1)\n",
    "    CAP_TRAIN_PER_CLASS = min(n0, n1)\n",
    "def _cap(paths, labels, cap):\n",
    "    return cap_per_class(paths, labels, cap, seed=RANDOM_SEED)\n",
    "train_paths, y_tr = _cap(train_paths, y_tr, CAP_TRAIN_PER_CLASS)\n",
    "val_paths,   y_va = _cap(val_paths,   y_va, CAP_VAL_PER_CLASS)\n",
    "\n",
    "print(f\"Train(after cap): {len(train_paths)} | real={sum(np.array(y_tr)==0)} fake={sum(np.array(y_tr)==1)}\")\n",
    "print(f\"Val  (after cap): {len(val_paths)}   | real={sum(np.array(y_va)==0)} fake={sum(np.array(y_va)==1)}\")\n",
    "\n",
    "# ---------------- Build Loaders ----------------\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    kwargs = dict(batch_size=batch_size, shuffle=shuffle,\n",
    "                  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                  drop_last=shuffle)\n",
    "    if NUM_WORKERS > 0:\n",
    "        kwargs.update(persistent_workers=True, prefetch_factor=2)\n",
    "    return DataLoader(dataset, **kwargs)\n",
    "\n",
    "train_set = ELAFromNPY(train_paths, y_tr, train=True,  size=INPUT_SIZE, train_center=TRAIN_CENTER)\n",
    "val_set   = ELAFromNPY(val_paths,   y_va, train=False, size=INPUT_SIZE, train_center=True)\n",
    "\n",
    "dl = make_loader(train_set, BATCH_SIZE, True)\n",
    "vl = make_loader(val_set,   BATCH_SIZE*2, False)\n",
    "\n",
    "ti_loader = make_loader(ELAFromNPY(test_iid, y_ti, train=False, size=INPUT_SIZE, train_center=True), BATCH_SIZE*2, False) if len(test_iid) else None\n",
    "to_loader = make_loader(ELAFromNPY(test_ood, y_to, train=False, size=INPUT_SIZE, train_center=True), BATCH_SIZE*2, False) if len(test_ood) else None\n",
    "\n",
    "# ---------------- Eval / Threshold helpers ----------------\n",
    "@torch.no_grad()\n",
    "def collect_val_scores(model, loader, device=\"cuda\"):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    for x,y in loader:\n",
    "        x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last if device=='cuda' else torch.contiguous_format)\n",
    "        logit = model(x)\n",
    "        ps.extend(torch.sigmoid(logit).float().cpu().numpy().tolist())\n",
    "        ys.extend(y.numpy().tolist())\n",
    "    return np.array(ys), np.array(ps)\n",
    "\n",
    "def best_threshold(y, p, mode=\"youden\"):\n",
    "    if mode == \"youden\":\n",
    "        fpr, tpr, thr = roc_curve(y, p); j = tpr - fpr\n",
    "        return float(thr[np.argmax(j)])\n",
    "    elif mode == \"acc\":\n",
    "        qs = np.quantile(p, np.linspace(0.01, 0.99, 99))\n",
    "        accs = [(((p>=t).astype(int)==y).mean()) for t in qs]\n",
    "        return float(qs[int(np.argmax(accs))])\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "# ---------------- Train ----------------\n",
    "try:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model = ELAForensicCNN().to(device).to(memory_format=torch.channels_last if device=='cuda' else torch.contiguous_format)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=WEIGHT_DECAY)\n",
    "lossf = nn.BCEWithLogitsLoss()\n",
    "use_amp = (device == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "steps_per_epoch = max(1, math.ceil(len(dl) / max(1, ACCUM_STEPS)))\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    opt, max_lr=LR_MAX, epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.15, div_factor=10.0, final_div_factor=10.0\n",
    ")\n",
    "\n",
    "best_auc = -1.0\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "best_path = os.path.join(OUTPUT_DIR, f\"ela_fromnpy_cnn_best_{stamp}.pt\")\n",
    "best_thr  = 0.5\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); running = 0.0\n",
    "    pbar = tqdm(dl, total=len(dl), desc=f\"train ep{ep}\")\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    for step,(x,y) in enumerate(pbar,1):\n",
    "        x = x.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last if device=='cuda' else torch.contiguous_format)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.autocast(device_type=(\"cuda\" if use_amp else \"cpu\"),\n",
    "                            dtype=(torch.float16 if use_amp else torch.bfloat16),\n",
    "                            enabled=use_amp):\n",
    "            logit = model(x)\n",
    "            y_s = y*0.95 + 0.025  # label smoothing 0.05\n",
    "            loss = lossf(logit, y_s) / max(1, ACCUM_STEPS)\n",
    "        scaler.scale(loss).backward()\n",
    "        if step % max(1, ACCUM_STEPS) == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sched.step()\n",
    "        running += loss.detach().float().item()\n",
    "\n",
    "    y_val, p_val = collect_val_scores(model, vl, device=device)\n",
    "    acc = ((p_val>=0.5).astype(int)==y_val).mean()\n",
    "    try: auc = roc_auc_score(y_val, p_val)\n",
    "    except ValueError: auc = float(\"nan\")\n",
    "    thr_star = best_threshold(y_val, p_val, mode=\"youden\")\n",
    "    acc_star = ((p_val>=thr_star).astype(int)==y_val).mean()\n",
    "\n",
    "    print(f\"[EP {ep:02d}] train_loss={running/max(1,len(dl)):.4f} | Val acc@0.5={acc:.4f} acc@thr*={acc_star:.4f} AUC={auc:.4f} thr*={thr_star:.3f}\")\n",
    "\n",
    "    if auc > best_auc + 1e-4:\n",
    "        best_auc = auc; best_thr = thr_star\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        with open(best_path.replace(\".pt\",\".thr.txt\"), \"w\") as f:\n",
    "            f.write(f\"{best_thr:.6f}\\n\")\n",
    "        print(\"  ↳ saved best:\", best_path)\n",
    "\n",
    "print(\"\\n=== Best model on Val ===\")\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "y_val, p_val = collect_val_scores(model, vl, device=device)\n",
    "acc = ((p_val>=0.5).astype(int)==y_val).mean()\n",
    "auc = roc_auc_score(y_val, p_val) if len(y_val) else float(\"nan\")\n",
    "thr_path = best_path.replace(\".pt\",\".thr.txt\")\n",
    "thr_star = float(open(thr_path).read().strip()) if os.path.exists(thr_path) else 0.5\n",
    "acc_star = ((p_val>=thr_star).astype(int)==y_val).mean()\n",
    "print(\"Val ACC@0.5:\", acc, \"ACC@thr*:\", acc_star, \"AUC:\", auc)\n",
    "print(confusion_matrix(y_val, (p_val>=thr_star).astype(int)))\n",
    "print(classification_report(y_val, (p_val>=thr_star).astype(int), target_names=[\"real(0)\",\"fake(1)\"]))\n",
    "\n",
    "# ---------------- Inference ----------------\n",
    "@torch.no_grad()\n",
    "def predict_one_ela_npy(npy_or_npz_path, model_path=best_path, input_size=INPUT_SIZE):\n",
    "    mdl = ELAForensicCNN().to(device).eval()\n",
    "    mdl.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    thr_path = model_path.replace(\".pt\",\".thr.txt\")\n",
    "    thr = float(open(thr_path).read().strip()) if os.path.exists(thr_path) else 0.5\n",
    "\n",
    "    x = load_ela_array(npy_or_npz_path)\n",
    "    x = crop_2d_hw3(x, size=input_size, center=True)\n",
    "    x = per_image_zscore_3ch(x)\n",
    "    x = np.transpose(x, (2,0,1))\n",
    "    t = torch.from_numpy(x).unsqueeze(0).to(device)\n",
    "    p = torch.sigmoid(mdl(t)).float().cpu().item()\n",
    "    return p, int(p >= thr)   # prob_fake, 1=fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caedebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val thresholds → youden=0.526 | fpr@5%=0.513 | saved*=0.526\n",
      "\n",
      "[Val (youden)] acc@thr=0.9236 | auc=0.9600 | thr=0.526\n",
      "[[6665  335]\n",
      " [ 734 6266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9008    0.9521    0.9258      7000\n",
      "         1.0     0.9493    0.8951    0.9214      7000\n",
      "\n",
      "    accuracy                         0.9236     14000\n",
      "   macro avg     0.9250    0.9236    0.9236     14000\n",
      "weighted avg     0.9250    0.9236    0.9236     14000\n",
      "\n",
      "\n",
      "[Test-IID (youden)] acc@thr=0.9255 | auc=0.9599 | thr=0.526\n",
      "[[6685  315]\n",
      " [ 728 6272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9018    0.9550    0.9276      7000\n",
      "         1.0     0.9522    0.8960    0.9232      7000\n",
      "\n",
      "    accuracy                         0.9255     14000\n",
      "   macro avg     0.9270    0.9255    0.9254     14000\n",
      "weighted avg     0.9270    0.9255    0.9254     14000\n",
      "\n",
      "\n",
      "[Test-OOD (youden)] acc@thr=0.5478 | auc=0.6227 | thr=0.526\n",
      "[[ 7523 16262]\n",
      " [ 5249 18536]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5890    0.3163    0.4116     23785\n",
      "         1.0     0.5327    0.7793    0.6328     23785\n",
      "\n",
      "    accuracy                         0.5478     47570\n",
      "   macro avg     0.5608    0.5478    0.5222     47570\n",
      "weighted avg     0.5608    0.5478    0.5222     47570\n",
      "\n",
      "\n",
      "[Val (FPR@5%)] acc@thr=0.9232 | auc=0.9600 | thr=0.513\n",
      "[[6651  349]\n",
      " [ 726 6274]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9016    0.9501    0.9252      7000\n",
      "         1.0     0.9473    0.8963    0.9211      7000\n",
      "\n",
      "    accuracy                         0.9232     14000\n",
      "   macro avg     0.9244    0.9232    0.9232     14000\n",
      "weighted avg     0.9244    0.9232    0.9232     14000\n",
      "\n",
      "\n",
      "[Test-IID (FPR@5%)] acc@thr=0.9256 | auc=0.9599 | thr=0.513\n",
      "[[6675  325]\n",
      " [ 716 6284]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9031    0.9536    0.9277      7000\n",
      "         1.0     0.9508    0.8977    0.9235      7000\n",
      "\n",
      "    accuracy                         0.9256     14000\n",
      "   macro avg     0.9270    0.9256    0.9256     14000\n",
      "weighted avg     0.9270    0.9256    0.9256     14000\n",
      "\n",
      "\n",
      "[Test-OOD (FPR@5%)] acc@thr=0.5477 | auc=0.6227 | thr=0.513\n",
      "[[ 7471 16314]\n",
      " [ 5202 18583]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5895    0.3141    0.4098     23785\n",
      "         1.0     0.5325    0.7813    0.6333     23785\n",
      "\n",
      "    accuracy                         0.5477     47570\n",
      "   macro avg     0.5610    0.5477    0.5216     47570\n",
      "weighted avg     0.5610    0.5477    0.5216     47570\n",
      "\n",
      "\n",
      "== OOD per-dataset (thr = FPR@5%) ==\n",
      "- midjourney n=17273 | acc=0.9832 | auc=nan\n",
      "[[    0     0]\n",
      " [  290 16983]]\n",
      "\n",
      "- places365  n=15000 | acc=0.0431 | auc=nan\n",
      "[[  647 14353]\n",
      " [    0     0]]\n",
      "\n",
      "- coco2017   n= 8785 | acc=0.7768 | auc=nan\n",
      "[[6824 1961]\n",
      " [   0    0]]\n",
      "\n",
      "- dalle3     n= 6512 | acc=0.2457 | auc=nan\n",
      "[[   0    0]\n",
      " [4912 1600]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/yaya/miniforge3/envs/imgfeat/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==== ELA-CNN：用 Val 找門檻，評估 Test-IID / Test-OOD，並做 OOD 逐來源報表 ====\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# 讀回最佳模型與 thr*\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "thr_saved = float(open(best_path.replace(\".pt\",\".thr.txt\")).read().strip())\n",
    "\n",
    "def eval_loader(name, loader, thr):\n",
    "    y, p = collect_val_scores(model, loader, device=device)\n",
    "    acc = ((p >= thr).astype(int) == y).mean()\n",
    "    auc = roc_auc_score(y, p)\n",
    "    print(f\"\\n[{name}] acc@thr={acc:.4f} | auc={auc:.4f} | thr={thr:.3f}\")\n",
    "    print(confusion_matrix(y, (p >= thr).astype(int)))\n",
    "    print(classification_report(y, (p >= thr).astype(int), digits=4))\n",
    "    return y, p\n",
    "\n",
    "# 1) 用 Val 取兩個門檻：Youden J 與 FPR@5%\n",
    "y_v, p_v = collect_val_scores(model, vl, device=device)\n",
    "fpr, tpr, thr = roc_curve(y_v, p_v); J = tpr - fpr\n",
    "t_youden = float(thr[np.argmax(J)])\n",
    "idx = np.where(fpr <= 0.05)[0]\n",
    "t_fpr05 = float(thr[idx[-1]]) if len(idx) else float(thr[0])\n",
    "print(f\"Val thresholds → youden={t_youden:.3f} | fpr@5%={t_fpr05:.3f} | saved*={thr_saved:.3f}\")\n",
    "\n",
    "# 2) 依兩種門檻做完整評估\n",
    "eval_loader(\"Val (youden)\", vl, t_youden)\n",
    "if 'ti_loader' in globals() and ti_loader is not None:\n",
    "    y_ti, p_ti = eval_loader(\"Test-IID (youden)\", ti_loader, t_youden)\n",
    "if 'to_loader' in globals() and to_loader is not None:\n",
    "    y_to, p_to = eval_loader(\"Test-OOD (youden)\", to_loader, t_youden)\n",
    "\n",
    "eval_loader(\"Val (FPR@5%)\", vl, t_fpr05)\n",
    "if 'ti_loader' in globals() and ti_loader is not None:\n",
    "    eval_loader(\"Test-IID (FPR@5%)\", ti_loader, t_fpr05)\n",
    "if 'to_loader' in globals() and to_loader is not None:\n",
    "    y_to2, p_to2 = eval_loader(\"Test-OOD (FPR@5%)\", to_loader, t_fpr05)\n",
    "\n",
    "# 3) （可選）OOD 逐來源報表：看 Unsplash / DALL·E3 誰在拖分\n",
    "SEPS = (\"__\", \"---\", \"--\", \"_\", \"-\", \" \")\n",
    "ALIASES = {\"unslpash\":\"unsplash\",\"dalle-3\":\"dalle3\",\"mj\":\"midjourney\",\"midj\":\"midjourney\"}\n",
    "\n",
    "def infer_tag(p: str, is_real: bool) -> str:\n",
    "    stem = Path(p).stem\n",
    "    cut = None\n",
    "    for s in SEPS:\n",
    "        i = stem.find(s)\n",
    "        if i != -1: cut = i if cut is None else min(cut, i)\n",
    "    tag = stem[:cut] if cut is not None else stem\n",
    "    tag = ALIASES.get(tag.lower().strip(), tag.lower().strip())\n",
    "    if (not tag) or tag.isdigit():\n",
    "        tag = \"imagenet\" if is_real else \"unknown\"\n",
    "    return tag\n",
    "\n",
    "if 'test_ood' in globals() and test_ood:\n",
    "    # 先把 OOD 的 y/p 取一次（用 FPR@5% 比較保守）\n",
    "    y_ood, p_ood = (y_to2, p_to2) if 'p_to2' in globals() else (y_to, p_to)\n",
    "    groups = defaultdict(list)\n",
    "    for pth, yy, pp in zip(test_ood, y_ood, p_ood):\n",
    "        groups[infer_tag(pth, is_real=(yy==0))].append((yy, pp))\n",
    "    print(\"\\n== OOD per-dataset (thr = FPR@5%) ==\")\n",
    "    for tag, items in sorted(groups.items(), key=lambda kv: -len(kv[1])):\n",
    "        yy = np.array([a for a,_ in items]); pp = np.array([b for _,b in items])\n",
    "        pred = (pp >= t_fpr05).astype(int)\n",
    "        acc = (pred == yy).mean()\n",
    "        try: auc = roc_auc_score(yy, pp)\n",
    "        except: auc = float(\"nan\")\n",
    "        cm = confusion_matrix(yy, pred)\n",
    "        print(f\"- {tag:10s} n={len(yy):5d} | acc={acc:.4f} | auc={auc:.4f}\\n{cm}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgfeat (PyTorch)",
   "language": "python",
   "name": "imgfeat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
